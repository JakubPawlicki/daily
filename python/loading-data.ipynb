{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-268e6c2569d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/orpington/Desktop/dbgen/tpch-100/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \"\"\"\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/home/orpington/Desktop/dbgen/tpch-100/*\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "CREATE TABLE [dbo].[CUSTOMER](\n",
    "\t[C_CUSTKEY] [int] NOT NULL,\n",
    "\t[C_NAME] [varchar](25) NOT NULL,\n",
    "\t[C_ADDRESS] [varchar](40) NOT NULL,\n",
    "\t[C_NATIONKEY] [int] NOT NULL,\n",
    "\t[C_PHONE] [char](15) NOT NULL,\n",
    "\t[C_ACCTBAL] [decimal](15, 2) NOT NULL,\n",
    "\t[C_MKTSEGMENT] [char](10) NOT NULL,\n",
    "\t[C_COMMENT] [varchar](117) NOT NULL\n",
    ");\n",
    "GO\n",
    "CREATE TABLE [dbo].[LINEITEM](\n",
    "\t[L_ORDERKEY] [int] NOT NULL,\n",
    "\t[L_PARTKEY] [int] NOT NULL,\n",
    "\t[L_SUPPKEY] [int] NOT NULL,\n",
    "\t[L_LINENUMBER] [int] NOT NULL,\n",
    "\t[L_QUANTITY] [decimal](15, 2) NOT NULL,\n",
    "\t[L_EXTENDEDPRICE] [decimal](15, 2) NOT NULL,\n",
    "\t[L_DISCOUNT] [decimal](15, 2) NOT NULL,\n",
    "\t[L_TAX] [decimal](15, 2) NOT NULL,\n",
    "\t[L_RETURNFLAG] [char](1) NOT NULL,\n",
    "\t[L_LINESTATUS] [char](1) NOT NULL,\n",
    "\t[L_SHIPDATE] [date] NOT NULL,\n",
    "\t[L_COMMITDATE] [date] NOT NULL,\n",
    "\t[L_RECEIPTDATE] [date] NOT NULL,\n",
    "\t[L_SHIPINSTRUCT] [char](25) NOT NULL,\n",
    "\t[L_SHIPMODE] [char](10) NOT NULL,\n",
    "\t[L_COMMENT] [varchar](44) NOT NULL\n",
    ");\n",
    "GO\n",
    "CREATE TABLE [dbo].[NATION](\n",
    "\t[N_NATIONKEY] [int] NOT NULL,\n",
    "\t[N_NAME] [char](25) NOT NULL,\n",
    "\t[N_REGIONKEY] [int] NOT NULL,\n",
    "\t[N_COMMENT] [varchar](152) NULL\n",
    ");\n",
    "GO\n",
    "CREATE TABLE [dbo].[ORDERS](\n",
    "\t[O_ORDERKEY] [int] NOT NULL,\n",
    "\t[O_CUSTKEY] [int] NOT NULL,\n",
    "\t[O_ORDERSTATUS] [char](1) NOT NULL,\n",
    "\t[O_TOTALPRICE] [decimal](15, 2) NOT NULL,\n",
    "\t[O_ORDERDATE] [date] NOT NULL,\n",
    "\t[O_ORDERPRIORITY] [char](15) NOT NULL,\n",
    "\t[O_CLERK] [char](15) NOT NULL,\n",
    "\t[O_SHIPPRIORITY] [int] NOT NULL,\n",
    "\t[O_COMMENT] [varchar](79) NOT NULL\n",
    ");\n",
    "GO\n",
    "CREATE TABLE [dbo].[PART](\n",
    "\t[P_PARTKEY] [int] NOT NULL,\n",
    "\t[P_NAME] [varchar](55) NOT NULL,\n",
    "\t[P_MFGR] [char](25) NOT NULL,\n",
    "\t[P_BRAND] [char](10) NOT NULL,\n",
    "\t[P_TYPE] [varchar](25) NOT NULL,\n",
    "\t[P_SIZE] [int] NOT NULL,\n",
    "\t[P_CONTAINER] [char](10) NOT NULL,\n",
    "\t[P_RETAILPRICE] [decimal](15, 2) NOT NULL,\n",
    "\t[P_COMMENT] [varchar](23) NOT NULL\n",
    ");\n",
    "GO\n",
    "CREATE TABLE [dbo].[PARTSUPP](\n",
    "\t[PS_PARTKEY] [int] NOT NULL,\n",
    "\t[PS_SUPPKEY] [int] NOT NULL,\n",
    "\t[PS_AVAILQTY] [int] NOT NULL,\n",
    "\t[PS_SUPPLYCOST] [decimal](15, 2) NOT NULL,\n",
    "\t[PS_COMMENT] [varchar](199) NOT NULL\n",
    ");\n",
    "GO\n",
    "CREATE TABLE [dbo].[REGION](\n",
    "\t[R_REGIONKEY] [int] NOT NULL,\n",
    "\t[R_NAME] [char](25) NOT NULL,\n",
    "\t[R_COMMENT] [varchar](152) NULL\n",
    ");\n",
    "GO\n",
    "CREATE TABLE [dbo].[SUPPLIER](\n",
    "\t[S_SUPPKEY] [int] NOT NULL,\n",
    "\t[S_NAME] [char](25) NOT NULL,\n",
    "\t[S_ADDRESS] [varchar](40) NOT NULL,\n",
    "\t[S_NATIONKEY] [int] NOT NULL,\n",
    "\t[S_PHONE] [char](15) NOT NULL,\n",
    "\t[S_ACCTBAL] [decimal](15, 2) NOT NULL,\n",
    "\t[S_COMMENT] [varchar](101) NOT NULL\n",
    ");\n",
    "GO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "splt = s.split(\"GO\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE [dbo].[CUSTOMER](\n",
      "\t[C_CUSTKEY] [int] NOT NULL,\n",
      "\t[C_NAME] [varchar](25) NOT NULL,\n",
      "\t[C_ADDRESS] [varchar](40) NOT NULL,\n",
      "\t[C_NATIONKEY] [int] NOT NULL,\n",
      "\t[C_PHONE] [char](15) NOT NULL,\n",
      "\t[C_ACCTBAL] [decimal](15, 2) NOT NULL,\n",
      "\t[C_MKTSEGMENT] [char](10) NOT NULL,\n",
      "\t[C_COMMENT] [varchar](117) NOT NULL\n",
      ");\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(splt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CUSTOMER]('"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.search(r\"[\\[][^\\]]*[\\]][(]\", splt).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25) NOT NULL,'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"(?<=[\\]][(]).+\", splt).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CUSTOMER]('"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"[^\\]\\[]*\\]\\(\", splt).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CUSTOMER'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_find = r\"[^\\]\\[]+(?=\\]\\()\"\n",
    "re.search(name_find, splt).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nCREATE TABLE [dbo].[CUSTOMER](\\n\\t[C_CUSTKEY] [int] NOT NULL',\n",
       " '\\n\\t[C_NAME] [varchar](25) NOT NULL',\n",
       " '\\n\\t[C_ADDRESS] [varchar](40) NOT NULL',\n",
       " '\\n\\t[C_NATIONKEY] [int] NOT NULL',\n",
       " '\\n\\t[C_PHONE] [char](15) NOT NULL',\n",
       " '\\n\\t[C_ACCTBAL] [decimal](15',\n",
       " ' 2) NOT NULL',\n",
       " '\\n\\t[C_MKTSEGMENT] [char](10) NOT NULL',\n",
       " '\\n\\t[C_COMMENT] [varchar](117) NOT NULL\\n);\\n']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splt.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_CUSTKEY\n",
      "C_NAME\n",
      "C_ADDRESS\n",
      "C_NATIONKEY\n",
      "C_PHONE\n",
      "C_ACCTBAL\n",
      "C_MKTSEGMENT\n",
      "C_COMMENT\n"
     ]
    }
   ],
   "source": [
    "for line in re.split(r\"\\n\\t\\[\", splt)[1:]:\n",
    "    print(line.split(\"]\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pymotw.com/2/collections/counter.html\n",
    "import re\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "\n",
    "with open(\"/home/orpington/Desktop/dbgen/schema.csv\", \"w\") as f:\n",
    "    for table_schema in s.split(\"GO\"):\n",
    "        table = re.search(r\"[^\\]\\[]+(?=\\]\\()\", table_schema)\n",
    "        if table:\n",
    "            table_name = table.group(0)\n",
    "            for line in re.split(r\"\\n\\t\\[\", table_schema)[1:]:\n",
    "                counter.update([table_name])\n",
    "                f.write(\"{},{},{}\\n\".format(\n",
    "                                            counter[table_name], \n",
    "                                            table_name, \n",
    "                                            line.split(\"]\")[0].split(\"_\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,CUSTOMER,CUSTKEY\\n', '2,CUSTOMER,NAME\\n', '3,CUSTOMER,ADDRESS\\n', '4,CUSTOMER,NATIONKEY\\n', '5,CUSTOMER,PHONE\\n', '6,CUSTOMER,ACCTBAL\\n', '7,CUSTOMER,MKTSEGMENT\\n', '8,CUSTOMER,COMMENT\\n', '1,LINEITEM,ORDERKEY\\n', '2,LINEITEM,PARTKEY\\n', '3,LINEITEM,SUPPKEY\\n', '4,LINEITEM,LINENUMBER\\n', '5,LINEITEM,QUANTITY\\n', '6,LINEITEM,EXTENDEDPRICE\\n', '7,LINEITEM,DISCOUNT\\n', '8,LINEITEM,TAX\\n', '9,LINEITEM,RETURNFLAG\\n', '10,LINEITEM,LINESTATUS\\n', '11,LINEITEM,SHIPDATE\\n', '12,LINEITEM,COMMITDATE\\n', '13,LINEITEM,RECEIPTDATE\\n', '14,LINEITEM,SHIPINSTRUCT\\n', '15,LINEITEM,SHIPMODE\\n', '16,LINEITEM,COMMENT\\n', '1,NATION,NATIONKEY\\n', '2,NATION,NAME\\n', '3,NATION,REGIONKEY\\n', '4,NATION,COMMENT\\n', '1,ORDERS,ORDERKEY\\n', '2,ORDERS,CUSTKEY\\n', '3,ORDERS,ORDERSTATUS\\n', '4,ORDERS,TOTALPRICE\\n', '5,ORDERS,ORDERDATE\\n', '6,ORDERS,ORDERPRIORITY\\n', '7,ORDERS,CLERK\\n', '8,ORDERS,SHIPPRIORITY\\n', '9,ORDERS,COMMENT\\n', '1,PART,PARTKEY\\n', '2,PART,NAME\\n', '3,PART,MFGR\\n', '4,PART,BRAND\\n', '5,PART,TYPE\\n', '6,PART,SIZE\\n', '7,PART,CONTAINER\\n', '8,PART,RETAILPRICE\\n', '9,PART,COMMENT\\n', '1,PARTSUPP,PARTKEY\\n', '2,PARTSUPP,SUPPKEY\\n', '3,PARTSUPP,AVAILQTY\\n', '4,PARTSUPP,SUPPLYCOST\\n', '5,PARTSUPP,COMMENT\\n', '1,REGION,REGIONKEY\\n', '2,REGION,NAME\\n', '3,REGION,COMMENT\\n', '1,SUPPLIER,SUPPKEY\\n', '2,SUPPLIER,NAME\\n', '3,SUPPLIER,ADDRESS\\n', '4,SUPPLIER,NATIONKEY\\n', '5,SUPPLIER,PHONE\\n', '6,SUPPLIER,ACCTBAL\\n', '7,SUPPLIER,COMMENT\\n']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"/home/orpington/Desktop/dbgen/schema.csv\", \"r\")\n",
    "print(repr(f.readlines()))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(ORDERKEY,StringType,false),StructField(PARTKEY,StringType,false),StructField(SUPPKEY,StringType,false),StructField(LINENUMBER,StringType,false),StructField(QUANTITY,StringType,false),StructField(EXTENDEDPRICE,StringType,false),StructField(DISCOUNT,StringType,false),StructField(TAX,StringType,false),StructField(RETURNFLAG,StringType,false),StructField(LINESTATUS,StringType,false),StructField(SHIPDATE,StringType,false),StructField(COMMITDATE,StringType,false),StructField(RECEIPTDATE,StringType,false),StructField(SHIPINSTRUCT,StringType,false),StructField(SHIPMODE,StringType,false),StructField(COMMENT,StringType,false)))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "csv_schema = spark.read\\\n",
    "                   .csv(\"/home/orpington/Desktop/dbgen/schema.csv\")\\\n",
    "                   .where(col(\"_c1\") == \"LINEITEM\")\\\n",
    "                   .rdd\\\n",
    "                   .map(lambda row: StructField(row[2], StringType(), False))\\\n",
    "                   .collect()\n",
    "StructType(csv_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---+---+--------+----+----+---+---+----------+----------+----------+-----------------+-------+--------------------+----+\n",
      "|_c0|   _c1| _c2|_c3|_c4|     _c5| _c6| _c7|_c8|_c9|      _c10|      _c11|      _c12|             _c13|   _c14|                _c15|_c16|\n",
      "+---+------+----+---+---+--------+----+----+---+---+----------+----------+----------+-----------------+-------+--------------------+----+\n",
      "|  1|155190|7706|  1| 17|21168.23|0.04|0.02|  N|  O|1996-03-13|1996-02-12|1996-03-22|DELIVER IN PERSON|  TRUCK|egular courts abo...|null|\n",
      "|  1| 67310|7311|  2| 36|45983.16|0.09|0.06|  N|  O|1996-04-12|1996-02-28|1996-04-20| TAKE BACK RETURN|   MAIL|ly final dependen...|null|\n",
      "|  1| 63700|3701|  3|  8|13309.60|0.10|0.02|  N|  O|1996-01-29|1996-03-05|1996-01-31| TAKE BACK RETURN|REG AIR|riously. regular,...|null|\n",
      "|  1|  2132|4633|  4| 28|28955.64|0.09|0.06|  N|  O|1996-04-21|1996-03-30|1996-05-16|             NONE|    AIR|lites. fluffily e...|null|\n",
      "|  1| 24027|1534|  5| 24|22824.48|0.10|0.04|  N|  O|1996-03-30|1996-03-14|1996-04-01|             NONE|    FOB| pending foxes. s...|null|\n",
      "|  1| 15635| 638|  6| 32|49620.16|0.07|0.02|  N|  O|1996-01-30|1996-02-07|1996-02-03|DELIVER IN PERSON|   MAIL|   arefully slyly ex|null|\n",
      "|  2|106170|1191|  1| 38|44694.46|0.00|0.05|  N|  O|1997-01-28|1997-01-14|1997-02-02| TAKE BACK RETURN|   RAIL|ven requests. dep...|null|\n",
      "|  3|  4297|1798|  1| 45|54058.05|0.06|0.00|  R|  F|1994-02-02|1994-01-04|1994-02-23|             NONE|    AIR|ongside of the fu...|null|\n",
      "|  3| 19036|6540|  2| 49|46796.47|0.10|0.00|  R|  F|1993-11-09|1993-12-20|1993-11-24| TAKE BACK RETURN|   RAIL| unusual accounts...|null|\n",
      "|  3|128449|3474|  3| 27|39890.88|0.06|0.07|  A|  F|1994-01-16|1993-11-22|1994-01-23|DELIVER IN PERSON|   SHIP|    nal foxes wake. |null|\n",
      "|  3| 29380|1883|  4|  2| 2618.76|0.01|0.06|  A|  F|1993-12-04|1994-01-07|1994-01-01|             NONE|  TRUCK|y. fluffily pendi...|null|\n",
      "|  3|183095| 650|  5| 28|32986.52|0.04|0.00|  R|  F|1993-12-14|1994-01-10|1994-01-01| TAKE BACK RETURN|    FOB|ages nag slyly pe...|null|\n",
      "|  3| 62143|9662|  6| 26|28733.64|0.10|0.02|  A|  F|1993-10-29|1993-12-18|1993-11-04| TAKE BACK RETURN|   RAIL|ges sleep after t...|null|\n",
      "|  4| 88035|5560|  1| 30|30690.90|0.03|0.08|  N|  O|1996-01-10|1995-12-14|1996-01-18|DELIVER IN PERSON|REG AIR|- quickly regular...|null|\n",
      "|  5|108570|8571|  1| 15|23678.55|0.02|0.04|  R|  F|1994-10-31|1994-08-31|1994-11-20|             NONE|    AIR|  ts wake furiously |null|\n",
      "|  5|123927|3928|  2| 26|50723.92|0.07|0.08|  R|  F|1994-10-16|1994-09-25|1994-10-19|             NONE|    FOB|sts use slyly qui...|null|\n",
      "|  5| 37531|  35|  3| 50|73426.50|0.08|0.03|  A|  F|1994-08-08|1994-10-13|1994-08-26|DELIVER IN PERSON|    AIR|eodolites. fluffi...|null|\n",
      "|  6|139636|2150|  1| 37|61998.31|0.08|0.03|  A|  F|1992-04-27|1992-05-15|1992-05-02| TAKE BACK RETURN|  TRUCK|p furiously speci...|null|\n",
      "|  7|182052|9607|  1| 12|13608.60|0.07|0.03|  N|  O|1996-05-07|1996-03-13|1996-06-03| TAKE BACK RETURN|    FOB|ss pinto beans wa...|null|\n",
      "|  7|145243|7758|  2|  9|11594.16|0.08|0.08|  N|  O|1996-02-01|1996-03-02|1996-02-19| TAKE BACK RETURN|   SHIP|    es. instructions|null|\n",
      "+---+------+----+---+---+--------+----+----+---+---+----------+----------+----------+-----------------+-------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read\\\n",
    "            .format(\"csv\")\\\n",
    "            .option(\"schema\", StructType(csv_schema))\\\n",
    "            .option(\"sep\", \"|\")\\\n",
    "            .load(\"/home/orpington/Desktop/dbgen/tpch-1/lineitem.tbl\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+----------+--------+-------------+--------+----+----------+----------+----------+----------+-----------+-----------------+--------+--------------------+\n",
      "|ORDERKEY|PARTKEY|SUPPKEY|LINENUMBER|QUANTITY|EXTENDEDPRICE|DISCOUNT| TAX|RETURNFLAG|LINESTATUS|  SHIPDATE|COMMITDATE|RECEIPTDATE|     SHIPINSTRUCT|SHIPMODE|             COMMENT|\n",
      "+--------+-------+-------+----------+--------+-------------+--------+----+----------+----------+----------+----------+-----------+-----------------+--------+--------------------+\n",
      "|       1| 155190|   7706|         1|      17|     21168.23|    0.04|0.02|         N|         O|1996-03-13|1996-02-12| 1996-03-22|DELIVER IN PERSON|   TRUCK|egular courts abo...|\n",
      "|       1|  67310|   7311|         2|      36|     45983.16|    0.09|0.06|         N|         O|1996-04-12|1996-02-28| 1996-04-20| TAKE BACK RETURN|    MAIL|ly final dependen...|\n",
      "|       1|  63700|   3701|         3|       8|     13309.60|    0.10|0.02|         N|         O|1996-01-29|1996-03-05| 1996-01-31| TAKE BACK RETURN| REG AIR|riously. regular,...|\n",
      "|       1|   2132|   4633|         4|      28|     28955.64|    0.09|0.06|         N|         O|1996-04-21|1996-03-30| 1996-05-16|             NONE|     AIR|lites. fluffily e...|\n",
      "|       1|  24027|   1534|         5|      24|     22824.48|    0.10|0.04|         N|         O|1996-03-30|1996-03-14| 1996-04-01|             NONE|     FOB| pending foxes. s...|\n",
      "|       1|  15635|    638|         6|      32|     49620.16|    0.07|0.02|         N|         O|1996-01-30|1996-02-07| 1996-02-03|DELIVER IN PERSON|    MAIL|   arefully slyly ex|\n",
      "|       2| 106170|   1191|         1|      38|     44694.46|    0.00|0.05|         N|         O|1997-01-28|1997-01-14| 1997-02-02| TAKE BACK RETURN|    RAIL|ven requests. dep...|\n",
      "|       3|   4297|   1798|         1|      45|     54058.05|    0.06|0.00|         R|         F|1994-02-02|1994-01-04| 1994-02-23|             NONE|     AIR|ongside of the fu...|\n",
      "|       3|  19036|   6540|         2|      49|     46796.47|    0.10|0.00|         R|         F|1993-11-09|1993-12-20| 1993-11-24| TAKE BACK RETURN|    RAIL| unusual accounts...|\n",
      "|       3| 128449|   3474|         3|      27|     39890.88|    0.06|0.07|         A|         F|1994-01-16|1993-11-22| 1994-01-23|DELIVER IN PERSON|    SHIP|    nal foxes wake. |\n",
      "|       3|  29380|   1883|         4|       2|      2618.76|    0.01|0.06|         A|         F|1993-12-04|1994-01-07| 1994-01-01|             NONE|   TRUCK|y. fluffily pendi...|\n",
      "|       3| 183095|    650|         5|      28|     32986.52|    0.04|0.00|         R|         F|1993-12-14|1994-01-10| 1994-01-01| TAKE BACK RETURN|     FOB|ages nag slyly pe...|\n",
      "|       3|  62143|   9662|         6|      26|     28733.64|    0.10|0.02|         A|         F|1993-10-29|1993-12-18| 1993-11-04| TAKE BACK RETURN|    RAIL|ges sleep after t...|\n",
      "|       4|  88035|   5560|         1|      30|     30690.90|    0.03|0.08|         N|         O|1996-01-10|1995-12-14| 1996-01-18|DELIVER IN PERSON| REG AIR|- quickly regular...|\n",
      "|       5| 108570|   8571|         1|      15|     23678.55|    0.02|0.04|         R|         F|1994-10-31|1994-08-31| 1994-11-20|             NONE|     AIR|  ts wake furiously |\n",
      "|       5| 123927|   3928|         2|      26|     50723.92|    0.07|0.08|         R|         F|1994-10-16|1994-09-25| 1994-10-19|             NONE|     FOB|sts use slyly qui...|\n",
      "|       5|  37531|     35|         3|      50|     73426.50|    0.08|0.03|         A|         F|1994-08-08|1994-10-13| 1994-08-26|DELIVER IN PERSON|     AIR|eodolites. fluffi...|\n",
      "|       6| 139636|   2150|         1|      37|     61998.31|    0.08|0.03|         A|         F|1992-04-27|1992-05-15| 1992-05-02| TAKE BACK RETURN|   TRUCK|p furiously speci...|\n",
      "|       7| 182052|   9607|         1|      12|     13608.60|    0.07|0.03|         N|         O|1996-05-07|1996-03-13| 1996-06-03| TAKE BACK RETURN|     FOB|ss pinto beans wa...|\n",
      "|       7| 145243|   7758|         2|       9|     11594.16|    0.08|0.08|         N|         O|1996-02-01|1996-03-02| 1996-02-19| TAKE BACK RETURN|    SHIP|    es. instructions|\n",
      "+--------+-------+-------+----------+--------+-------------+--------+----+----------+----------+----------+----------+-----------+-----------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read\\\n",
    "            .csv(\"/home/orpington/Desktop/dbgen/tpch-1/lineitem.tbl\", \n",
    "                 schema=StructType(csv_schema),\n",
    "                 sep=\"|\"\n",
    "                )\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6001215"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+-------+\n",
      "|input_file_name()                                       |count  |\n",
      "+--------------------------------------------------------+-------+\n",
      "|file:///home/orpington/Desktop/dbgen/tpch-1/lineitem.tbl|6001215|\n",
      "|file:///home/orpington/Desktop/dbgen/tpch-1/customer.tbl|150000 |\n",
      "|file:///home/orpington/Desktop/dbgen/tpch-1/supplier.tbl|10000  |\n",
      "|file:///home/orpington/Desktop/dbgen/tpch-1/partsupp.tbl|800000 |\n",
      "|file:///home/orpington/Desktop/dbgen/tpch-1/nation.tbl  |25     |\n",
      "|file:///home/orpington/Desktop/dbgen/tpch-1/region.tbl  |5      |\n",
      "|file:///home/orpington/Desktop/dbgen/tpch-1/part.tbl    |200000 |\n",
      "|file:///home/orpington/Desktop/dbgen/tpch-1/orders.tbl  |1500000|\n",
      "+--------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, regexp_extract\n",
    "data = spark.read\\\n",
    "            .csv(\"/home/orpington/Desktop/dbgen/tpch-1/*.tbl\")\\\n",
    "            .groupby(input_file_name())\\\n",
    "            .count()\n",
    "data.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|table       |count  |\n",
      "+------------+-------+\n",
      "|lineitem.tbl|6001215|\n",
      "|nation.tbl  |25     |\n",
      "|supplier.tbl|10000  |\n",
      "|customer.tbl|150000 |\n",
      "|orders.tbl  |1500000|\n",
      "|region.tbl  |5      |\n",
      "|part.tbl    |200000 |\n",
      "|partsupp.tbl|800000 |\n",
      "+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, split, reverse, size\n",
    "data = spark.read\\\n",
    "            .csv(\"/home/orpington/Desktop/dbgen/tpch-1/*.tbl\")\\\n",
    "            .withColumn(\"split\", split(input_file_name(), r\"/\"))\\\n",
    "            .groupby(col(\"split\")[size(col(\"split\")) - 1].alias(\"table\"))\\\n",
    "            .count()\n",
    "\n",
    "data.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+----+\n",
      "|                 _c0|        _c1|                 _c2| _c3|\n",
      "+--------------------+-----------+--------------------+----+\n",
      "|                   0|     AFRICA|lar deposits. bli...|null|\n",
      "|                   1|    AMERICA|hs use ironic, ev...|null|\n",
      "|                   2|       ASIA|ges. thinly even ...|null|\n",
      "|                   3|     EUROPE|ly final courts c...|null|\n",
      "|                   4|MIDDLE EAST|uickly special ac...|null|\n",
      "|fdfdf bad rcrdssdssd|       null|                null|null|\n",
      "+--------------------+-----------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv = spark.read.option(\"sep\", \"|\")\\\n",
    "                .format(\"csv\")\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/orpington/Desktop/dbgen/tpch-1/region_bad.tbl\")\n",
    "csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error!\n"
     ]
    }
   ],
   "source": [
    "csv = spark.read.option(\"sep\", \"|\")\\\n",
    "            .format(\"csv\")\\\n",
    "            .option(\"mode\", \"FAILFAST\")\\\n",
    "            .load(\"/home/orpington/Desktop/dbgen/tpch-1/region_bad.tbl\")\n",
    "try:\n",
    "    csv.show()\n",
    "except:\n",
    "    print(\"error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+----+\n",
      "|_c0|        _c1|                 _c2| _c3|\n",
      "+---+-----------+--------------------+----+\n",
      "|  0|     AFRICA|lar deposits. bli...|null|\n",
      "|  1|    AMERICA|hs use ironic, ev...|null|\n",
      "|  2|       ASIA|ges. thinly even ...|null|\n",
      "|  3|     EUROPE|ly final courts c...|null|\n",
      "|  4|MIDDLE EAST|uickly special ac...|null|\n",
      "+---+-----------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv = spark.read.option(\"sep\", \"|\")\\\n",
    "                .format(\"csv\")\\\n",
    "                .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "                .load(\"/home/orpington/Desktop/dbgen/tpch-1/region_bad.tbl\")\n",
    "csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 _c0|                 _c1|\n",
      "+--------------------+--------------------+\n",
      "|          PAR1\u0015\u0000\u0015>\u0015@|                   \u0015|\n",
      "|\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u00014\u0018\u00010\u0016\u0000\u0000\u0000...|                   \u0015|\n",
      "|\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018\u000b",
      "MIDDLE E...|                null|\n",
      "|\u0004ME\u0001\u000b",
      "\u0000\u0004\u0001\u000b",
      "\bSIA\u0001\u001d",
      "PE...|                   \u0015|\n",
      "|\u0015\u0000\u0015\u0006\u0015\b\u001c",
      "\u0018luickly s...|                null|\n",
      "|        #\u0018account\t%\u0001|                null|\n",
      "||rding to \u001f\u0000\u0000\u0000hs ...| even\u001d",
      "D\u0000s\u0001#(ges. ...|\n",
      "|           �\u0010court\u0001\u0016|jole furious\u0015\u001e",
      "\\ex...|\n",
      "|            \u0015\u0000\u0015\u0006\u0015\b\u001c",
      "6|                null|\n",
      "|\u0019\u001c",
      "\u0019L&\b\u001c",
      "\u0015\f",
      "\u00195\b\u0006\u0000\u0019\u0018\u0003...|                null|\n",
      "|\u0016t\u0016v&\b<\u0018\u00014\u0018\u00010\u0016\u0000\u0000\u0019...|                null|\n",
      "|\u0016�\u0001\u0016�\u0001&~<\u0018\u000b",
      "MIDDLE...|                null|\n",
      "|\u0016�\b\u0016�\u0007&�\u0002<\u0018luickl...|                null|\n",
      "|           \u00166\u0016:&�\t<6|                null|\n",
      "|                   0|              AFRICA|\n",
      "|                   1|             AMERICA|\n",
      "|                   2|                ASIA|\n",
      "|                   3|              EUROPE|\n",
      "|                   4|         MIDDLE EAST|\n",
      "|                   0|              AFRICA|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    csv.write.csv(\"/tmp/herp2.txt\", mode=\"append\")\n",
    "\n",
    "spark.read.csv(\"/tmp/herp2.txt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+----+\n",
      "|_c0|        _c1|                 _c2| _c3|\n",
      "+---+-----------+--------------------+----+\n",
      "|  0|     AFRICA|lar deposits. bli...|null|\n",
      "|  1|    AMERICA|hs use ironic, ev...|null|\n",
      "|  2|       ASIA|ges. thinly even ...|null|\n",
      "|  3|     EUROPE|ly final courts c...|null|\n",
      "|  4|MIDDLE EAST|uickly special ac...|null|\n",
      "+---+-----------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    csv.write.csv(\"/tmp/herp2.txt\", mode=\"overwrite\")\n",
    "\n",
    "spark.read.csv(\"/tmp/herp2.txt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+----+\n",
      "|_c0|        _c1|                 _c2| _c3|\n",
      "+---+-----------+--------------------+----+\n",
      "|  0|     AFRICA|lar deposits. bli...|null|\n",
      "|  1|    AMERICA|hs use ironic, ev...|null|\n",
      "|  2|       ASIA|ges. thinly even ...|null|\n",
      "|  3|     EUROPE|ly final courts c...|null|\n",
      "|  4|MIDDLE EAST|uickly special ac...|null|\n",
      "+---+-----------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    csv.write.csv(\"/tmp/herp2.txt\", mode=\"ignore\")\n",
    "\n",
    "spark.read.csv(\"/tmp/herp2.txt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/tmp/herp2.txt\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read\\\n",
    "     .csv(\"/home/orpington/Desktop/herp.txt\", inferSchema=True)\\\n",
    "     .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------------------+----+\n",
      "|_c0|          _c1|                 _c2| _c3|\n",
      "+---+-------------+--------------------+----+\n",
      "|  0|       AFRICA|lar deposits. bli...|null|\n",
      "|  1|      AMERICA|hsuseironic,evenr...|null|\n",
      "|  2|         ASIA|ges. thinly even ...|null|\n",
      "|  3|   EUROPE    |ly final courts c...|null|\n",
      "|  4|  MIDDLE EAST|uickly special ac...|null|\n",
      "+---+-------------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/home/orpington/Desktop/herp.txt\", escape=\" \").show() #??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------------------+----+\n",
      "|_c0|        _c1|                 _c2| _c3|\n",
      "+---+-----------+--------------------+----+\n",
      "|  0|     AFRICA|lar deposits. bli...|null|\n",
      "|  1|    AMERICA|hs use ironic, ev...|null|\n",
      "|  2|       ASIA|ges. thinly even ...|null|\n",
      "|  3|     EUROPE|ly final courts c...|null|\n",
      "|  4|MIDDLE EAST|uickly special ac...|null|\n",
      "+---+-----------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/home/orpington/Desktop/herp.txt\", ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------+----+\n",
      "| _c0|          _c1|                 _c2| _c3|\n",
      "+----+-------------+--------------------+----+\n",
      "|   0|       AFRICA|lar deposits. bli...|null|\n",
      "|null|      AMERICA|hs use ironic, ev...|null|\n",
      "|   2|         ASIA|ges. thinly even ...|null|\n",
      "|   3|   EUROPE    |ly final courts c...|null|\n",
      "|   4|  MIDDLE EAST|uickly special ac...|null|\n",
      "+----+-------------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/home/orpington/Desktop/herp.txt\",  nullValue=1, positiveInf=2).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.write.json(\"/home/orpington/Desktop/herp3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "json() got an unexpected keyword argument 'wholeFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-c4208c8836d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/orpington/Desktop/herp3.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwholeFile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: json() got an unexpected keyword argument 'wholeFile'"
     ]
    }
   ],
   "source": [
    "csv.write.json(\"/home/orpington/Desktop/herp3.txt\", wholeFile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__all__', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '_joinrealpath', '_unicode', '_uvarprog', '_varprog', 'abspath', 'altsep', 'basename', 'commonprefix', 'curdir', 'defpath', 'devnull', 'dirname', 'exists', 'expanduser', 'expandvars', 'extsep', 'genericpath', 'getatime', 'getctime', 'getmtime', 'getsize', 'isabs', 'isdir', 'isfile', 'islink', 'ismount', 'join', 'lexists', 'normcase', 'normpath', 'os', 'pardir', 'pathsep', 'realpath', 'relpath', 'samefile', 'sameopenfile', 'samestat', 'sep', 'split', 'splitdrive', 'splitext', 'stat', 'supports_unicode_filenames', 'sys', 'walk', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(dir(os.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/data.csv                  ---> 291830740\n",
      "/tmp/data.json                 ---> 291830740\n",
      "/tmp/data.orc                  ---> 291830740\n",
      "/tmp/data.parquet              ---> 291830740\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "data = spark.read\\\n",
    "            .format(\"csv\")\\\n",
    "            .option(\"sep\", \"|\")\\\n",
    "            .load(\"/home/orpington/Desktop/dbgen/tpch-1/lineitem.tbl\", schema=\n",
    "                   StructType([StructField(row[2], StringType(), False) \n",
    "                                for row in spark.read\n",
    "                                                .csv(\"/home/orpington/Desktop/dbgen/schema.csv\")\n",
    "                                                .where(col(\"_c1\") == \"LINEITEM\")\n",
    "                                                .collect()]))\n",
    "\n",
    "import tempfile, os\n",
    "tempdir = tempfile.gettempdir()\n",
    "#compression\n",
    "\n",
    "for write_format in [\"csv\", \"json\", \"orc\", \"parquet\"]:\n",
    "#for write_format in [\"csv\"]:\n",
    "    #for compression_format in [\"uncompressed\", \"bzip2\", \"gzip\"]:\n",
    "    #file_name = \"{}_{}.{}\".format(\"data\", compression_format, write_format)\n",
    "    file_name = \"{}.{}\".format(\"data\", write_format)\n",
    "    full_save_path = os.path.join(tempdir, file_name)\n",
    "    #data.write.mode(\"OVERWRITE\").option(\"compression\",compression_format).format(write_format).save(full_save_path)\n",
    "    data.write.mode(\"OVERWRITE\").format(write_format).save(full_save_path)\n",
    "    print(\"{:<30} ---> {}\".format(full_save_path, sum(os.path.getsize(os.path.join(p, fs)) for fs in f for parent, subdir, f in os.walk(full_save_path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize(\"/tmp/data_gzip.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (1, 3)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(x, y) for x in range(2) for y in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291830740"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(os.path.getsize(os.path.join(p, fs)) for fs in f for parent, subdir, f in os.walk(\"/tmp/data_gzip.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-6372f3dfaddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/data_gzip.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "[ os.path.join(p, f) for f in files for parent, subsirs, files in os.walk(\"/tmp/data_gzip.json\") ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/data_gzip.json',\n",
       " [],\n",
       " ['.part-00004-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz.crc',\n",
       "  'part-00000-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz',\n",
       "  'part-00003-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz',\n",
       "  '.part-00001-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz.crc',\n",
       "  'part-00004-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz',\n",
       "  'part-00005-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz',\n",
       "  'part-00002-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz',\n",
       "  '_SUCCESS',\n",
       "  '._SUCCESS.crc',\n",
       "  'part-00001-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz',\n",
       "  '.part-00003-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz.crc',\n",
       "  '.part-00000-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz.crc',\n",
       "  '.part-00002-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz.crc',\n",
       "  '.part-00005-8837c228-0ab2-4831-b8f8-ce667cdb8706-c000.json.gz.crc']]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ g for w in os.walk(\"/tmp/data_gzip.json\") for g in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EX_CANTCREAT',\n",
       " 'EX_CONFIG',\n",
       " 'EX_DATAERR',\n",
       " 'EX_IOERR',\n",
       " 'EX_NOHOST',\n",
       " 'EX_NOINPUT',\n",
       " 'EX_NOPERM',\n",
       " 'EX_NOUSER',\n",
       " 'EX_OK',\n",
       " 'EX_OSERR',\n",
       " 'EX_OSFILE',\n",
       " 'EX_PROTOCOL',\n",
       " 'EX_SOFTWARE',\n",
       " 'EX_TEMPFAIL',\n",
       " 'EX_UNAVAILABLE',\n",
       " 'EX_USAGE',\n",
       " 'F_OK',\n",
       " 'NGROUPS_MAX',\n",
       " 'O_APPEND',\n",
       " 'O_ASYNC',\n",
       " 'O_CREAT',\n",
       " 'O_DIRECT',\n",
       " 'O_DIRECTORY',\n",
       " 'O_DSYNC',\n",
       " 'O_EXCL',\n",
       " 'O_LARGEFILE',\n",
       " 'O_NDELAY',\n",
       " 'O_NOATIME',\n",
       " 'O_NOCTTY',\n",
       " 'O_NOFOLLOW',\n",
       " 'O_NONBLOCK',\n",
       " 'O_RDONLY',\n",
       " 'O_RDWR',\n",
       " 'O_RSYNC',\n",
       " 'O_SYNC',\n",
       " 'O_TRUNC',\n",
       " 'O_WRONLY',\n",
       " 'P_NOWAIT',\n",
       " 'P_NOWAITO',\n",
       " 'P_WAIT',\n",
       " 'R_OK',\n",
       " 'SEEK_CUR',\n",
       " 'SEEK_END',\n",
       " 'SEEK_SET',\n",
       " 'ST_APPEND',\n",
       " 'ST_MANDLOCK',\n",
       " 'ST_NOATIME',\n",
       " 'ST_NODEV',\n",
       " 'ST_NODIRATIME',\n",
       " 'ST_NOEXEC',\n",
       " 'ST_NOSUID',\n",
       " 'ST_RDONLY',\n",
       " 'ST_RELATIME',\n",
       " 'ST_SYNCHRONOUS',\n",
       " 'ST_WRITE',\n",
       " 'TMP_MAX',\n",
       " 'UserDict',\n",
       " 'WCONTINUED',\n",
       " 'WCOREDUMP',\n",
       " 'WEXITSTATUS',\n",
       " 'WIFCONTINUED',\n",
       " 'WIFEXITED',\n",
       " 'WIFSIGNALED',\n",
       " 'WIFSTOPPED',\n",
       " 'WNOHANG',\n",
       " 'WSTOPSIG',\n",
       " 'WTERMSIG',\n",
       " 'WUNTRACED',\n",
       " 'W_OK',\n",
       " 'X_OK',\n",
       " '_Environ',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '_copy_reg',\n",
       " '_execvpe',\n",
       " '_exists',\n",
       " '_exit',\n",
       " '_get_exports_list',\n",
       " '_make_stat_result',\n",
       " '_make_statvfs_result',\n",
       " '_pickle_stat_result',\n",
       " '_pickle_statvfs_result',\n",
       " '_spawnvef',\n",
       " 'abort',\n",
       " 'access',\n",
       " 'altsep',\n",
       " 'chdir',\n",
       " 'chmod',\n",
       " 'chown',\n",
       " 'chroot',\n",
       " 'close',\n",
       " 'closerange',\n",
       " 'confstr',\n",
       " 'confstr_names',\n",
       " 'ctermid',\n",
       " 'curdir',\n",
       " 'defpath',\n",
       " 'devnull',\n",
       " 'dup',\n",
       " 'dup2',\n",
       " 'environ',\n",
       " 'errno',\n",
       " 'error',\n",
       " 'execl',\n",
       " 'execle',\n",
       " 'execlp',\n",
       " 'execlpe',\n",
       " 'execv',\n",
       " 'execve',\n",
       " 'execvp',\n",
       " 'execvpe',\n",
       " 'extsep',\n",
       " 'fchdir',\n",
       " 'fchmod',\n",
       " 'fchown',\n",
       " 'fdatasync',\n",
       " 'fdopen',\n",
       " 'fork',\n",
       " 'forkpty',\n",
       " 'fpathconf',\n",
       " 'fstat',\n",
       " 'fstatvfs',\n",
       " 'fsync',\n",
       " 'ftruncate',\n",
       " 'getcwd',\n",
       " 'getcwdu',\n",
       " 'getegid',\n",
       " 'getenv',\n",
       " 'geteuid',\n",
       " 'getgid',\n",
       " 'getgroups',\n",
       " 'getloadavg',\n",
       " 'getlogin',\n",
       " 'getpgid',\n",
       " 'getpgrp',\n",
       " 'getpid',\n",
       " 'getppid',\n",
       " 'getresgid',\n",
       " 'getresuid',\n",
       " 'getsid',\n",
       " 'getuid',\n",
       " 'initgroups',\n",
       " 'isatty',\n",
       " 'kill',\n",
       " 'killpg',\n",
       " 'lchown',\n",
       " 'linesep',\n",
       " 'link',\n",
       " 'listdir',\n",
       " 'lseek',\n",
       " 'lstat',\n",
       " 'major',\n",
       " 'makedev',\n",
       " 'makedirs',\n",
       " 'minor',\n",
       " 'mkdir',\n",
       " 'mkfifo',\n",
       " 'mknod',\n",
       " 'name',\n",
       " 'nice',\n",
       " 'open',\n",
       " 'openpty',\n",
       " 'pardir',\n",
       " 'path',\n",
       " 'pathconf',\n",
       " 'pathconf_names',\n",
       " 'pathsep',\n",
       " 'pipe',\n",
       " 'popen',\n",
       " 'popen2',\n",
       " 'popen3',\n",
       " 'popen4',\n",
       " 'putenv',\n",
       " 'read',\n",
       " 'readlink',\n",
       " 'remove',\n",
       " 'removedirs',\n",
       " 'rename',\n",
       " 'renames',\n",
       " 'rmdir',\n",
       " 'sep',\n",
       " 'setegid',\n",
       " 'seteuid',\n",
       " 'setgid',\n",
       " 'setgroups',\n",
       " 'setpgid',\n",
       " 'setpgrp',\n",
       " 'setregid',\n",
       " 'setresgid',\n",
       " 'setresuid',\n",
       " 'setreuid',\n",
       " 'setsid',\n",
       " 'setuid',\n",
       " 'spawnl',\n",
       " 'spawnle',\n",
       " 'spawnlp',\n",
       " 'spawnlpe',\n",
       " 'spawnv',\n",
       " 'spawnve',\n",
       " 'spawnvp',\n",
       " 'spawnvpe',\n",
       " 'stat',\n",
       " 'stat_float_times',\n",
       " 'stat_result',\n",
       " 'statvfs',\n",
       " 'statvfs_result',\n",
       " 'strerror',\n",
       " 'symlink',\n",
       " 'sys',\n",
       " 'sysconf',\n",
       " 'sysconf_names',\n",
       " 'system',\n",
       " 'tcgetpgrp',\n",
       " 'tcsetpgrp',\n",
       " 'tempnam',\n",
       " 'times',\n",
       " 'tmpfile',\n",
       " 'tmpnam',\n",
       " 'ttyname',\n",
       " 'umask',\n",
       " 'uname',\n",
       " 'unlink',\n",
       " 'unsetenv',\n",
       " 'urandom',\n",
       " 'utime',\n",
       " 'wait',\n",
       " 'wait3',\n",
       " 'wait4',\n",
       " 'waitpid',\n",
       " 'walk',\n",
       " 'write']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.utils import IllegalArgumentException\n",
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "data = spark.read\\\n",
    "            .format(\"csv\")\\\n",
    "            .option(\"sep\", \"|\")\\\n",
    "            .load(\"/home/orpington/Desktop/dbgen/tpch-1/lineitem.tbl\", schema=\n",
    "                   StructType([StructField(row[2], StringType(), False) \n",
    "                                for row in spark.read\n",
    "                                                .csv(\"/home/orpington/Desktop/dbgen/schema.csv\")\n",
    "                                                .where(col(\"_c1\") == \"LINEITEM\")\n",
    "                                                .collect()]))\n",
    "\n",
    "import tempfile, os\n",
    "tempdir = tempfile.gettempdir()\n",
    "from math import floor\n",
    "results = []\n",
    "\n",
    "for write_format in [\"csv\", \"json\", \"orc\", \"parquet\"]:\n",
    "    for compression_format in [\"uncompressed\", \"bzip2\", \"gzip\", \"lzo\", \"snappy\", \"zlib\", \"none\"]:\n",
    "        file_name = \"{}_{}.{}\".format(\"data\", compression_format, write_format)\n",
    "        full_save_path = os.path.join(tempdir, file_name)\n",
    "        data_write = data.write\n",
    "        data_write = data_write.mode(\"OVERWRITE\")\n",
    "        data_write = data_write.option(\"compression\",compression_format)\n",
    "        data_write = data_write.format(write_format)\n",
    "        size = 0\n",
    "        try:\n",
    "            data_write.save(full_save_path)\n",
    "        except (IllegalArgumentException, Py4JJavaError):\n",
    "            pass\n",
    "        else:\n",
    "            for parent, subdir, files in os.walk(full_save_path):\n",
    "                for f in files:\n",
    "                    size += os.path.getsize(os.path.join(parent, f))\n",
    "        results += [(full_save_path, write_format, compression_format, size)]\n",
    "        \n",
    "for full_save_path, write_format, compression_format, size in reversed(sorted(results, key=lambda t: t[3])):\n",
    "    magnitudes = [\"\", \"K\", \"M\", \"G\"]\n",
    "    for magnitude in range(len(magnitudes)):\n",
    "        if size/(1024**(magnitude+1)) < 1:\n",
    "            end_size = int(floor(size/1024**(magnitude)))\n",
    "            end_magnitude = magnitudes[magnitude]\n",
    "            break\n",
    "    print(\"{:<30} ---> {:<6} {}\".format(full_save_path, end_size, end_magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-10-03 17:21:46--  https://dl.dropboxusercontent.com/s/zegtlp7q47qltdh/Chinook_Sqlite.sqlite\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.66.6, 2620:100:6022:6::a27d:4206\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.66.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1091584 (1,0M) [text/plain]\n",
      "Saving to: ‘Chinook_Sqlite.sqlite’\n",
      "\n",
      "Chinook_Sqlite.sqli 100%[===================>]   1,04M  1,82MB/s    in 0,6s    \n",
      "\n",
      "2017-10-03 17:21:47 (1,82 MB/s) - ‘Chinook_Sqlite.sqlite’ saved [1091584/1091584]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.dropboxusercontent.com/s/zegtlp7q47qltdh/Chinook_Sqlite.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext(conf=SparkConf().set(\"spark.driver.extraClassPath\", \"/home/orpington/Desktop/jars/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo 'spark.driver.extraClassPath /home/orpington/Desktop/jars/*' > /opt/spark/conf/spark-defaults.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.dropboxusercontent.com/s/zegtlp7q47qltdh/Chinook_Sqlite.sqlite -o /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|EmployeeId|LastName|FirstName|\n",
      "+----------+--------+---------+\n",
      "|         1|   Adams|   Andrew|\n",
      "|         2| Edwards|    Nancy|\n",
      "|         3| Peacock|     Jane|\n",
      "|         4|    Park| Margaret|\n",
      "|         5| Johnson|    Steve|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         7|    King|   Robert|\n",
      "|         8|Callahan|    Laura|\n",
      "+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read\\\n",
    "             .format('jdbc')\\\n",
    "             .options(url='jdbc:sqlite:Chinook_Sqlite.sqlite', \n",
    "                      dbtable='employee',\n",
    "                      driver='org.sqlite.JDBC')\\\n",
    "             .load()\\\n",
    "             .select(\"EmployeeId\", \"LastName\", \"FirstName\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write\\\n",
    "     .format('jdbc')\\\n",
    "     .mode(\"append\")\\\n",
    "     .options(url=os.path.expandvars(\"jdbc:sqlserver://pysparksrvsql.database.windows.net:1433;database=pysparkdbsql;user=${AZ_DATABASE_ADMIN}@pysparksrvsql;password=${AZ_DATABASE_PASS};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"), \n",
    "              dbtable='herp',\n",
    "              driver='com.microsoft.sqlserver.jdbc.SQLServerDriver')\\\n",
    "     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|EmployeeId|LastName|FirstName|\n",
      "+----------+--------+---------+\n",
      "|         1|   Adams|   Andrew|\n",
      "|         2| Edwards|    Nancy|\n",
      "|         3| Peacock|     Jane|\n",
      "|         4|    Park| Margaret|\n",
      "|         5| Johnson|    Steve|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         7|    King|   Robert|\n",
      "|         8|Callahan|    Laura|\n",
      "|         1|   Adams|   Andrew|\n",
      "|         2| Edwards|    Nancy|\n",
      "|         3| Peacock|     Jane|\n",
      "|         4|    Park| Margaret|\n",
      "|         5| Johnson|    Steve|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         7|    King|   Robert|\n",
      "|         8|Callahan|    Laura|\n",
      "+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "spark.read\\\n",
    "     .format('jdbc')\\\n",
    "     .options(url=os.path.expandvars(\"jdbc:sqlserver://pysparksrvsql.database.windows.net:1433;database=pysparkdbsql;user=${AZ_DATABASE_ADMIN}@pysparksrvsql;password=${AZ_DATABASE_PASS};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"), \n",
    "              dbtable='herp',\n",
    "              driver='com.microsoft.sqlserver.jdbc.SQLServerDriver')\\\n",
    "     .load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "opts = {}\n",
    "opts[\"url\"] = \"jdbc:sqlserver://pysparksrvsql.database.windows.net:1433;\"\n",
    "opts[\"url\"] += \"database=pysparkdbsql;\"\n",
    "opts[\"url\"] += os.path.expandvars(\"user=${AZ_DATABASE_ADMIN}@pysparksrvsql;\")\n",
    "opts[\"url\"] += os.path.expandvars(\"password=${AZ_DATABASE_PASS};\")\n",
    "opts[\"url\"] += \"trustServerCertificate=false;\"\n",
    "opts[\"url\"] += \"hostNameInCertificate=*.database.windows.net;\"\n",
    "opts[\"url\"] += \"loginTimeout=30;\"\n",
    "opts[\"dbtable\"] = 'herp'\n",
    "opts[\"driver\"] = 'com.microsoft.sqlserver.jdbc.SQLServerDriver'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write\\\n",
    "     .format(\"jdbc\")\\\n",
    "     .mode(\"append\")\\\n",
    "     .options(**opts)\\\n",
    "     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|EmployeeId|LastName|FirstName|\n",
      "+----------+--------+---------+\n",
      "|         1|   Adams|   Andrew|\n",
      "|         2| Edwards|    Nancy|\n",
      "|         3| Peacock|     Jane|\n",
      "|         4|    Park| Margaret|\n",
      "|         5| Johnson|    Steve|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         7|    King|   Robert|\n",
      "|         8|Callahan|    Laura|\n",
      "|         1|   Adams|   Andrew|\n",
      "|         2| Edwards|    Nancy|\n",
      "|         3| Peacock|     Jane|\n",
      "|         1|   Adams|   Andrew|\n",
      "|         2| Edwards|    Nancy|\n",
      "|         3| Peacock|     Jane|\n",
      "|         4|    Park| Margaret|\n",
      "|         5| Johnson|    Steve|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         7|    King|   Robert|\n",
      "|         8|Callahan|    Laura|\n",
      "|         4|    Park| Margaret|\n",
      "+----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read\\\n",
    "     .format(\"jdbc\")\\\n",
    "     .options(**opts)\\\n",
    "     .load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.id=local-1507058403052\n",
      "spark.app.name=pyspark-shell\n",
      "spark.driver.extraClassPath=/home/orpington/Desktop/jars/*\n",
      "spark.driver.host=192.168.8.129\n",
      "spark.driver.port=46189\n",
      "spark.executor.id=driver\n",
      "spark.master=local[*]\n",
      "spark.rdd.compress=True\n",
      "spark.serializer.objectStreamReset=100\n",
      "spark.submit.deployMode=client\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext._conf.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df = spark.read\\\n",
    "             .format(\"jdbc\")\\\n",
    "             .options(**opts)\\\n",
    "             .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[LastName#209], functions=[])\n",
      "+- Exchange hashpartitioning(LastName#209, 200)\n",
      "   +- *HashAggregate(keys=[LastName#209], functions=[])\n",
      "      +- *Scan JDBCRelation(herp) [numPartitions=1] [LastName#209] ReadSchema: struct<LastName:string>\n"
     ]
    }
   ],
   "source": [
    "db_df.select(\"LastName\").distinct().explain()\n",
    "#Peacock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[LastName#209], functions=[])\n",
      "+- Exchange hashpartitioning(LastName#209, 200)\n",
      "   +- *HashAggregate(keys=[LastName#209], functions=[])\n",
      "      +- *Scan JDBCRelation(herp) [numPartitions=1] [LastName#209] PushedFilters: [*IsNotNull(LastName), *EqualTo(LastName,Peacock)], ReadSchema: struct<LastName:string>\n"
     ]
    }
   ],
   "source": [
    "db_df.where(\"LastName = 'Peacock'\").select(\"LastName\").distinct().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Scan JDBCRelation((select LastName from herp where LastName = 'Peacock') as merp) [numPartitions=1] [LastName#231] ReadSchema: struct<LastName:string>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "opts = {}\n",
    "opts[\"url\"] = \"jdbc:sqlserver://pysparksrvsql.database.windows.net:1433;\"\n",
    "opts[\"url\"] += \"database=pysparkdbsql;\"\n",
    "opts[\"url\"] += os.path.expandvars(\"user=${AZ_DATABASE_ADMIN}@pysparksrvsql;\")\n",
    "opts[\"url\"] += os.path.expandvars(\"password=${AZ_DATABASE_PASS};\")\n",
    "opts[\"url\"] += \"trustServerCertificate=false;\"\n",
    "opts[\"url\"] += \"hostNameInCertificate=*.database.windows.net;\"\n",
    "opts[\"url\"] += \"loginTimeout=30;\"\n",
    "opts[\"table\"] = \"(select LastName from herp where LastName = 'Peacock') as merp\"\n",
    "opts[\"properties\"] = {\"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
    "spark.read.jdbc(**opts).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|EmployeeId|LastName|FirstName|\n",
      "+----------+--------+---------+\n",
      "|         6|Mitchell|  Michael|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         5| Johnson|    Steve|\n",
      "|         5| Johnson|    Steve|\n",
      "|         5| Johnson|    Steve|\n",
      "|         5| Johnson|    Steve|\n",
      "|         5| Johnson|    Steve|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         6|Mitchell|  Michael|\n",
      "|         6|Mitchell|  Michael|\n",
      "+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "opts = {}\n",
    "opts[\"url\"] = \"jdbc:sqlserver://pysparksrvsql.database.windows.net:1433;\"\n",
    "opts[\"url\"] += \"database=pysparkdbsql;\"\n",
    "opts[\"url\"] += os.path.expandvars(\"user=${AZ_DATABASE_ADMIN}@pysparksrvsql;\")\n",
    "opts[\"url\"] += os.path.expandvars(\"password=${AZ_DATABASE_PASS};\")\n",
    "opts[\"url\"] += \"trustServerCertificate=false;\"\n",
    "opts[\"url\"] += \"hostNameInCertificate=*.database.windows.net;\"\n",
    "opts[\"url\"] += \"loginTimeout=30;\"\n",
    "opts[\"table\"] = \"herp\"\n",
    "#opts[\"predicates\"] = [\"LastName = 'Mitchell'\",\"LastName = 'Johnson'\"]\n",
    "opts[\"properties\"] = {\"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
    "spark.read.jdbc(predicates=[\n",
    "    \"LastName = 'Mitchell'\",\n",
    "    \"LastName = 'Johnson'\",\n",
    "    \"LastName = 'Mitchell'\"], **opts).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o19.sql.\n: com.microsoft.sqlserver.jdbc.SQLServerException: Invalid object name 'pysparkdbsql.herpp'.\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:217)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1655)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:440)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:385)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7505)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2445)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:191)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:166)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:62)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:113)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:306)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:77)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-4c0788eb8479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \"\"\".format(\"jdbc:sqlserver://pysparksrvsql.database.windows.net:1433\", \"pysparkdbsql.herpp\", \"orpington\", os.path.expandvars(\"${AZ_DATABASE_PASS}\"))\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o19.sql.\n: com.microsoft.sqlserver.jdbc.SQLServerException: Invalid object name 'pysparkdbsql.herpp'.\n\tat com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:217)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1655)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:440)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:385)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7505)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:2445)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:191)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:166)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:297)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:62)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:113)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:306)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:77)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:67)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:67)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"CREATE TABLE jdbcTable\n",
    "USING org.apache.spark.sql.jdbc\n",
    "OPTIONS (\n",
    "  url \"{}\",\n",
    "  dbtable \"{}\",\n",
    "  user '{}',\n",
    "  password '{}'\n",
    ")\n",
    "\"\"\".format(\"jdbc:sqlserver://pysparksrvsql.database.windows.net:1433\", \"pysparkdbsql.herpp\", \"orpington\", os.path.expandvars(\"${AZ_DATABASE_PASS}\"))\n",
    "spark.sql(s).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method jdbc in module pyspark.sql.readwriter:\n",
      "\n",
      "jdbc(self, url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Construct a :class:`DataFrame` representing the database table named ``table``\n",
      "    accessible via JDBC URL ``url`` and connection ``properties``.\n",
      "    \n",
      "    Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      "    ``predicates`` is specified. ``lowerBound`, ``upperBound`` and ``numPartitions``\n",
      "    is needed when ``column`` is specified.\n",
      "    \n",
      "    If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      "    \n",
      "    .. note:: Don't create too many partitions in parallel on a large cluster;         otherwise Spark might crash your external database systems.\n",
      "    \n",
      "    :param url: a JDBC URL of the form ``jdbc:subprotocol:subname``\n",
      "    :param table: the name of the table\n",
      "    :param column: the name of an integer column that will be used for partitioning;\n",
      "                   if this parameter is specified, then ``numPartitions``, ``lowerBound``\n",
      "                   (inclusive), and ``upperBound`` (exclusive) will form partition strides\n",
      "                   for generated WHERE clause expressions used to split the column\n",
      "                   ``column`` evenly\n",
      "    :param lowerBound: the minimum value of ``column`` used to decide partition stride\n",
      "    :param upperBound: the maximum value of ``column`` used to decide partition stride\n",
      "    :param numPartitions: the number of partitions\n",
      "    :param predicates: a list of expressions suitable for inclusion in WHERE clauses;\n",
      "                       each one defines one partition of the :class:`DataFrame`\n",
      "    :param properties: a dictionary of JDBC database connection arguments. Normally at\n",
      "                       least properties \"user\" and \"password\" with their corresponding values.\n",
      "                       For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      "    :return: a DataFrame\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.jdbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Scan JDBCRelation(herp) [numPartitions=4] [EmployeeId#300,LastName#301,FirstName#302] ReadSchema: struct<EmployeeId:int,LastName:string,FirstName:string>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "opts = {}\n",
    "opts[\"url\"] = \"jdbc:sqlserver://pysparksrvsql.database.windows.net:1433;\"\n",
    "opts[\"url\"] += \"database=pysparkdbsql;\"\n",
    "opts[\"url\"] += os.path.expandvars(\"user=${AZ_DATABASE_ADMIN}@pysparksrvsql;\")\n",
    "opts[\"url\"] += os.path.expandvars(\"password=${AZ_DATABASE_PASS};\")\n",
    "opts[\"url\"] += \"trustServerCertificate=false;\"\n",
    "opts[\"url\"] += \"hostNameInCertificate=*.database.windows.net;\"\n",
    "opts[\"url\"] += \"loginTimeout=30;\"\n",
    "opts[\"table\"] = \"herp\"\n",
    "#opts[\"predicates\"] = [\"LastName = 'Mitchell'\",\"LastName = 'Johnson'\"]\n",
    "opts[\"properties\"] = {\"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"}\n",
    "spark.read.jdbc(column=\"EmployeeId\", lowerBound=1L, upperBound=8L, numPartitions=4, **opts).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

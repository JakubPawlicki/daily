{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "assert spark.range(5).rdd.flatMap(lambda x: x).sum() == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Ben Benson\", 0, [100]),\n",
    "    (1, \"Tom Tomhson\", 1, [500, 250, 100]),\n",
    "    (2, \"Jill Jilli\", 1, [250, 100])\n",
    "]).toDF(\"id\", \"name\", \"study_program\", \"spark_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------------+---------------+\n",
      "| id|       name|study_program|   spark_status|\n",
      "+---+-----------+-------------+---------------+\n",
      "|  0| Ben Benson|            0|          [100]|\n",
      "|  1|Tom Tomhson|            1|[500, 250, 100]|\n",
      "|  2| Jill Jilli|            1|     [250, 100]|\n",
      "+---+-----------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+----------------+\n",
      "| id| degree|  department|          school|\n",
      "+---+-------+------------+----------------+\n",
      "|  0|Masters| Apache Apex|Great university|\n",
      "|  1|Masters|Apache Spark|Great university|\n",
      "|  2|  Ph.D.|Apache Spark|Great university|\n",
      "+---+-------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "studyProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"Apache Apex\", \"Great university\"),\n",
    "    (1, \"Masters\", \"Apache Spark\", \"Great university\"),\n",
    "    (2, \"Ph.D.\", \"Apache Spark\", \"Great university\")\n",
    "]).toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "studyProgram.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|        status|\n",
      "+---+--------------+\n",
      "|500|Vice President|\n",
      "|250|    PMC Member|\n",
      "|100|   Contributor|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")\n",
    "]).toDF(\"id\", \"status\")\n",
    "sparkStatus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------------+---------------+\n",
      "| id|       name|study_program|   spark_status|\n",
      "+---+-----------+-------------+---------------+\n",
      "|  1|Tom Tomhson|            1|[500, 250, 100]|\n",
      "|  2| Jill Jilli|            1|     [250, 100]|\n",
      "+---+-----------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "person.where(array_contains(\"spark_status\", \"250\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f80f66149b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_contains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_contains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark_status\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"250\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark/sql/functions.pyc\u001b[0m in \u001b[0;36marray_contains\u001b[0;34m(col, value)\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \"\"\"\n\u001b[1;32m   1688\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_contains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0margs_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALL_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m_build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m_get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m                         \u001b[0mtemp_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m                         \u001b[0mtemp_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m                         \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j/java_collections.pyc\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mHashMap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJavaClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"java.util.HashMap\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mjava_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHashMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0mjava_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains, col, lit\n",
    "person.where(array_contains(col(\"spark_status\"), lit(\"250\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========inner\n",
      "  id  degree    department    school              id  name           study_program  spark_status\n",
      "----  --------  ------------  ----------------  ----  -----------  ---------------  ---------------\n",
      "   0  Masters   Apache Apex   Great university     0  Ben Benson                 0  [100]\n",
      "   1  Masters   Apache Spark  Great university     1  Tom Tomhson                1  [500, 250, 100]\n",
      "   1  Masters   Apache Spark  Great university     2  Jill Jilli                 1  [250, 100]\n",
      "==========cross\n",
      "  id  degree    department    school              id  name           study_program  spark_status\n",
      "----  --------  ------------  ----------------  ----  -----------  ---------------  ---------------\n",
      "   0  Masters   Apache Apex   Great university     0  Ben Benson                 0  [100]\n",
      "   1  Masters   Apache Spark  Great university     1  Tom Tomhson                1  [500, 250, 100]\n",
      "   1  Masters   Apache Spark  Great university     2  Jill Jilli                 1  [250, 100]\n",
      "==========outer\n",
      "  id  degree    department    school              id  name           study_program  spark_status\n",
      "----  --------  ------------  ----------------  ----  -----------  ---------------  ---------------\n",
      "   0  Masters   Apache Apex   Great university     0  Ben Benson                 0  [100]\n",
      "   1  Masters   Apache Spark  Great university     1  Tom Tomhson                1  [500, 250, 100]\n",
      "   1  Masters   Apache Spark  Great university     2  Jill Jilli                 1  [250, 100]\n",
      "   2  Ph.D.     Apache Spark  Great university\n",
      "==========full\n",
      "  id  degree    department    school              id  name           study_program  spark_status\n",
      "----  --------  ------------  ----------------  ----  -----------  ---------------  ---------------\n",
      "   0  Masters   Apache Apex   Great university     0  Ben Benson                 0  [100]\n",
      "   1  Masters   Apache Spark  Great university     1  Tom Tomhson                1  [500, 250, 100]\n",
      "   1  Masters   Apache Spark  Great university     2  Jill Jilli                 1  [250, 100]\n",
      "   2  Ph.D.     Apache Spark  Great university\n",
      "==========full_outer\n",
      "  id  degree    department    school              id  name           study_program  spark_status\n",
      "----  --------  ------------  ----------------  ----  -----------  ---------------  ---------------\n",
      "   0  Masters   Apache Apex   Great university     0  Ben Benson                 0  [100]\n",
      "   1  Masters   Apache Spark  Great university     1  Tom Tomhson                1  [500, 250, 100]\n",
      "   1  Masters   Apache Spark  Great university     2  Jill Jilli                 1  [250, 100]\n",
      "   2  Ph.D.     Apache Spark  Great university\n",
      "==========left\n",
      "  id  degree    department    school              id  name           study_program  spark_status\n",
      "----  --------  ------------  ----------------  ----  -----------  ---------------  ---------------\n",
      "   0  Masters   Apache Apex   Great university     0  Ben Benson                 0  [100]\n",
      "   1  Masters   Apache Spark  Great university     1  Tom Tomhson                1  [500, 250, 100]\n",
      "   1  Masters   Apache Spark  Great university     2  Jill Jilli                 1  [250, 100]\n",
      "   2  Ph.D.     Apache Spark  Great university\n",
      "==========left_outer\n",
      "  id  degree    department    school              id  name           study_program  spark_status\n",
      "----  --------  ------------  ----------------  ----  -----------  ---------------  ---------------\n",
      "   0  Masters   Apache Apex   Great university     0  Ben Benson                 0  [100]\n",
      "   1  Masters   Apache Spark  Great university     1  Tom Tomhson                1  [500, 250, 100]\n",
      "   1  Masters   Apache Spark  Great university     2  Jill Jilli                 1  [250, 100]\n",
      "   2  Ph.D.     Apache Spark  Great university\n",
      "==========right\n",
      "  id  degree    department    school              id  name           study_program  spark_status\n",
      "----  --------  ------------  ----------------  ----  -----------  ---------------  ---------------\n",
      "   0  Masters   Apache Apex   Great university     0  Ben Benson                 0  [100]\n",
      "   1  Masters   Apache Spark  Great university     1  Tom Tomhson                1  [500, 250, 100]\n",
      "   1  Masters   Apache Spark  Great university     2  Jill Jilli                 1  [250, 100]\n",
      "==========right_outer\n",
      "  id  degree    department    school              id  name           study_program  spark_status\n",
      "----  --------  ------------  ----------------  ----  -----------  ---------------  ---------------\n",
      "   0  Masters   Apache Apex   Great university     0  Ben Benson                 0  [100]\n",
      "   1  Masters   Apache Spark  Great university     1  Tom Tomhson                1  [500, 250, 100]\n",
      "   1  Masters   Apache Spark  Great university     2  Jill Jilli                 1  [250, 100]\n",
      "==========left_semi\n",
      "  id  degree    department    school\n",
      "----  --------  ------------  ----------------\n",
      "   0  Masters   Apache Apex   Great university\n",
      "   1  Masters   Apache Spark  Great university\n",
      "==========left_anti\n",
      "  id  degree    department    school\n",
      "----  --------  ------------  ----------------\n",
      "   2  Ph.D.     Apache Spark  Great university\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "join_expr = person[\"study_program\"] == studyProgram[\"id\"]\n",
    "for join_type in [\"inner\", \"cross\", \"outer\",\n",
    "                  \"full\", \"full_outer\", \"left\", \n",
    "                  \"left_outer\", \"right\", \"right_outer\",\n",
    "                  \"left_semi\", \"left_anti\"]:\n",
    "    joined_df = studyProgram.join(person, join_expr, join_type)\n",
    "    print(\"=\"*10 + join_type)\n",
    "    print(tabulate(joined_df.collect(), joined_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    (1, \"aaa\"),\n",
    "    (2, \"bbb\"),\n",
    "    (3, \"bbb\"),    \n",
    "]).toDF(\"id\", \"name1\")\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    (0, \"cow\"),\n",
    "    (1, \"goose\"),\n",
    "    (1, \"duck\"),    \n",
    "]).toDF(\"id\", \"name2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"Reference 'id' is ambiguous, could be: id#597L, id#607L.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d0190bf7207c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be basestring\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"Reference 'id' is ambiguous, could be: id#597L, id#607L.;\""
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1.join(df2, col(\"id\") == col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----+\n",
      "| id|name1| id|name2|\n",
      "+---+-----+---+-----+\n",
      "|  1|  aaa|  0|  cow|\n",
      "|  1|  aaa|  1|goose|\n",
      "|  1|  aaa|  1| duck|\n",
      "|  2|  bbb|  0|  cow|\n",
      "|  2|  bbb|  1|goose|\n",
      "|  2|  bbb|  1| duck|\n",
      "|  3|  bbb|  0|  cow|\n",
      "|  3|  bbb|  1|goose|\n",
      "|  3|  bbb|  1| duck|\n",
      "+---+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.crossJoin(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+-----+\n",
      "| id|name1| id|name2|\n",
      "+---+-----+---+-----+\n",
      "|  1|  aaa|  1|goose|\n",
      "|  1|  aaa|  1| duck|\n",
      "+---+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1.join(df2, df1[\"id\"] == df2[\"id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id|name1|name2|\n",
      "+---+-----+-----+\n",
      "|  1|  aaa|goose|\n",
      "|  1|  aaa| duck|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+------------+\n",
      "| id|name1|addition_first|addition_two|\n",
      "+---+-----+--------------+------------+\n",
      "|  2|  bbb|             1|           1|\n",
      "|  1|  aaa|             0|           0|\n",
      "|  3|  bbb|             2|           2|\n",
      "+---+-----+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df1_1 = df1.withColumn(\"addition_first\", monotonically_increasing_id())\n",
    "df1_2 = df1.withColumn(\"addition_two\", monotonically_increasing_id())\n",
    "\n",
    "df1_1.join(df1_2, [\"id\", \"name1\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*SortMergeJoin [id#114L], [study_program#68L], Inner\n",
      ":- *Sort [id#114L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(id#114L, 200)\n",
      ":     +- *Project [_1#105L AS id#114L, _2#106 AS degree#115, _3#107 AS department#116, _4#108 AS school#117]\n",
      ":        +- *Filter isnotnull(_1#105L)\n",
      ":           +- Scan ExistingRDD[_1#105L,_2#106,_3#107,_4#108]\n",
      "+- *Sort [study_program#68L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(study_program#68L, 200)\n",
      "      +- *Project [_1#57L AS id#66L, _2#58 AS name#67, _3#59L AS study_program#68L, _4#60 AS spark_status#69]\n",
      "         +- *Filter isnotnull(_3#59L)\n",
      "            +- Scan ExistingRDD[_1#57L,_2#58,_3#59L,_4#60]\n"
     ]
    }
   ],
   "source": [
    "join_expr = person[\"study_program\"] == studyProgram[\"id\"]\n",
    "joined_df = studyProgram.join(person, join_expr, \"inner\")\n",
    "joined_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*BroadcastHashJoin [id#114L], [study_program#68L], Inner, BuildRight\n",
      ":- *Project [_1#105L AS id#114L, _2#106 AS degree#115, _3#107 AS department#116, _4#108 AS school#117]\n",
      ":  +- *Filter isnotnull(_1#105L)\n",
      ":     +- Scan ExistingRDD[_1#105L,_2#106,_3#107,_4#108]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[2, bigint, true]))\n",
      "   +- *Project [_1#57L AS id#66L, _2#58 AS name#67, _3#59L AS study_program#68L, _4#60 AS spark_status#69]\n",
      "      +- *Filter isnotnull(_3#59L)\n",
      "         +- Scan ExistingRDD[_1#57L,_2#58,_3#59L,_4#60]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "join_expr = person[\"study_program\"] == studyProgram[\"id\"]\n",
    "joined_df = studyProgram.join(broadcast(person), join_expr, \"inner\")\n",
    "joined_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id  name1    name2                id  name1    name2\n",
      "----  -------  -------            ----  -------  -------\n",
      "   1  aaa      goose                 1  aaa      goose\n",
      "   1  aaa      duck                  1  aaa      duck \n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "df = df1.join(df2, \"id\")\n",
    "first = [\"{:<34}\".format(line) for line \n",
    "                   in  tabulate(df.collect(), df.columns).split(\"\\n\")]\n",
    "\n",
    "second = [\"{:<20}\".format(line) for line \n",
    "                   in  tabulate(df.collect(), df.columns).split(\"\\n\")]\n",
    "\n",
    "print(\"\\n\".join([t[0] + t[1] for t in zip(first, second)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### help(pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer.tbl  nation.tbl  partsupp.tbl\tregion.tbl\r\n",
      "lineitem.tbl  orders.tbl  part.tbl\tsupplier.tbl\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/dbgen-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|(1 + 1)|\n",
      "+-------+\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select 1+1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schema.csv  tpch-1  tpch-100  tpch-dbgen\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/orpington/Desktop/dbgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,IntegerType,true),StructField(_c1,StringType,true),StructField(_c2,StringType,true)))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(\"/home/orpington/Desktop/dbgen/schema.csv\", inferSchema=True).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "|_c0|     _c1|    _c2|\n",
      "+---+--------+-------+\n",
      "|  1|CUSTOMER|CUSTKEY|\n",
      "|  2|  ORDERS|CUSTKEY|\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/home/orpington/Desktop/dbgen/schema.csv\").where(\"instr(_c2, 'CUST') != 0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+---+---------------+-------+----------+--------------------+----+\n",
      "|_c0|               _c1|                 _c2|_c3|            _c4|    _c5|       _c6|                 _c7| _c8|\n",
      "+---+------------------+--------------------+---+---------------+-------+----------+--------------------+----+\n",
      "|  1|Customer#000000001|   IVhzIApeRb ot,c,E| 15|25-989-741-2988| 711.56|  BUILDING|to the even, regu...|null|\n",
      "|  2|Customer#000000002|XSTf4,NCwDVaWNe6t...| 13|23-768-687-3665| 121.65|AUTOMOBILE|l accounts. blith...|null|\n",
      "|  3|Customer#000000003|        MG9kdTD2WBHm|  1|11-719-748-3364|7498.12|AUTOMOBILE| deposits eat sly...|null|\n",
      "|  4|Customer#000000004|         XxVSJsLAGtn|  4|14-128-190-5944|2866.83| MACHINERY| requests. final,...|null|\n",
      "|  5|Customer#000000005|KvpyuHCplrB84WgAi...|  3|13-750-942-6364| 794.47| HOUSEHOLD|n accounts will h...|null|\n",
      "+---+------------------+--------------------+---+---------------+-------+----------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers = spark.read.csv(\"/home/orpington/Desktop/dbgen/tpch-1/customer.tbl\", inferSchema=True, sep=\"|\")\n",
    "customers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               _c0|\n",
      "+-------+------------------+\n",
      "|  count|            150000|\n",
      "|   mean|           75000.5|\n",
      "| stddev|43301.414526548666|\n",
      "|    min|                 1|\n",
      "|    max|             99999|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers.select(customers[0]).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------+----------+--------+---------------+---+--------------------+----+\n",
      "|_c0|   _c1|_c2|      _c3|       _c4|     _c5|            _c6|_c7|                 _c8| _c9|\n",
      "+---+------+---+---------+----------+--------+---------------+---+--------------------+----+\n",
      "|  1| 36901|  O|173665.47|1996-01-02|   5-LOW|Clerk#000000951|  0|nstructions sleep...|null|\n",
      "|  2| 78002|  O| 46929.18|1996-12-01|1-URGENT|Clerk#000000880|  0| foxes. pending a...|null|\n",
      "|  3|123314|  F|193846.25|1993-10-14|   5-LOW|Clerk#000000955|  0|sly final account...|null|\n",
      "|  4|136777|  O| 32151.78|1995-10-11|   5-LOW|Clerk#000000124|  0|sits. slyly regul...|null|\n",
      "|  5| 44485|  F|144659.20|1994-07-30|   5-LOW|Clerk#000000925|  0|quickly. bold dep...|null|\n",
      "+---+------+---+---------+----------+--------+---------------+---+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders = spark.read.csv(\"/home/orpington/Desktop/dbgen/tpch-1/orders.tbl\", sep=\"|\")\n",
    "orders.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>_c9</th>\n",
       "      <th>_c10</th>\n",
       "      <th>_c11</th>\n",
       "      <th>_c12</th>\n",
       "      <th>_c13</th>\n",
       "      <th>_c14</th>\n",
       "      <th>_c15</th>\n",
       "      <th>_c16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>6001215</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>3000279.604204982</td>\n",
       "      <td>100017.98932999402</td>\n",
       "      <td>5000.602606138924</td>\n",
       "      <td>3.0005757167506912</td>\n",
       "      <td>25.507967136654827</td>\n",
       "      <td>38255.138484657095</td>\n",
       "      <td>0.049999430115268445</td>\n",
       "      <td>0.04001350893116897</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1732187.8734803204</td>\n",
       "      <td>57735.69082650498</td>\n",
       "      <td>2886.961998730632</td>\n",
       "      <td>1.732431403651942</td>\n",
       "      <td>14.42626253701689</td>\n",
       "      <td>23300.43871096209</td>\n",
       "      <td>0.03161985510812605</td>\n",
       "      <td>0.02581655179884265</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>1992-01-02</td>\n",
       "      <td>1992-01-31</td>\n",
       "      <td>1992-01-04</td>\n",
       "      <td>COLLECT COD</td>\n",
       "      <td>AIR</td>\n",
       "      <td>Tiresias</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>999975</td>\n",
       "      <td>99999</td>\n",
       "      <td>9999</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>99999.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>R</td>\n",
       "      <td>O</td>\n",
       "      <td>1998-12-01</td>\n",
       "      <td>1998-10-31</td>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>TAKE BACK RETURN</td>\n",
       "      <td>TRUCK</td>\n",
       "      <td>zzle? slyly final platelets sleep quickly.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 _c0                 _c1                _c2  \\\n",
       "0   count             6001215             6001215            6001215   \n",
       "1    mean   3000279.604204982  100017.98932999402  5000.602606138924   \n",
       "2  stddev  1732187.8734803204   57735.69082650498  2886.961998730632   \n",
       "3     min                   1                   1                  1   \n",
       "4     max              999975               99999               9999   \n",
       "\n",
       "                  _c3                 _c4                 _c5  \\\n",
       "0             6001215             6001215             6001215   \n",
       "1  3.0005757167506912  25.507967136654827  38255.138484657095   \n",
       "2   1.732431403651942   14.42626253701689   23300.43871096209   \n",
       "3                   1                   1             1000.00   \n",
       "4                   7                   9            99999.50   \n",
       "\n",
       "                    _c6                  _c7      _c8      _c9        _c10  \\\n",
       "0               6001215              6001215  6001215  6001215     6001215   \n",
       "1  0.049999430115268445  0.04001350893116897     None     None        None   \n",
       "2   0.03161985510812605  0.02581655179884265     None     None        None   \n",
       "3                  0.00                 0.00        A        F  1992-01-02   \n",
       "4                  0.10                 0.08        R        O  1998-12-01   \n",
       "\n",
       "         _c11        _c12              _c13     _c14  \\\n",
       "0     6001215     6001215           6001215  6001215   \n",
       "1        None        None              None     None   \n",
       "2        None        None              None     None   \n",
       "3  1992-01-31  1992-01-04       COLLECT COD      AIR   \n",
       "4  1998-10-31  1998-12-31  TAKE BACK RETURN    TRUCK   \n",
       "\n",
       "                                          _c15  _c16  \n",
       "0                                      6001215     0  \n",
       "1                                         None  None  \n",
       "2                                         None  None  \n",
       "3                                    Tiresias   None  \n",
       "4  zzle? slyly final platelets sleep quickly.   None  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(\"/home/orpington/Desktop/dbgen/tpch-1/lineitem.tbl\", sep=\"|\").describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "      <th>_c5</th>\n",
       "      <th>_c6</th>\n",
       "      <th>_c7</th>\n",
       "      <th>_c8</th>\n",
       "      <th>_c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>2999991.5</td>\n",
       "      <td>75006.04057466667</td>\n",
       "      <td>None</td>\n",
       "      <td>151219.5376316404</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1732051.384920602</td>\n",
       "      <td>43304.48900767441</td>\n",
       "      <td>None</td>\n",
       "      <td>88621.43136365115</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1000.55</td>\n",
       "      <td>1992-01-01</td>\n",
       "      <td>1-URGENT</td>\n",
       "      <td>Clerk#000000001</td>\n",
       "      <td>0</td>\n",
       "      <td>Tiresias about the blithely ironic a</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>999975</td>\n",
       "      <td>99998</td>\n",
       "      <td>P</td>\n",
       "      <td>99999.89</td>\n",
       "      <td>1998-08-02</td>\n",
       "      <td>5-LOW</td>\n",
       "      <td>Clerk#000001000</td>\n",
       "      <td>0</td>\n",
       "      <td>zzle? furiously ironic instructions among the ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                _c0                _c1      _c2                _c3  \\\n",
       "0   count            1500000            1500000  1500000            1500000   \n",
       "1    mean          2999991.5  75006.04057466667     None  151219.5376316404   \n",
       "2  stddev  1732051.384920602  43304.48900767441     None  88621.43136365115   \n",
       "3     min                  1                  1        F            1000.55   \n",
       "4     max             999975              99998        P           99999.89   \n",
       "\n",
       "          _c4       _c5              _c6      _c7  \\\n",
       "0     1500000   1500000          1500000  1500000   \n",
       "1        None      None             None      0.0   \n",
       "2        None      None             None      0.0   \n",
       "3  1992-01-01  1-URGENT  Clerk#000000001        0   \n",
       "4  1998-08-02     5-LOW  Clerk#000001000        0   \n",
       "\n",
       "                                                 _c8   _c9  \n",
       "0                                            1500000     0  \n",
       "1                                               None  None  \n",
       "2                                               None  None  \n",
       "3               Tiresias about the blithely ironic a  None  \n",
       "4  zzle? furiously ironic instructions among the ...  None  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "customer_ids = [lit(customer_id) \n",
    "                for cutomer_id \n",
    "                in customers.select(customers[0])\\\n",
    "                            .distinct()\\\n",
    "                            .rdd.flatMap(lambda r: r)\\\n",
    "                            .collect()]\n",
    "\n",
    "customer_is_in_orders = orders.groupby(orders[1].isin(*customer_ids)).count()\n",
    "customer_is_in_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| herp|   merp|\n",
      "+-----+-------+\n",
      "| true|     17|\n",
      "|false|1499983|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_is_in_orders.select(customer_is_in_orders[0].alias(\"herp\"), customer_is_in_orders[1].alias(\"merp\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|is in orders|  count|\n",
      "+------------+-------+\n",
      "|        true|     17|\n",
      "|       false|1499983|\n",
      "+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_is_in_orders.withColumnRenamed(customer_is_in_orders.columns[0], \"is in orders\")\\\n",
    "                     .withColumnRenamed(customer_is_in_orders.columns[1], \"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------+----------+---------------+---------------+---+--------------------+----+\n",
      "|_c0|   _c1|_c2|      _c3|       _c4|            _c5|            _c6|_c7|                 _c8| _c9|\n",
      "+---+------+---+---------+----------+---------------+---------------+---+--------------------+----+\n",
      "|  1| 36901|  O|173665.47|1996-01-02|          5-LOW|Clerk#000000951|  0|nstructions sleep...|null|\n",
      "|  2| 78002|  O| 46929.18|1996-12-01|       1-URGENT|Clerk#000000880|  0| foxes. pending a...|null|\n",
      "|  3|123314|  F|193846.25|1993-10-14|          5-LOW|Clerk#000000955|  0|sly final account...|null|\n",
      "|  4|136777|  O| 32151.78|1995-10-11|          5-LOW|Clerk#000000124|  0|sits. slyly regul...|null|\n",
      "|  5| 44485|  F|144659.20|1994-07-30|          5-LOW|Clerk#000000925|  0|quickly. bold dep...|null|\n",
      "|  6| 55624|  F| 58749.59|1992-02-21|4-NOT SPECIFIED|Clerk#000000058|  0|ggle. special, fi...|null|\n",
      "|  7| 39136|  O|252004.18|1996-01-10|         2-HIGH|Clerk#000000470|  0|ly special requests |null|\n",
      "| 32|130057|  O|208660.75|1995-07-16|         2-HIGH|Clerk#000000616|  0|ise blithely bold...|null|\n",
      "| 33| 66958|  F|163243.98|1993-10-27|       3-MEDIUM|Clerk#000000409|  0|uriously. furious...|null|\n",
      "| 34| 61001|  O| 58949.67|1998-07-21|       3-MEDIUM|Clerk#000000223|  0|ly final packages...|null|\n",
      "| 35|127588|  O|253724.56|1995-10-23|4-NOT SPECIFIED|Clerk#000000259|  0|zzle. carefully e...|null|\n",
      "| 36|115252|  O| 68289.96|1995-11-03|       1-URGENT|Clerk#000000358|  0| quick packages a...|null|\n",
      "| 37| 86116|  F|206680.66|1992-06-03|       3-MEDIUM|Clerk#000000456|  0|kly regular pinto...|null|\n",
      "| 38|124828|  O| 82500.05|1996-08-21|4-NOT SPECIFIED|Clerk#000000604|  0|haggle blithely. ...|null|\n",
      "| 39| 81763|  O|341734.47|1996-09-20|       3-MEDIUM|Clerk#000000659|  0|ole express, iron...|null|\n",
      "| 64| 32113|  F| 39414.99|1994-07-16|       3-MEDIUM|Clerk#000000661|  0|wake fluffily. so...|null|\n",
      "| 65| 16252|  P|110643.60|1995-03-18|       1-URGENT|Clerk#000000632|  0|ular requests are...|null|\n",
      "| 66|129200|  F|103740.67|1994-01-20|          5-LOW|Clerk#000000743|  0|y pending request...|null|\n",
      "| 67| 56614|  O|169405.01|1996-12-19|4-NOT SPECIFIED|Clerk#000000547|  0|symptotes haggle ...|null|\n",
      "| 68| 28547|  O|330793.52|1998-04-18|       3-MEDIUM|Clerk#000000440|  0| pinto beans slee...|null|\n",
      "+---+------+---+---------+----------+---------------+---------------+---+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.join(customers, customers[0] == orders[1], \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CUSTOMER',\n",
       " 'LINEITEM',\n",
       " 'PARTSUPP',\n",
       " 'REGION',\n",
       " 'NATION',\n",
       " 'PART',\n",
       " 'SUPPLIER',\n",
       " 'ORDERS']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemas = spark.read\\\n",
    "               .csv(\"/home/orpington/Desktop/dbgen/schema.csv\")\\\n",
    "               .selectExpr(\"encode(_c1, 'UTF-8') as table\", \n",
    "                           \"encode(_c2, 'UTF-8') as col\")\\\n",
    "               .groupby(\"table\")\\\n",
    "               .agg({\"col\":\"collect_list\"})\\\n",
    "               .rdd.map(lambda r: (str(r[0]), [str(e) for e in r[1]]))\\\n",
    "               .collectAsMap()\n",
    "schemas.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = spark.createDataFrame(orders.rdd, \n",
    "                                  schema=schemas[\"ORDERS\"], \n",
    "                                  samplingRatio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ORDERKEY    CUSTKEY  ORDERSTATUS      TOTALPRICE  ORDERDATE    ORDERPRIORITY    CLERK\n",
      "----------  ---------  -------------  ------------  -----------  ---------------  ---------------\n",
      "         1      36901  O                  173665    1996-01-02   5-LOW            Clerk#000000951\n",
      "         2      78002  O                   46929.2  1996-12-01   1-URGENT         Clerk#000000880\n",
      "         3     123314  F                  193846    1993-10-14   5-LOW            Clerk#000000955\n",
      "         4     136777  O                   32151.8  1995-10-11   5-LOW            Clerk#000000124\n",
      "         5      44485  F                  144659    1994-07-30   5-LOW            Clerk#000000925\n",
      "\n",
      "  SHIPPRIORITY  COMMENT\n",
      "--------------  -------------------------------------------------------------------------\n",
      "             0  nstructions sleep furiously among\n",
      "             0  foxes. pending accounts at the pending, silent asymptot\n",
      "             0  sly final accounts boost. carefully regular ideas cajole carefully. depos\n",
      "             0  sits. slyly regular warthogs cajole. regular, regular theodolites acro\n",
      "             0  quickly. bold deposits sleep slyly. packages use slyly\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate \n",
    "print(tabulate(df_orders.select(*df_orders.columns[:7]).limit(5).collect(), df_orders.columns[:7]))\n",
    "print\n",
    "print(tabulate(df_orders.select(*df_orders.columns[7:-1]).limit(5).collect(), df_orders.columns[7:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers = spark.createDataFrame(customers.rdd, \n",
    "                                     schema=schemas[\"CUSTOMER\"], \n",
    "                                     samplingRatio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CUSTKEY  NAME                ADDRESS                           NATIONKEY  PHONE              ACCTBAL\n",
      "---------  ------------------  ------------------------------  -----------  ---------------  ---------\n",
      "        1  Customer#000000001  IVhzIApeRb ot,c,E                        15  25-989-741-2988     711.56\n",
      "        2  Customer#000000002  XSTf4,NCwDVaWNe6tEgvwfmRchLXak           13  23-768-687-3665     121.65\n",
      "        3  Customer#000000003  MG9kdTD2WBHm                              1  11-719-748-3364    7498.12\n",
      "        4  Customer#000000004  XxVSJsLAGtn                               4  14-128-190-5944    2866.83\n",
      "        5  Customer#000000005  KvpyuHCplrB84WgAiGV6sYpZq7Tj              3  13-750-942-6364     794.47\n",
      "\n",
      "MKTSEGMENT    COMMENT\n",
      "------------  -----------------------------------------------------------------------------------------------------\n",
      "BUILDING      to the even, regular platelets. regular, ironic epitaphs nag e\n",
      "AUTOMOBILE    l accounts. blithely ironic theodolites integrate boldly: caref\n",
      "AUTOMOBILE    deposits eat slyly ironic, even instructions. express foxes detect slyly. blithely even accounts abov\n",
      "MACHINERY     requests. final, regular ideas sleep final accou\n",
      "HOUSEHOLD     n accounts will have to unwind. foxes cajole accor\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate \n",
    "columns_up = df_customers.columns[:6]\n",
    "columns_down = df_customers.columns[6:-1]\n",
    "df_part = lambda cols: df_customers.select(*cols).limit(5).collect()\n",
    "for_tabulate = lambda cols: [df_part(cols), cols] \n",
    "do_tabulate = lambda cols: tabulate(*for_tabulate(cols))\n",
    "\n",
    "print(do_tabulate(columns_up))\n",
    "print\n",
    "print(do_tabulate(columns_down))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Function(name=u'!', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name=u'%', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
       " Function(name=u'&', description=None, className=u'org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
       " Function(name=u'*', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
       " Function(name=u'+', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
       " Function(name=u'-', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
       " Function(name=u'/', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
       " Function(name=u'<', description=None, className=u'org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
       " Function(name=u'<=', description=None, className=u'org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
       " Function(name=u'<=>', description=None, className=u'org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
       " Function(name=u'=', description=None, className=u'org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name=u'==', description=None, className=u'org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
       " Function(name=u'>', description=None, className=u'org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
       " Function(name=u'>=', description=None, className=u'org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
       " Function(name=u'^', description=None, className=u'org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
       " Function(name=u'abs', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
       " Function(name=u'acos', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
       " Function(name=u'add_months', description=None, className=u'org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
       " Function(name=u'and', description=None, className=u'org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
       " Function(name=u'approx_count_distinct', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
       " Function(name=u'approx_percentile', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name=u'array', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
       " Function(name=u'array_contains', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
       " Function(name=u'ascii', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
       " Function(name=u'asin', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
       " Function(name=u'assert_true', description=None, className=u'org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
       " Function(name=u'atan', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
       " Function(name=u'atan2', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
       " Function(name=u'avg', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name=u'base64', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
       " Function(name=u'bigint', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'bin', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
       " Function(name=u'binary', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'boolean', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'bround', description=None, className=u'org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
       " Function(name=u'cast', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'cbrt', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
       " Function(name=u'ceil', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name=u'ceiling', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
       " Function(name=u'coalesce', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
       " Function(name=u'collect_list', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
       " Function(name=u'collect_set', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
       " Function(name=u'concat', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
       " Function(name=u'concat_ws', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
       " Function(name=u'conv', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
       " Function(name=u'corr', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
       " Function(name=u'cos', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
       " Function(name=u'cosh', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
       " Function(name=u'count', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
       " Function(name=u'count_min_sketch', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
       " Function(name=u'covar_pop', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
       " Function(name=u'covar_samp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
       " Function(name=u'crc32', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
       " Function(name=u'cube', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cube', isTemporary=True),\n",
       " Function(name=u'cume_dist', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
       " Function(name=u'current_database', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
       " Function(name=u'current_date', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
       " Function(name=u'current_timestamp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name=u'date', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'date_add', description=None, className=u'org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
       " Function(name=u'date_format', description=None, className=u'org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
       " Function(name=u'date_sub', description=None, className=u'org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
       " Function(name=u'datediff', description=None, className=u'org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
       " Function(name=u'day', description=None, className=u'org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name=u'dayofmonth', description=None, className=u'org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
       " Function(name=u'dayofyear', description=None, className=u'org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
       " Function(name=u'decimal', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'decode', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
       " Function(name=u'degrees', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
       " Function(name=u'dense_rank', description=None, className=u'org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
       " Function(name=u'double', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'e', description=None, className=u'org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
       " Function(name=u'elt', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
       " Function(name=u'encode', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
       " Function(name=u'exp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
       " Function(name=u'explode', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name=u'explode_outer', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
       " Function(name=u'expm1', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
       " Function(name=u'factorial', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
       " Function(name=u'find_in_set', description=None, className=u'org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
       " Function(name=u'first', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name=u'first_value', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
       " Function(name=u'float', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'floor', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True),\n",
       " Function(name=u'format_number', description=None, className=u'org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
       " Function(name=u'format_string', description=None, className=u'org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name=u'from_json', description=None, className=u'org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
       " Function(name=u'from_unixtime', description=None, className=u'org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
       " Function(name=u'from_utc_timestamp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
       " Function(name=u'get_json_object', description=None, className=u'org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
       " Function(name=u'greatest', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
       " Function(name=u'grouping', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
       " Function(name=u'grouping_id', description=None, className=u'org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
       " Function(name=u'hash', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
       " Function(name=u'hex', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
       " Function(name=u'hour', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
       " Function(name=u'hypot', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
       " Function(name=u'if', description=None, className=u'org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
       " Function(name=u'ifnull', description=None, className=u'org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True),\n",
       " Function(name=u'in', description=None, className=u'org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
       " Function(name=u'initcap', description=None, className=u'org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
       " Function(name=u'inline', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name=u'inline_outer', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
       " Function(name=u'input_file_block_length', description=None, className=u'org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
       " Function(name=u'input_file_block_start', description=None, className=u'org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
       " Function(name=u'input_file_name', description=None, className=u'org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
       " Function(name=u'instr', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
       " Function(name=u'int', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'isnan', description=None, className=u'org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
       " Function(name=u'isnotnull', description=None, className=u'org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
       " Function(name=u'isnull', description=None, className=u'org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
       " Function(name=u'java_method', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name=u'json_tuple', description=None, className=u'org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
       " Function(name=u'kurtosis', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
       " Function(name=u'lag', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
       " Function(name=u'last', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name=u'last_day', description=None, className=u'org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
       " Function(name=u'last_value', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
       " Function(name=u'lcase', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name=u'lead', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
       " Function(name=u'least', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
       " Function(name=u'length', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
       " Function(name=u'levenshtein', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
       " Function(name=u'like', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
       " Function(name=u'ln', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
       " Function(name=u'locate', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
       " Function(name=u'log', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
       " Function(name=u'log10', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
       " Function(name=u'log1p', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
       " Function(name=u'log2', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
       " Function(name=u'lower', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
       " Function(name=u'lpad', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True),\n",
       " Function(name=u'ltrim', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
       " Function(name=u'map', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
       " Function(name=u'map_keys', description=None, className=u'org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
       " Function(name=u'map_values', description=None, className=u'org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
       " Function(name=u'max', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
       " Function(name=u'md5', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
       " Function(name=u'mean', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
       " Function(name=u'min', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
       " Function(name=u'minute', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
       " Function(name=u'monotonically_increasing_id', description=None, className=u'org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
       " Function(name=u'month', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
       " Function(name=u'months_between', description=None, className=u'org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
       " Function(name=u'named_struct', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
       " Function(name=u'nanvl', description=None, className=u'org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
       " Function(name=u'negative', description=None, className=u'org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
       " Function(name=u'next_day', description=None, className=u'org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
       " Function(name=u'not', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
       " Function(name=u'now', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
       " Function(name=u'ntile', description=None, className=u'org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
       " Function(name=u'nullif', description=None, className=u'org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
       " Function(name=u'nvl', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
       " Function(name=u'nvl2', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
       " Function(name=u'or', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
       " Function(name=u'parse_url', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
       " Function(name=u'percent_rank', description=None, className=u'org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
       " Function(name=u'percentile', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
       " Function(name=u'percentile_approx', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
       " Function(name=u'pi', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
       " Function(name=u'pmod', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
       " Function(name=u'posexplode', description=None, className=u'org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name=u'posexplode_outer', description=None, className=u'org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
       " Function(name=u'positive', description=None, className=u'org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
       " Function(name=u'pow', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name=u'power', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
       " Function(name=u'printf', description=None, className=u'org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
       " Function(name=u'quarter', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
       " Function(name=u'radians', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
       " Function(name=u'rand', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
       " Function(name=u'randn', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
       " Function(name=u'rank', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
       " Function(name=u'reflect', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
       " Function(name=u'regexp_extract', description=None, className=u'org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
       " Function(name=u'regexp_replace', description=None, className=u'org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
       " Function(name=u'repeat', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
       " Function(name=u'reverse', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringReverse', isTemporary=True),\n",
       " Function(name=u'rint', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
       " Function(name=u'rlike', description=None, className=u'org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
       " Function(name=u'rollup', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Rollup', isTemporary=True),\n",
       " Function(name=u'round', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
       " Function(name=u'row_number', description=None, className=u'org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
       " Function(name=u'rpad', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True),\n",
       " Function(name=u'rtrim', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
       " Function(name=u'second', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
       " Function(name=u'sentences', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
       " Function(name=u'sha', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name=u'sha1', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
       " Function(name=u'sha2', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
       " Function(name=u'shiftleft', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
       " Function(name=u'shiftright', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
       " Function(name=u'shiftrightunsigned', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
       " Function(name=u'sign', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name=u'signum', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
       " Function(name=u'sin', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
       " Function(name=u'sinh', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
       " Function(name=u'size', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
       " Function(name=u'skewness', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
       " Function(name=u'smallint', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'sort_array', description=None, className=u'org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
       " Function(name=u'soundex', description=None, className=u'org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
       " Function(name=u'space', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
       " Function(name=u'spark_partition_id', description=None, className=u'org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
       " Function(name=u'split', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
       " Function(name=u'sqrt', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
       " Function(name=u'stack', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
       " Function(name=u'std', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name=u'stddev', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name=u'stddev_pop', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
       " Function(name=u'stddev_samp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
       " Function(name=u'str_to_map', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
       " Function(name=u'string', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'struct', description=None, className=u'org.apache.spark.sql.catalyst.expressions.NamedStruct', isTemporary=True),\n",
       " Function(name=u'substr', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name=u'substring', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
       " Function(name=u'substring_index', description=None, className=u'org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
       " Function(name=u'sum', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
       " Function(name=u'tan', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
       " Function(name=u'tanh', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
       " Function(name=u'timestamp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'tinyint', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
       " Function(name=u'to_date', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
       " Function(name=u'to_json', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
       " Function(name=u'to_timestamp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
       " Function(name=u'to_unix_timestamp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
       " Function(name=u'to_utc_timestamp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
       " Function(name=u'translate', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
       " Function(name=u'trim', description=None, className=u'org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
       " Function(name=u'trunc', description=None, className=u'org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
       " Function(name=u'ucase', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name=u'unbase64', description=None, className=u'org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
       " Function(name=u'unhex', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
       " Function(name=u'unix_timestamp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
       " Function(name=u'upper', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
       " Function(name=u'var_pop', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
       " Function(name=u'var_samp', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name=u'variance', description=None, className=u'org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
       " Function(name=u'weekofyear', description=None, className=u'org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
       " Function(name=u'when', description=None, className=u'org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
       " Function(name=u'window', description=None, className=u'org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
       " Function(name=u'xpath', description=None, className=u'org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
       " Function(name=u'xpath_boolean', description=None, className=u'org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
       " Function(name=u'xpath_double', description=None, className=u'org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name=u'xpath_float', description=None, className=u'org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
       " Function(name=u'xpath_int', description=None, className=u'org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
       " Function(name=u'xpath_long', description=None, className=u'org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
       " Function(name=u'xpath_number', description=None, className=u'org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
       " Function(name=u'xpath_short', description=None, className=u'org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
       " Function(name=u'xpath_string', description=None, className=u'org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
       " Function(name=u'year', description=None, className=u'org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
       " Function(name=u'|', description=None, className=u'org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
       " Function(name=u'~', description=None, className=u'org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|CUSTKEY|\n",
      "+-------+\n",
      "|     26|\n",
      "|     29|\n",
      "|    474|\n",
      "|    964|\n",
      "|   1677|\n",
      "|   1697|\n",
      "|   1806|\n",
      "|   1950|\n",
      "|   2040|\n",
      "|   2214|\n",
      "|   2250|\n",
      "|   2453|\n",
      "|   2509|\n",
      "|   2529|\n",
      "|   2927|\n",
      "|   3091|\n",
      "|   3506|\n",
      "|   3764|\n",
      "|   4590|\n",
      "|   4823|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.select(df_customers[0]).distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|CUSTKEY|\n",
      "+-------+\n",
      "|      1|\n",
      "|      2|\n",
      "|      4|\n",
      "|      5|\n",
      "|      7|\n",
      "|      8|\n",
      "|     10|\n",
      "|     11|\n",
      "|     13|\n",
      "|     14|\n",
      "|     16|\n",
      "|     17|\n",
      "|     19|\n",
      "|     20|\n",
      "|     22|\n",
      "|     23|\n",
      "|     25|\n",
      "|     26|\n",
      "|     28|\n",
      "|     29|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.select(df_orders[1].cast(\"int\").alias(\"CUSTKEY\"))\\\n",
    "         .distinct()\\\n",
    "         .sort(\"CUSTKEY\")\\\n",
    "         .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_semi: 99996\n",
      "left_anti: 50004\n"
     ]
    }
   ],
   "source": [
    "cust_ids = df_customers.select(df_customers[0]).distinct()\n",
    "order_cust_ids = df_orders.select(df_orders[1].cast(\"int\").alias(\"CUSTKEY\")).distinct()\n",
    "print(\"left_semi: {}\".format(cust_ids.join(order_cust_ids, \n",
    "                                           order_cust_ids[0] == cust_ids[0], \n",
    "                                           \"left_semi\").count()))\n",
    "print(\"left_anti: {}\".format(cust_ids.join(order_cust_ids, \n",
    "                                           order_cust_ids[0] == cust_ids[0], \n",
    "                                           \"left_anti\").count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "df_orders.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'customers', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True),\n",
       " Table(name=u'orders', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1650000"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select CUSTKEY from customers\").union(spark.sql(\"select CUSTKEY from orders\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 1500000|\n",
      "|   99996|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*) from orders\n",
    "where CUSTKEY in (select CUSTKEY from customers)\n",
    "union all\n",
    "select count(*) from customers\n",
    "where CUSTKEY in (select CUSTKEY from orders)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|        |customers|       true|\n",
      "|        |   orders|       true|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NATIONKEY STRING,\n",
      "NAME STRING,\n",
      "REGIONKEY STRING,\n",
      "COMMENT STRING)\n"
     ]
    }
   ],
   "source": [
    "print(\"(\" + \" STRING,\\n\".join(schemas[\"NATION\"]) + \" STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer.tbl  metastore_db  partsupp.tbl    region.tbl\r\n",
      "derby.log     nation.tbl    part.tbl\t    supplier.tbl\r\n",
      "lineitem.tbl  orders.tbl    region_bad.tbl\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/orpington/Desktop/dbgen/tpch-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 1))\n",
      "\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "u\"\\nno viable alternative at input 'CREATE OR REPLACE TABLE'(line 2, pos 18)\\n\\n== SQL ==\\n\\nCREATE OR REPLACE TABLE nation (\\n------------------^^^\\nNATIONKEY STRING,\\nNAME STRING,\\nREGIONKEY STRING,\\nCOMMENT STRING)\\nUSING csv\\nOPTIONS (\\n  inferSchema true,\\n  header true,\\n  sep '|',\\n  path '/home/orpington/Desktop/dbgen/tpch-1/nation.tbl'\\n)\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-2ac68011bd76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0msep\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m'/home/orpington/Desktop/dbgen/tpch-1/nation.tbl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m )\"\"\")\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: u\"\\nno viable alternative at input 'CREATE OR REPLACE TABLE'(line 2, pos 18)\\n\\n== SQL ==\\n\\nCREATE OR REPLACE TABLE nation (\\n------------------^^^\\nNATIONKEY STRING,\\nNAME STRING,\\nREGIONKEY STRING,\\nCOMMENT STRING)\\nUSING csv\\nOPTIONS (\\n  inferSchema true,\\n  header true,\\n  sep '|',\\n  path '/home/orpington/Desktop/dbgen/tpch-1/nation.tbl'\\n)\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE nation (\n",
    "NATIONKEY STRING,\n",
    "NAME STRING,\n",
    "REGIONKEY STRING,\n",
    "COMMENT STRING)\n",
    "USING csv\n",
    "OPTIONS (\n",
    "  inferSchema true,\n",
    "  header true,\n",
    "  sep '|',\n",
    "  path '/home/orpington/Desktop/dbgen/tpch-1/nation.tbl'\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'nation', database=u'default', description=None, tableType=u'EXTERNAL', isTemporary=False),\n",
       " Table(name=u'customers', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True),\n",
       " Table(name=u'orders', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+-------+\n",
      "|           NATIONKEY|                NAME|REGIONKEY|COMMENT|\n",
      "+--------------------+--------------------+---------+-------+\n",
      "|1|ARGENTINA|1|al ...|                null|     null|   null|\n",
      "|2|BRAZIL|1|y alon...|                null|     null|   null|\n",
      "|3|CANADA|1|eas ha...| silent packages....|     null|   null|\n",
      "|4|EGYPT|4|y above...|                null|     null|   null|\n",
      "|5|ETHIOPIA|0|ven ...|                null|     null|   null|\n",
      "+--------------------+--------------------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nation limit 10\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.uncacheTable(\"nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'nation', database=u'default', description=None, tableType=u'EXTERNAL', isTemporary=False),\n",
       " Table(name=u'customers', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True),\n",
       " Table(name=u'orders', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'customers', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True),\n",
       " Table(name=u'orders', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE nation (\n",
    "NATIONKEY STRING,\n",
    "NAME STRING,\n",
    "REGIONKEY STRING,\n",
    "COMMENT STRING)\n",
    "USING csv\n",
    "OPTIONS (\n",
    "  inferSchema true,\n",
    "  header true,\n",
    "  sep '|',\n",
    "  path '/home/orpington/Desktop/dbgen/tpch-1/nation.tbl'\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+--------------------+\n",
      "|NATIONKEY|     NAME|REGIONKEY|             COMMENT|\n",
      "+---------+---------+---------+--------------------+\n",
      "|        1|ARGENTINA|        1|al foxes promise ...|\n",
      "|        2|   BRAZIL|        1|y alongside of th...|\n",
      "|        3|   CANADA|        1|eas hang ironic, ...|\n",
      "|        4|    EGYPT|        4|y above the caref...|\n",
      "|        5| ETHIOPIA|        0|ven packages wake...|\n",
      "+---------+---------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nation limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/orpington/Desktop/dbgen/tpch-1/nation.tbl > /tmp/nation_tmp.tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE nation_tmp (\n",
    "NATIONKEY STRING,\n",
    "NAME STRING,\n",
    "REGIONKEY STRING,\n",
    "COMMENT STRING)\n",
    "USING csv\n",
    "OPTIONS (\n",
    "  inferSchema true,\n",
    "  header true,\n",
    "  sep '|',\n",
    "  path '/tmp/nation_tmp.tbl'\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from nation_tmp\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/orpington/Desktop/dbgen/tpch-1/nation.tbl >> /tmp/nation_tmp.tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from nation_tmp\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.uncacheTable(\"nation_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from nation_tmp\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 /tmp/nation_tmp.tbl\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l /tmp/nation_tmp.tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"Hive support is required to CREATE Hive TABLE (AS SELECT);;\\n'CreateTable `nation_tmp_2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\\n+- Project [NATIONKEY#5809, NAME#5810, REGIONKEY#5811, COMMENT#5812]\\n   +- SubqueryAlias nation_tmp\\n      +- Relation[NATIONKEY#5809,NAME#5810,REGIONKEY#5811,COMMENT#5812] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-2a24844469e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create table nation_tmp_2 as select * from nation_tmp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"Hive support is required to CREATE Hive TABLE (AS SELECT);;\\n'CreateTable `nation_tmp_2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\\n+- Project [NATIONKEY#5809, NAME#5810, REGIONKEY#5811, COMMENT#5812]\\n   +- SubqueryAlias nation_tmp\\n      +- Relation[NATIONKEY#5809,NAME#5810,REGIONKEY#5811,COMMENT#5812] csv\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"create table nation_tmp_2 as select * from nation_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 1))\n",
      "\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "u\"\\nmismatched input 'OPTIONS' expecting <EOF>(line 9, pos 0)\\n\\n== SQL ==\\n\\nCREATE TABLE nation_partitioned (\\nNATIONKEY STRING,\\nNAME STRING,\\nREGIONKEY STRING,\\nCOMMENT STRING)\\nUSING csv\\nPARTITIONED BY (REGIONKEY)\\nOPTIONS (\\n^^^\\n  inferSchema true,\\n  header true,\\n  sep '|',\\n  path '/tmp/nation_tmp.tbl'\\n)\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-222-44b39ba5399f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0msep\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m'/tmp/nation_tmp.tbl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m )\"\"\")\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: u\"\\nmismatched input 'OPTIONS' expecting <EOF>(line 9, pos 0)\\n\\n== SQL ==\\n\\nCREATE TABLE nation_partitioned (\\nNATIONKEY STRING,\\nNAME STRING,\\nREGIONKEY STRING,\\nCOMMENT STRING)\\nUSING csv\\nPARTITIONED BY (REGIONKEY)\\nOPTIONS (\\n^^^\\n  inferSchema true,\\n  header true,\\n  sep '|',\\n  path '/tmp/nation_tmp.tbl'\\n)\\n\""
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE nation_partitioned (\n",
    "NATIONKEY STRING,\n",
    "NAME STRING,\n",
    "REGIONKEY STRING,\n",
    "COMMENT STRING)\n",
    "USING csv\n",
    "PARTITIONED BY (REGIONKEY)\n",
    "OPTIONS (\n",
    "  inferSchema true,\n",
    "  header true,\n",
    "  sep '|',\n",
    "  path '/tmp/nation_tmp.tbl'\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "| col_name|data_type|comment|\n",
      "+---------+---------+-------+\n",
      "|NATIONKEY|   string|   null|\n",
      "|     NAME|   string|   null|\n",
      "|REGIONKEY|   string|   null|\n",
      "|  COMMENT|   string|   null|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table nation_tmp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"REFRESH table nation_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Operation not allowed: MSCK REPAIR TABLE only works on partitioned tables: `default`.`nation_tmp`;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-225-aa9ebfc437c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSCK REPAIR TABLE nation_tmp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Operation not allowed: MSCK REPAIR TABLE only works on partitioned tables: `default`.`nation_tmp`;'"
     ]
    }
   ],
   "source": [
    "spark.sql(\"MSCK REPAIR TABLE nation_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\n",
      "*FileScan csv default.nation_tmp[NATIONKEY#5870,NAME#5871,REGIONKEY#5872,COMMENT#5873] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tmp/nation_tmp.tbl], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<NATIONKEY:string,NAME:string,REGIONKEY:string,COMMENT:string>|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"EXPLAIN SELECT * FROM nation_tmp\").show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"create database herp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|        herp|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use herp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Table or view not found: nation_tmp; line 1 pos 14'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-f8880547de1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from nation_tmp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Table or view not found: nation_tmp; line 1 pos 14'"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nation_tmp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+---------+--------------------+\n",
      "|NATIONKEY|        NAME|REGIONKEY|             COMMENT|\n",
      "+---------+------------+---------+--------------------+\n",
      "|        1|   ARGENTINA|        1|al foxes promise ...|\n",
      "|        2|      BRAZIL|        1|y alongside of th...|\n",
      "|        3|      CANADA|        1|eas hang ironic, ...|\n",
      "|        4|       EGYPT|        4|y above the caref...|\n",
      "|        5|    ETHIOPIA|        0|ven packages wake...|\n",
      "|        6|      FRANCE|        3|refully final req...|\n",
      "|        7|     GERMANY|        3|l platelets. regu...|\n",
      "|        8|       INDIA|        2|ss excuses cajole...|\n",
      "|        9|   INDONESIA|        2| slyly express as...|\n",
      "|       10|        IRAN|        4|efully alongside ...|\n",
      "|       11|        IRAQ|        4|nic deposits boos...|\n",
      "|       12|       JAPAN|        2|ously. final, exp...|\n",
      "|       13|      JORDAN|        4|ic deposits are b...|\n",
      "|       14|       KENYA|        0| pending excuses ...|\n",
      "|       15|     MOROCCO|        0|rns. blithely bol...|\n",
      "|       16|  MOZAMBIQUE|        0|s. ironic, unusua...|\n",
      "|       17|        PERU|        1|platelets. blithe...|\n",
      "|       18|       CHINA|        2|c dependencies. f...|\n",
      "|       19|     ROMANIA|        3|ular asymptotes a...|\n",
      "|       20|SAUDI ARABIA|        4|ts. silent reques...|\n",
      "+---------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.nation_tmp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select current_database()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop database if exists herp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.createGlobalTempView(\"orders_global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------+----------+---------------+---------------+---+--------------------+----+\n",
      "|_c0|   _c1|_c2|      _c3|       _c4|            _c5|            _c6|_c7|                 _c8| _c9|\n",
      "+---+------+---+---------+----------+---------------+---------------+---+--------------------+----+\n",
      "|  1| 36901|  O|173665.47|1996-01-02|          5-LOW|Clerk#000000951|  0|nstructions sleep...|null|\n",
      "|  2| 78002|  O| 46929.18|1996-12-01|       1-URGENT|Clerk#000000880|  0| foxes. pending a...|null|\n",
      "|  3|123314|  F|193846.25|1993-10-14|          5-LOW|Clerk#000000955|  0|sly final account...|null|\n",
      "|  4|136777|  O| 32151.78|1995-10-11|          5-LOW|Clerk#000000124|  0|sits. slyly regul...|null|\n",
      "|  5| 44485|  F|144659.20|1994-07-30|          5-LOW|Clerk#000000925|  0|quickly. bold dep...|null|\n",
      "|  6| 55624|  F| 58749.59|1992-02-21|4-NOT SPECIFIED|Clerk#000000058|  0|ggle. special, fi...|null|\n",
      "|  7| 39136|  O|252004.18|1996-01-10|         2-HIGH|Clerk#000000470|  0|ly special requests |null|\n",
      "| 32|130057|  O|208660.75|1995-07-16|         2-HIGH|Clerk#000000616|  0|ise blithely bold...|null|\n",
      "| 33| 66958|  F|163243.98|1993-10-27|       3-MEDIUM|Clerk#000000409|  0|uriously. furious...|null|\n",
      "| 34| 61001|  O| 58949.67|1998-07-21|       3-MEDIUM|Clerk#000000223|  0|ly final packages...|null|\n",
      "| 35|127588|  O|253724.56|1995-10-23|4-NOT SPECIFIED|Clerk#000000259|  0|zzle. carefully e...|null|\n",
      "| 36|115252|  O| 68289.96|1995-11-03|       1-URGENT|Clerk#000000358|  0| quick packages a...|null|\n",
      "| 37| 86116|  F|206680.66|1992-06-03|       3-MEDIUM|Clerk#000000456|  0|kly regular pinto...|null|\n",
      "| 38|124828|  O| 82500.05|1996-08-21|4-NOT SPECIFIED|Clerk#000000604|  0|haggle blithely. ...|null|\n",
      "| 39| 81763|  O|341734.47|1996-09-20|       3-MEDIUM|Clerk#000000659|  0|ole express, iron...|null|\n",
      "| 64| 32113|  F| 39414.99|1994-07-16|       3-MEDIUM|Clerk#000000661|  0|wake fluffily. so...|null|\n",
      "| 65| 16252|  P|110643.60|1995-03-18|       1-URGENT|Clerk#000000632|  0|ular requests are...|null|\n",
      "| 66|129200|  F|103740.67|1994-01-20|          5-LOW|Clerk#000000743|  0|y pending request...|null|\n",
      "| 67| 56614|  O|169405.01|1996-12-19|4-NOT SPECIFIED|Clerk#000000547|  0|symptotes haggle ...|null|\n",
      "| 68| 28547|  O|330793.52|1998-04-18|       3-MEDIUM|Clerk#000000440|  0| pinto beans slee...|null|\n",
      "+---+------+---+---------+----------+---------------+---------------+---+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.orders_global\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---------+----------+---------------+---------------+---+--------------------+----+\n",
      "|_c0|   _c1|_c2|      _c3|       _c4|            _c5|            _c6|_c7|                 _c8| _c9|\n",
      "+---+------+---+---------+----------+---------------+---------------+---+--------------------+----+\n",
      "|  1| 36901|  O|173665.47|1996-01-02|          5-LOW|Clerk#000000951|  0|nstructions sleep...|null|\n",
      "|  2| 78002|  O| 46929.18|1996-12-01|       1-URGENT|Clerk#000000880|  0| foxes. pending a...|null|\n",
      "|  3|123314|  F|193846.25|1993-10-14|          5-LOW|Clerk#000000955|  0|sly final account...|null|\n",
      "|  4|136777|  O| 32151.78|1995-10-11|          5-LOW|Clerk#000000124|  0|sits. slyly regul...|null|\n",
      "|  5| 44485|  F|144659.20|1994-07-30|          5-LOW|Clerk#000000925|  0|quickly. bold dep...|null|\n",
      "|  6| 55624|  F| 58749.59|1992-02-21|4-NOT SPECIFIED|Clerk#000000058|  0|ggle. special, fi...|null|\n",
      "|  7| 39136|  O|252004.18|1996-01-10|         2-HIGH|Clerk#000000470|  0|ly special requests |null|\n",
      "| 32|130057|  O|208660.75|1995-07-16|         2-HIGH|Clerk#000000616|  0|ise blithely bold...|null|\n",
      "| 33| 66958|  F|163243.98|1993-10-27|       3-MEDIUM|Clerk#000000409|  0|uriously. furious...|null|\n",
      "| 34| 61001|  O| 58949.67|1998-07-21|       3-MEDIUM|Clerk#000000223|  0|ly final packages...|null|\n",
      "| 35|127588|  O|253724.56|1995-10-23|4-NOT SPECIFIED|Clerk#000000259|  0|zzle. carefully e...|null|\n",
      "| 36|115252|  O| 68289.96|1995-11-03|       1-URGENT|Clerk#000000358|  0| quick packages a...|null|\n",
      "| 37| 86116|  F|206680.66|1992-06-03|       3-MEDIUM|Clerk#000000456|  0|kly regular pinto...|null|\n",
      "| 38|124828|  O| 82500.05|1996-08-21|4-NOT SPECIFIED|Clerk#000000604|  0|haggle blithely. ...|null|\n",
      "| 39| 81763|  O|341734.47|1996-09-20|       3-MEDIUM|Clerk#000000659|  0|ole express, iron...|null|\n",
      "| 64| 32113|  F| 39414.99|1994-07-16|       3-MEDIUM|Clerk#000000661|  0|wake fluffily. so...|null|\n",
      "| 65| 16252|  P|110643.60|1995-03-18|       1-URGENT|Clerk#000000632|  0|ular requests are...|null|\n",
      "| 66|129200|  F|103740.67|1994-01-20|          5-LOW|Clerk#000000743|  0|y pending request...|null|\n",
      "| 67| 56614|  O|169405.01|1996-12-19|4-NOT SPECIFIED|Clerk#000000547|  0|symptotes haggle ...|null|\n",
      "| 68| 28547|  O|330793.52|1998-04-18|       3-MEDIUM|Clerk#000000440|  0| pinto beans slee...|null|\n",
      "+---+------+---+---------+----------+---------------+---------------+---+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.newSession().sql(\"select * from global_temp.orders_global\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

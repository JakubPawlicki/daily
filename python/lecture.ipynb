{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|test|\n",
      "+---+----+\n",
      "|  0|   4|\n",
      "|  1|   4|\n",
      "|  2|   4|\n",
      "|  3|   4|\n",
      "|  4|   4|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select 2 + 2 as test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id from range(5)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|col_one|col_two|\n",
      "+-------+-------+\n",
      "|      1|    ddd|\n",
      "|      2|    fff|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([(1, \"ddd\"), (2, \"fff\")], [\"col_one\",\"col_two\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 'aaaaaaaaaaaaaaaaaaaa'),\n",
       " (21, 'aaaaaaaaaaaaaaaaaaaaa'),\n",
       " (22, 'aaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (23, 'aaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (24, 'aaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (25, 'aaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (26, 'aaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (27, 'aaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (28, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (29, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (30, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (31, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (32, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (33, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (34, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (35, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (36, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (37, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (38, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'),\n",
       " (39, 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = [(num, \"a\" * num) for num in range(20, 40)]\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| _1|                  _2|\n",
      "+---+--------------------+\n",
      "| 20|aaaaaaaaaaaaaaaaaaaa|\n",
      "| 21|aaaaaaaaaaaaaaaaa...|\n",
      "| 22|aaaaaaaaaaaaaaaaa...|\n",
      "| 23|aaaaaaaaaaaaaaaaa...|\n",
      "| 24|aaaaaaaaaaaaaaaaa...|\n",
      "| 25|aaaaaaaaaaaaaaaaa...|\n",
      "| 26|aaaaaaaaaaaaaaaaa...|\n",
      "| 27|aaaaaaaaaaaaaaaaa...|\n",
      "| 28|aaaaaaaaaaaaaaaaa...|\n",
      "| 29|aaaaaaaaaaaaaaaaa...|\n",
      "| 30|aaaaaaaaaaaaaaaaa...|\n",
      "| 31|aaaaaaaaaaaaaaaaa...|\n",
      "| 32|aaaaaaaaaaaaaaaaa...|\n",
      "| 33|aaaaaaaaaaaaaaaaa...|\n",
      "| 34|aaaaaaaaaaaaaaaaa...|\n",
      "| 35|aaaaaaaaaaaaaaaaa...|\n",
      "| 36|aaaaaaaaaaaaaaaaa...|\n",
      "| 37|aaaaaaaaaaaaaaaaa...|\n",
      "| 38|aaaaaaaaaaaaaaaaa...|\n",
      "| 39|aaaaaaaaaaaaaaaaa...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [(num, \"a\" * num) for num in range(20, 40)]\n",
    "spark.createDataFrame(rows).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------+\n",
      "|_1 |_2                                     |\n",
      "+---+---------------------------------------+\n",
      "|20 |aaaaaaaaaaaaaaaaaaaa                   |\n",
      "|21 |aaaaaaaaaaaaaaaaaaaaa                  |\n",
      "|22 |aaaaaaaaaaaaaaaaaaaaaa                 |\n",
      "|23 |aaaaaaaaaaaaaaaaaaaaaaa                |\n",
      "|24 |aaaaaaaaaaaaaaaaaaaaaaaa               |\n",
      "|25 |aaaaaaaaaaaaaaaaaaaaaaaaa              |\n",
      "|26 |aaaaaaaaaaaaaaaaaaaaaaaaaa             |\n",
      "|27 |aaaaaaaaaaaaaaaaaaaaaaaaaaa            |\n",
      "|28 |aaaaaaaaaaaaaaaaaaaaaaaaaaaa           |\n",
      "|29 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaa          |\n",
      "|30 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaaa         |\n",
      "|31 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa        |\n",
      "|32 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa       |\n",
      "|33 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa      |\n",
      "|34 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa     |\n",
      "|35 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa    |\n",
      "|36 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa   |\n",
      "|37 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  |\n",
      "|38 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa |\n",
      "|39 |aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa|\n",
      "+---+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [(num, \"a\" * num) for num in range(20, 40)]\n",
    "spark.createDataFrame(rows).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [(num, \"a\" * num) for num in range(20, 40)]\n",
    "spark.createDataFrame(rows).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: boolean (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      " |-- _3: double (nullable = true)\n",
      " |-- _4: decimal(38,18) (nullable = true)\n",
      " |-- _5: binary (nullable = true)\n",
      " |-- _6: timestamp (nullable = true)\n",
      " |-- _7: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- _8: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import decimal, datetime\n",
    "spark.createDataFrame([(True, \n",
    "                        4, \n",
    "                        1.0, \n",
    "                        decimal.Decimal(\"1.0\"),\n",
    "                        bytearray([1, 2]),\n",
    "                        datetime.datetime(2000, 1, 2, 3, 4),\n",
    "                        [\"ddd\", \"vvv\"],\n",
    "                       {\"a\":2})]).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+--------------------+----+\n",
      "|_c0|      _c1|_c2|                 _c3| _c4|\n",
      "+---+---------+---+--------------------+----+\n",
      "|  0|  ALGERIA|  0| haggle. carefull...|null|\n",
      "|  1|ARGENTINA|  1|al foxes promise ...|null|\n",
      "|  2|   BRAZIL|  1|y alongside of th...|null|\n",
      "|  3|   CANADA|  1|eas hang ironic, ...|null|\n",
      "|  4|    EGYPT|  4|y above the caref...|null|\n",
      "+---+---------+---+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+---------+\n",
      "|_c0|      _c1|\n",
      "+---+---------+\n",
      "|  0|  ALGERIA|\n",
      "|  1|ARGENTINA|\n",
      "|  2|   BRAZIL|\n",
      "|  3|   CANADA|\n",
      "|  4|    EGYPT|\n",
      "+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/home/orpington/Desktop/dbgen/tpch-1/nation.tbl\", sep=\"|\")\n",
    "df.show(5)\n",
    "df.select(\"_c0\", \"_c1\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   _1|\n",
      "+-----+\n",
      "|eagle|\n",
      "| duck|\n",
      "|  fox|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([(\"fox\",), (\"duck\",), (\"eagle\",), (\"fox\",)]).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+\n",
      "| id|(id % 2)|(id % 2)|\n",
      "+---+--------+--------+\n",
      "|  0|       0|       0|\n",
      "|  1|       1|       1|\n",
      "|  2|       0|       0|\n",
      "|  3|       1|       1|\n",
      "|  4|       0|       0|\n",
      "|  5|       1|       1|\n",
      "|  6|       0|       0|\n",
      "|  7|       1|       1|\n",
      "|  8|       0|       0|\n",
      "|  9|       1|       1|\n",
      "+---+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "spark.range(10).select(\"id\", expr(\"id % 2\"), col(\"id\") % 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| flag|\n",
      "+---+-----+\n",
      "|  0|false|\n",
      "|  1| true|\n",
      "|  2|false|\n",
      "|  3| true|\n",
      "|  4|false|\n",
      "|  5| true|\n",
      "|  6|false|\n",
      "|  7| true|\n",
      "|  8|false|\n",
      "|  9| true|\n",
      "+---+-----+\n",
      "\n",
      "+---+----+\n",
      "| id|flag|\n",
      "+---+----+\n",
      "|  1|true|\n",
      "|  3|true|\n",
      "|  5|true|\n",
      "|  7|true|\n",
      "|  9|true|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10).withColumn(\"flag\", expr(\"id % 2 == 1\"))\n",
    "df.show()\n",
    "df.where(\"flag\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|something else than id|\n",
      "+----------------------+\n",
      "|                     0|\n",
      "|                     1|\n",
      "|                     2|\n",
      "|                     3|\n",
      "|                     4|\n",
      "|                     5|\n",
      "|                     6|\n",
      "|                     7|\n",
      "|                     8|\n",
      "|                     9|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).withColumnRenamed(\"id\", \"something else than id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  9|\n",
      "|  8|\n",
      "|  7|\n",
      "|  6|\n",
      "|  5|\n",
      "|  4|\n",
      "|  3|\n",
      "|  2|\n",
      "|  1|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "spark.range(10).orderBy(desc(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|rand1|               rand2|\n",
      "+---+-----+--------------------+\n",
      "|  5|  cat|  -1.321720703237387|\n",
      "|  0|  cat|-0.01936405426632...|\n",
      "|  1|  cat|  0.0561330407300823|\n",
      "|  3|  cat|  0.6742340794515306|\n",
      "|  8|  cat|  1.0598055846873387|\n",
      "|  6|  dog| -1.8712019672434888|\n",
      "|  7|  dog|  -1.559670868144924|\n",
      "|  2|  dog|   0.259390814238652|\n",
      "|  4|  dog|  0.5799376199421837|\n",
      "|  9|  dog|  0.9884335789160633|\n",
      "+---+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, randn, when, lit\n",
    "spark.range(10).withColumn(\"rand1\", when(randn() > 0, \"cat\").otherwise(\"dog\"))\\\n",
    "               .withColumn(\"rand2\", randn())\\\n",
    "               .orderBy(\"rand1\", \"rand2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10).sample(withReplacement=True, fraction=0.5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  5|\n",
      "|  6|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "|  7|\n",
      "| 11|\n",
      "| 12|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df in spark.range(20).randomSplit([0.75, 0.25]):\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).union(spark.range(10, 15)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+--------------------+----+\n",
      "| id|  country|num|                desc|   n|\n",
      "+---+---------+---+--------------------+----+\n",
      "|  0|  ALGERIA|  0| haggle. carefull...|null|\n",
      "|  1|ARGENTINA|  1|al foxes promise ...|null|\n",
      "|  2|   BRAZIL|  1|y alongside of th...|null|\n",
      "|  3|   CANADA|  1|eas hang ironic, ...|null|\n",
      "|  4|    EGYPT|  4|y above the caref...|null|\n",
      "+---+---------+---+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "          .format(\"csv\")\\\n",
    "          .option(\"sep\", \"|\")\\\n",
    "          .option(\"header\", False)\\\n",
    "          .option(\"inferSchema\", True)\\\n",
    "          .option(\"ignoreTrailingWhitespace\", True)\\\n",
    "          .load(\"/home/orpington/dbgen-data/nation.tbl\")\n",
    "\n",
    "for i, new_name in enumerate([\"id\", \"country\", \"num\", \"desc\", \"n\"]):\n",
    "    df = df.withColumnRenamed(df.columns[i], new_name)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+--------------------+----+\n",
      "| id|  country|num|                desc|   n|\n",
      "+---+---------+---+--------------------+----+\n",
      "|  8|    INDIA|  2|ss excuses cajole...|null|\n",
      "|  9|INDONESIA|  2| slyly express as...|null|\n",
      "| 11|     IRAQ|  4|nic deposits boos...|null|\n",
      "| 12|    JAPAN|  2|ously. final, exp...|null|\n",
      "| 13|   JORDAN|  4|ic deposits are b...|null|\n",
      "+---+---------+---+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "          .format(\"csv\")\\\n",
    "          .option(\"sep\", \"|\")\\\n",
    "          .option(\"header\", False)\\\n",
    "          .option(\"inferSchema\", True)\\\n",
    "          .option(\"ignoreTrailingWhitespace\", True)\\\n",
    "          .load(\"/home/orpington/dbgen-data/nation.tbl\")\n",
    "\n",
    "for i, new_name in enumerate([\"id\", \"country\", \"num\", \"desc\", \"n\"]):\n",
    "    df = df.withColumnRenamed(df.columns[i], new_name)\n",
    "\n",
    "#filtering\n",
    "all_i = df[\"country\"].startswith(\"J\")\n",
    "or_j = all_i | df[\"country\"].startswith(\"I\")\n",
    "df.where(or_j)\\\n",
    "  .filter(col(\"country\") != \"IRAN\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+--------------------+----+\n",
      "| id|  country|num|                desc|   n|\n",
      "+---+---------+---+--------------------+----+\n",
      "|  8|    INDIA|  2|ss excuses cajole...|null|\n",
      "|  9|INDONESIA|  2| slyly express as...|null|\n",
      "| 11|     IRAQ|  4|nic deposits boos...|null|\n",
      "| 12|    JAPAN|  2|ously. final, exp...|null|\n",
      "| 13|   JORDAN|  4|ic deposits are b...|null|\n",
      "+---+---------+---+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "          .format(\"csv\")\\\n",
    "          .option(\"sep\", \"|\")\\\n",
    "          .option(\"header\", False)\\\n",
    "          .option(\"inferSchema\", True)\\\n",
    "          .option(\"ignoreTrailingWhitespace\", True)\\\n",
    "          .load(\"/home/orpington/dbgen-data/nation.tbl\")\n",
    "\n",
    "for i, new_name in enumerate([\"id\", \"country\", \"num\", \"desc\", \"n\"]):\n",
    "    df = df.withColumnRenamed(df.columns[i], new_name)\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM df\n",
    "WHERE (instr(country, 'I') = 1 or instr(country, 'J') = 1)\n",
    "  AND country != 'IRAN'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            function|\n",
      "+--------------------+\n",
      "|                 abs|\n",
      "|                acos|\n",
      "|          add_months|\n",
      "|approx_count_dist...|\n",
      "|      array_contains|\n",
      "|               ascii|\n",
      "|                asin|\n",
      "|         assert_true|\n",
      "|              base64|\n",
      "|                cast|\n",
      "|            coalesce|\n",
      "|        collect_list|\n",
      "|         collect_set|\n",
      "|           concat_ws|\n",
      "|                 cos|\n",
      "|                cosh|\n",
      "|    count_min_sketch|\n",
      "|          covar_samp|\n",
      "|           cume_dist|\n",
      "|    current_database|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show functions\").where(col(\"function\").contains(\"s\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----+-------+\n",
      "|  country|power|value|rounded|\n",
      "+---------+-----+-----+-------+\n",
      "|  ALGERIA|  0.0|  1.0|    0.0|\n",
      "|ARGENTINA|  1.0|  2.0|    1.0|\n",
      "|   BRAZIL|  4.0|  5.0|    1.0|\n",
      "+---------+-----+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pow, round, expr, col\n",
    "\n",
    "df.select(expr(\"country\"), \n",
    "          pow(col(\"id\") * col(\"num\"), 2).alias(\"power\"),\n",
    "          expr(\"POWER((id * num), 2) +1 as value\"),\n",
    "          round(col(\"id\") / 2).alias(\"rounded\")\n",
    "         ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "spark.range(1).select(round(lit(2.5)), bround(lit(2.5))).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017025689444518023"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1000).selectExpr(\"id\", \"randn() as r\").stat.corr(\"id\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         corr(id, r)|\n",
      "+--------------------+\n",
      "|0.007465452328744746|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import randn, corr\n",
    "spark.range(1000).withColumn(\"r\", randn()).select(corr(\"id\", \"r\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------+------------------+--------------------+----+\n",
      "|summary|               id|country|               num|                desc|   n|\n",
      "+-------+-----------------+-------+------------------+--------------------+----+\n",
      "|  count|               25|     25|                25|                  25|   0|\n",
      "|   mean|             12.0|   null|               2.0|                null|null|\n",
      "| stddev|7.359800721939872|   null|1.4433756729740645|                null|null|\n",
      "|    min|                0|ALGERIA|                 0| haggle. carefull...|null|\n",
      "|    max|               24|VIETNAM|                 4|y final packages....|null|\n",
      "+-------+-----------------+-------+------------------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "          .format(\"csv\")\\\n",
    "          .option(\"sep\", \"|\")\\\n",
    "          .option(\"header\", False)\\\n",
    "          .option(\"inferSchema\", True)\\\n",
    "          .option(\"ignoreTrailingWhitespace\", True)\\\n",
    "          .load(\"/home/orpington/dbgen-data/nation.tbl\")\n",
    "\n",
    "for i, new_name in enumerate([\"id\", \"country\", \"num\", \"desc\", \"n\"]):\n",
    "    df = df.withColumnRenamed(df.columns[i], new_name)\n",
    "    \n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|lens|num|\n",
      "+----+---+\n",
      "|   7|  0|\n",
      "|   9|  1|\n",
      "|   6|  1|\n",
      "|   6|  1|\n",
      "|   5|  4|\n",
      "+----+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+---+---+---+---+---+\n",
      "|lens_num|  0|  1|  2|  3|  4|\n",
      "+--------+---+---+---+---+---+\n",
      "|      12|  0|  0|  0|  0|  1|\n",
      "|       8|  1|  0|  0|  0|  0|\n",
      "|       4|  0|  1|  0|  0|  2|\n",
      "|       9|  0|  1|  1|  0|  0|\n",
      "|      13|  0|  1|  0|  0|  0|\n",
      "|       5|  1|  0|  3|  0|  1|\n",
      "|      10|  1|  0|  0|  0|  0|\n",
      "|       6|  0|  2|  0|  2|  1|\n",
      "|      14|  0|  0|  0|  1|  0|\n",
      "|       7|  2|  0|  1|  2|  0|\n",
      "+--------+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, col\n",
    "df = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "\n",
    "for i, new_name in enumerate([\"id\", \"country\", \"num\", \"desc\", \"n\"]):\n",
    "    df = df.withColumnRenamed(df.columns[i], new_name)\n",
    "    \n",
    "df_base = df.withColumn(\"lens\", length(col(\"country\")).cast(\"string\")).select(\"lens\", \"num\")\n",
    "df_base.show(5)\n",
    "df_base.crosstab(\"lens\", \"num\").show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      lens_freqItems|\n",
      "+--------------------+\n",
      "|[5, 12, 8, 7, 4, ...|\n",
      "+--------------------+\n",
      "\n",
      "+---+\n",
      "|col|\n",
      "+---+\n",
      "|  5|\n",
      "| 12|\n",
      "|  8|\n",
      "|  7|\n",
      "|  4|\n",
      "| 14|\n",
      "|  6|\n",
      "|  9|\n",
      "| 10|\n",
      "| 13|\n",
      "+---+\n",
      "\n",
      "+---+---+\n",
      "|pos|col|\n",
      "+---+---+\n",
      "|  0|  5|\n",
      "|  1| 12|\n",
      "|  2|  8|\n",
      "|  3|  7|\n",
      "|  4|  4|\n",
      "|  5| 14|\n",
      "|  6|  6|\n",
      "|  7|  9|\n",
      "|  8| 10|\n",
      "|  9| 13|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, col, explode, posexplode\n",
    "df = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "\n",
    "for i, new_name in enumerate([\"id\", \"country\", \"num\", \"desc\", \"n\"]):\n",
    "    df = df.withColumnRenamed(df.columns[i], new_name)\n",
    "    \n",
    "freq = df.withColumn(\"lens\", length(col(\"country\")).cast(\"string\"))\\\n",
    "         .freqItems([\"lens\"])\n",
    "freq.show()\n",
    "\n",
    "freq.select(explode(freq.columns[0])).show()\n",
    "freq.select(posexplode(freq.columns[0])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+---+\n",
      "|           initcap|             lower|             upper|lit|\n",
      "+------------------+------------------+------------------+---+\n",
      "|   Country Algeria|   country algeria|   COUNTRY ALGERIA|ddd|\n",
      "| Country Argentina| country argentina| COUNTRY ARGENTINA|ddd|\n",
      "|    Country Brazil|    country brazil|    COUNTRY BRAZIL|ddd|\n",
      "|    Country Canada|    country canada|    COUNTRY CANADA|ddd|\n",
      "|     Country Egypt|     country egypt|     COUNTRY EGYPT|ddd|\n",
      "|  Country Ethiopia|  country ethiopia|  COUNTRY ETHIOPIA|ddd|\n",
      "|    Country France|    country france|    COUNTRY FRANCE|ddd|\n",
      "|   Country Germany|   country germany|   COUNTRY GERMANY|ddd|\n",
      "|     Country India|     country india|     COUNTRY INDIA|ddd|\n",
      "| Country Indonesia| country indonesia| COUNTRY INDONESIA|ddd|\n",
      "|      Country Iran|      country iran|      COUNTRY IRAN|ddd|\n",
      "|      Country Iraq|      country iraq|      COUNTRY IRAQ|ddd|\n",
      "|     Country Japan|     country japan|     COUNTRY JAPAN|ddd|\n",
      "|    Country Jordan|    country jordan|    COUNTRY JORDAN|ddd|\n",
      "|     Country Kenya|     country kenya|     COUNTRY KENYA|ddd|\n",
      "|   Country Morocco|   country morocco|   COUNTRY MOROCCO|ddd|\n",
      "|Country Mozambique|country mozambique|COUNTRY MOZAMBIQUE|ddd|\n",
      "|      Country Peru|      country peru|      COUNTRY PERU|ddd|\n",
      "|     Country China|     country china|     COUNTRY CHINA|ddd|\n",
      "|   Country Romania|   country romania|   COUNTRY ROMANIA|ddd|\n",
      "+------------------+------------------+------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "exprs = [initcap(concat(lit(\"country \"),col(\"country\"))).alias(\"initcap\")]\n",
    "exprs += [lower(exprs[0]).alias(\"lower\")]\n",
    "exprs += [upper(exprs[0]).alias(\"upper\")]\n",
    "exprs += [lit(\"ddd\").alias(\"lit\")]\n",
    "\n",
    "spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\\\n",
    "     .withColumnRenamed(\"_c1\",\"country\")\\\n",
    "     .select(*exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+--------+--------+-------------+-------------+----+\n",
      "|    lpad|        rpad|       ltrim|   rtrim|    trim|       regexp|    translate|extr|\n",
      "+--------+------------+------------+--------+--------+-------------+-------------+----+\n",
      "|=ALGERIA|=ALGERIA====| ALGERIA====|=ALGERIA| ALGERIA|      ALGERI-|      _+GERI_|   G|\n",
      "|=GERMANY|=GERMANY====| GERMANY====|=GERMANY| GERMANY|      GERMANY|      GERM_NY|   R|\n",
      "|===INDIA|===INDIA====|   INDIA====|===INDIA|   INDIA|        I-DI-|        INDI_|   D|\n",
      "|===JAPAN|===JAPAN====|   JAPAN====|===JAPAN|   JAPAN|        JAPAN|        J_P_N|   P|\n",
      "|UNITED S|UNITED S====|UNITED=S====|UNITED=S|UNITED=S|UNI-ED STATES|UNITED ST_TES|   I|\n",
      "+--------+------------+------------+--------+--------+-------------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean = lambda fun: regexp_replace(fun(regexp_replace(exprs[1], \"=\", \" \")), \" \", \"=\")\n",
    "exprs = [lpad(col(\"country\"), 8, \"=\")]\n",
    "exprs += [rpad(exprs[0], 12, \"=\")]\n",
    "exprs += [clean(ltrim)]\n",
    "exprs += [clean(rtrim)]\n",
    "exprs += [clean(trim)]\n",
    "exprs += [regexp_replace(col(\"country\"), \"(I).\", \"$1-\")]\n",
    "exprs += [translate(col(\"country\"), \"AL\", \"_+\")]\n",
    "exprs += [regexp_extract(col(\"country\"), r\"(\\w)(\\w)(\\w)\", 3)]\n",
    "spark.createDataFrame(spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\\\n",
    "     .withColumnRenamed(\"_c1\",\"country\")\\\n",
    "     .sample(False, 0.1, 5)\\\n",
    "     .select(*exprs).rdd, [\"lpad\", \"rpad\", \"ltrim\", \"rtrim\", \"trim\", \"regexp\", \"translate\", \"extr\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+---------------------+\n",
      "|  country|instr(country, A)|locate(A, country, 1)|\n",
      "+---------+-----------------+---------------------+\n",
      "|  ALGERIA|                1|                    1|\n",
      "|ARGENTINA|                1|                    1|\n",
      "|   BRAZIL|                3|                    3|\n",
      "|   CANADA|                2|                    2|\n",
      "|    EGYPT|                0|                    0|\n",
      "+---------+-----------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\\\n",
    "     .withColumnRenamed(\"_c1\",\"country\")\\\n",
    "     .select(\"country\", \n",
    "             instr(col(\"country\"),\"A\"), \n",
    "             locate(\"A\",col(\"country\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2017-10-05|2017-10-05 12:19:11.989|\n",
      "|1  |2017-10-05|2017-10-05 12:19:11.989|\n",
      "|2  |2017-10-05|2017-10-05 12:19:11.989|\n",
      "+---+----------+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "dateDf = spark.range(5)\\\n",
    "              .withColumn(\"today\", current_date())\\\n",
    "              .withColumn(\"now\", current_timestamp())\n",
    "dateDf.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+-------------------+----------+\n",
      "|today     |start     |end       |wrong     |unix      |unix_time          |month_diff|\n",
      "+----------+----------+----------+----------+----------+-------------------+----------+\n",
      "|2017-10-05|2017-10-10|2017-09-30|null      |1546390922|2019-02-02 00:02:02|0.35483871|\n",
      "|2017-10-05|2017-10-10|2017-09-30|2020-01-01|1546390922|2019-02-02 00:02:02|0.35483871|\n",
      "|2017-10-05|2017-10-10|2017-09-30|2020-02-01|1546390922|2019-02-02 00:02:02|0.35483871|\n",
      "+----------+----------+----------+----------+----------+-------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "dateDf = spark.range(5)\\\n",
    "              .withColumn(\"today\", current_date())\\\n",
    "              .withColumn(\"now\", current_timestamp())\\\n",
    "              .select(\n",
    "                  \"*\",\n",
    "                  date_add(col(\"today\"), 5).alias(\"start\"),\n",
    "                  date_sub(col(\"today\"), 5).alias(\"end\"),\n",
    "                  to_date(concat(lit(\"2020-\"),lpad(col(\"id\"), 2, \"0\"), lit(\"-01\"))).alias(\"wrong\"),\n",
    "                  unix_timestamp(lit(\"2019-02-02 02:02:02\"), \"yyyy-mm-dd HH:mm:ss\").alias(\"unix\"),\n",
    "                  from_unixtime(lit(\"1546383722\"), \"yyyy-mm-dd HH:mm:ss\").alias(\"unix_time\"))\\\n",
    "            .drop(\"id\", \"now\")\\\n",
    "            .withColumn(\"month_diff\", months_between(col(\"start\"), col(\"end\")))\n",
    "dateDf.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10**5).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|one_null|with_null|\n",
      "+--------+---------+\n",
      "|    null|     null|\n",
      "|       1|        1|\n",
      "|       2|     null|\n",
      "|       3|        3|\n",
      "|       4|     null|\n",
      "|       5|        5|\n",
      "|       6|     null|\n",
      "|       7|        7|\n",
      "|       8|     null|\n",
      "|       9|        9|\n",
      "+--------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|one_null|with_null|\n",
      "+--------+---------+\n",
      "|       1|        1|\n",
      "|       3|        3|\n",
      "|       5|        5|\n",
      "|       7|        7|\n",
      "|       9|        9|\n",
      "+--------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|one_null|with_null|\n",
      "+--------+---------+\n",
      "|       1|        1|\n",
      "|       2|     null|\n",
      "|       3|        3|\n",
      "|       4|     null|\n",
      "|       5|        5|\n",
      "|       6|     null|\n",
      "|       7|        7|\n",
      "|       8|     null|\n",
      "|       9|        9|\n",
      "+--------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|one_null|with_null|\n",
      "+--------+---------+\n",
      "|       1|        1|\n",
      "|       2|     null|\n",
      "|       3|        3|\n",
      "|       4|     null|\n",
      "|       5|        5|\n",
      "|       6|     null|\n",
      "|       7|        7|\n",
      "|       8|     null|\n",
      "|       9|        9|\n",
      "+--------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|one_null|with_null|\n",
      "+--------+---------+\n",
      "|       1|        1|\n",
      "|       3|        3|\n",
      "|       5|        5|\n",
      "|       7|        7|\n",
      "|       9|        9|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\\\n",
    "          .selectExpr(\"case when id = 0 then null else id end as one_null\",\n",
    "                      \"case when id % 2 = 0 then null else id end as with_null\")\n",
    "df.show()\n",
    "df.na.drop(\"any\").show()\n",
    "df.na.drop(\"all\").show()\n",
    "df.na.drop(\"all\", subset=[\"one_null\"]).show()\n",
    "df.na.drop(\"all\", subset=[\"with_null\"]).show()\n",
    "#dateDf2.na.drop(\"any\").limit(limit_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "| 44|\n",
      "|  3|\n",
      "| -1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).show()\n",
    "spark.range(5).na.replace([2, 4], [44, -1], \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|complex|col2|\n",
      "+-------+----+\n",
      "|  [0,0]|   0|\n",
      "|  [1,1]|   1|\n",
      "|  [2,0]|   0|\n",
      "+-------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).selectExpr(\"(id, id % 2) as complex\").select(\"*\", \"complex.col2\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- complex: struct (nullable = false)\n",
      " |    |-- id: long (nullable = false)\n",
      " |    |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).selectExpr(\"struct(id, id % 2) as complex\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----+-------+\n",
      "|      complex| id|col2|   col3|\n",
      "+-------------+---+----+-------+\n",
      "|[0,0,a value]|  0|   0|a value|\n",
      "|[1,1,a value]|  1|   1|a value|\n",
      "|[2,0,a value]|  2|   0|a value|\n",
      "|[3,1,a value]|  3|   1|a value|\n",
      "|[4,0,a value]|  4|   0|a value|\n",
      "+-------------+---+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).selectExpr(\"struct(id, id % 2, 'a value') as complex\").select(\"complex\",\"complex.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-10-05 12:48:17--  https://raw.githubusercontent.com/JakubPawlicki/daily/master/rxpy/course.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15982 (16K) [text/plain]\n",
      "Saving to: ‘course.py’\n",
      "\n",
      "course.py           100%[===================>]  15,61K  --.-KB/s    in 0,05s   \n",
      "\n",
      "2017-10-05 12:48:17 (292 KB/s) - ‘course.py’ saved [15982/15982]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/JakubPawlicki/daily/master/rxpy/course.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              arr[0]|              arr[0]|\n",
      "+--------------------+--------------------+\n",
      "|                from|                from|\n",
      "|                    |                    |\n",
      "|                 def|                 def|\n",
      "|                    |                    |\n",
      "|             letters|             letters|\n",
      "|                    |                    |\n",
      "|               class|               class|\n",
      "|                    |                    |\n",
      "|                 def|                 def|\n",
      "|        print(value)|        print(value)|\n",
      "|                    |                    |\n",
      "|                 def|                 def|\n",
      "|       print(\"done\")|       print(\"done\")|\n",
      "|                    |                    |\n",
      "|                 def|                 def|\n",
      "|        print(error)|        print(error)|\n",
      "|                    |                    |\n",
      "|letters.subscribe...|letters.subscribe...|\n",
      "|                    |                    |\n",
      "|                 def|                 def|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-------------------+\n",
      "|                 arr|              expld|\n",
      "+--------------------+-------------------+\n",
      "| [def, class_one():]|                def|\n",
      "| [def, class_one():]|       class_one():|\n",
      "|[def, on_next(sel...|                def|\n",
      "|[def, on_next(sel...|      on_next(self,|\n",
      "|[def, on_next(sel...|            value):|\n",
      "|[def, on_complete...|                def|\n",
      "|[def, on_complete...|on_completed(self):|\n",
      "|[def, on_error(se...|                def|\n",
      "|[def, on_error(se...|     on_error(self,|\n",
      "|[def, on_error(se...|            error):|\n",
      "| [def, class_two():]|                def|\n",
      "| [def, class_two():]|       class_two():|\n",
      "|[def, class_three...|                def|\n",
      "|[def, class_three...|     class_three():|\n",
      "|[def, class_four():]|                def|\n",
      "|[def, class_four():]|      class_four():|\n",
      "|[def, class_five():]|                def|\n",
      "|[def, class_five():]|      class_five():|\n",
      "| [def, class_six():]|                def|\n",
      "| [def, class_six():]|       class_six():|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "text_df = spark.read.text(\"course.py\")\n",
    "#text_df.show()\n",
    "text_df_processed = text_df.select(split(trim(col(\"value\")), \" \").alias(\"arr\"))\n",
    "text_df_processed.select(expr(\"arr[0]\"), expr(\"arr\").getItem(0)).show()\n",
    "\n",
    "text_df_processed.where(array_contains(col(\"arr\"), \"def\")).withColumn(\"expld\", explode(col(\"arr\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|collect_list(id)|\n",
      "+----------------+\n",
      "| [0, 1, 2, 3, 4]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).select(collect_list(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+-----+---------------------------------+\n",
      "|key  |value|key |value|d                                |\n",
      "+-----+-----+----+-----+---------------------------------+\n",
      "|name |nope |dddd|0    |Map(name -> nope, name2 -> again)|\n",
      "|name2|again|dddd|0    |Map(name -> nope, name2 -> again)|\n",
      "|name |nope |dddd|1    |Map(name -> nope, name2 -> again)|\n",
      "|name2|again|dddd|1    |Map(name -> nope, name2 -> again)|\n",
      "|name |nope |dddd|2    |Map(name -> nope, name2 -> again)|\n",
      "|name2|again|dddd|2    |Map(name -> nope, name2 -> again)|\n",
      "|name |nope |dddd|3    |Map(name -> nope, name2 -> again)|\n",
      "|name2|again|dddd|3    |Map(name -> nope, name2 -> again)|\n",
      "|name |nope |dddd|4    |Map(name -> nope, name2 -> again)|\n",
      "|name2|again|dddd|4    |Map(name -> nope, name2 -> again)|\n",
      "+-----+-----+----+-----+---------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random_words import RandomWords\n",
    "rows = [Row(i, \"dddd\", {\"name\": \"nope\", \"name2\": \"again\"}) for i in range(10)]\n",
    "df_map = spark.createDataFrame(rows, [\"id\", \"word\", \"d\"])\\\n",
    "              .select(create_map(col(\"word\"), col(\"id\")).alias(\"map\"), col(\"d\"))\\\n",
    "              .select(explode(col(\"map\")), col(\"d\"))\\\n",
    "              .select(explode(col(\"d\")), col(\"*\"))\\\n",
    "              .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------------+\n",
      "|    key|value|                   d|\n",
      "+-------+-----+--------------------+\n",
      "|   duck|    0|Map(name -> nope,...|\n",
      "|  eagle|    1|Map(name -> nope,...|\n",
      "|pidgeon|    2|Map(name -> nope,...|\n",
      "+-------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [Row(i, word, {\"name\": \"nope\", \"name2\": \"again\"}) \n",
    "        for i, word \n",
    "        in enumerate([\"duck\", \"eagle\", \"pidgeon\"])]\n",
    "\n",
    "df_map = spark.createDataFrame(rows, [\"id\", \"word\", \"d\"])\n",
    "#df_map.show()\n",
    "df_map = df_map.select(create_map(col(\"word\"), col(\"id\")).alias(\"map\"), col(\"d\"))\n",
    "#df_map.show()\n",
    "df_map = df_map.select(explode(col(\"map\")), col(\"d\"))\n",
    "df_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          jsonstring|\n",
      "+--------------------+\n",
      "|{\"myJSONKey\": {\"m...|\n",
      "+--------------------+\n",
      "\n",
      "root\n",
      " |-- jsonstring: string (nullable = false)\n",
      "\n",
      "+---------------+-----------------------+------------------------------+\n",
      "|get_json_object|json_tuple             |json                          |\n",
      "+---------------+-----------------------+------------------------------+\n",
      "|1              |{\"myJSONValue\":[1,2,3]}|{\"col1\":\"first\",\"col2\":\"last\"}|\n",
      "+---------------+-----------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "json = \"\"\"\n",
    "'{\"myJSONKey\": {\"myJSONValue\":[1,2,3]}}' as jsonstring\n",
    "\"\"\"\n",
    "json_df = spark.range(1).selectExpr(json)\n",
    "json_df.show()\n",
    "json_df.printSchema()\n",
    "\n",
    "expressive_json = get_json_object(col(\"jsonstring\"), \"$.myJSONKey.myJSONValue[0]\")\n",
    "simple_json = json_tuple(col(\"jsonstring\"), \"myJSONKey\")\n",
    "struct_to_json = to_json(struct(lit('first'),lit('last')))\n",
    "\n",
    "json_df.select(expressive_json.name(\"get_json_object\"),\n",
    "               simple_json.name(\"json_tuple\"),\n",
    "               struct_to_json.name(\"json\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|json                                                          |\n",
      "+--------------------------------------------------------------+\n",
      "|{\"col1\":{\"col\":\"one\",\"id\":25769803776},\"today\":\"2017-10-05\"}  |\n",
      "|{\"col1\":{\"col\":\"two\",\"id\":25769803777},\"today\":\"2017-10-05\"}  |\n",
      "|{\"col1\":{\"col\":\"three\",\"id\":25769803778},\"today\":\"2017-10-05\"}|\n",
      "+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(\"name1\", StringType(), True),\n",
    "     StructField(\"name2\", StringType(), True)])\n",
    "\n",
    "random_words_expr = array([lit(word) for word in [\"one\", \"two\", \"three\"]])\n",
    "\n",
    "inner_struct = struct(col(\"col\"), col(\"id\"))\n",
    "outer_struct = struct(inner_struct, current_date().alias(\"today\"))\n",
    "                      \n",
    "df = spark.range(1)\\\n",
    "          .select(explode(random_words_expr))\\\n",
    "          .withColumn(\"id\", monotonically_increasing_id())\\\n",
    "          .select(to_json(outer_struct).alias(\"json\"))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|<lambda>(id)|\n",
      "+------------+\n",
      "|           0|\n",
      "|           1|\n",
      "|           8|\n",
      "|          27|\n",
      "|          64|\n",
      "|         125|\n",
      "|         216|\n",
      "|         343|\n",
      "|         512|\n",
      "|         729|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "spark.range(10).select(udf(lambda x: x**3)(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------+\n",
      "|go-go-go(id)|go-go-go2(id)|go-go-go3(id)|\n",
      "+------------+-------------+-------------+\n",
      "|        null|          0.0|          0.0|\n",
      "|        null|          1.0|          1.0|\n",
      "|        null|          8.0|          8.0|\n",
      "|        null|         27.0|         27.0|\n",
      "|        null|         64.0|         64.0|\n",
      "|        null|        125.0|        125.0|\n",
      "|        null|        216.0|        216.0|\n",
      "|        null|        343.0|        343.0|\n",
      "|        null|        512.0|        512.0|\n",
      "|        null|        729.0|        729.0|\n",
      "+------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "spark.udf.register(\"go-go-go\", lambda x: x**3*1.0, IntegerType())\n",
    "spark.udf.register(\"go-go-go2\", lambda x: x**3*1.0, StringType())\n",
    "spark.udf.register(\"go-go-go3\", lambda x: x**3*1.0, DoubleType())\n",
    "spark.range(10).selectExpr(\"`go-go-go`(id)\", \"`go-go-go2`(id)\", \"`go-go-go3`(id)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id|  a|  b|  c|\n",
      "+---+---+---+---+\n",
      "|  1| a1| b1| c1|\n",
      "|  2| a2| b2| c2|\n",
      "|  3| a3| b3| c3|\n",
      "+---+---+---+---+\n",
      "\n",
      "\n",
      "+----+----+----+\n",
      "|col0|col1|col2|\n",
      "+----+----+----+\n",
      "|   1|   a|  a1|\n",
      "|   1|   b|  b1|\n",
      "|   1|   c|  c1|\n",
      "|   2|   a|  a2|\n",
      "|   2|   b|  b2|\n",
      "|   2|   c|  c2|\n",
      "|   3|   a|  a3|\n",
      "|   3|   b|  b3|\n",
      "|   3|   c|  c3|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "rows = [pyspark.sql.Row(str(i), \"a\" + str(i),\"b\" + str(i),\"c\" + str(i)) for i in range(1,4)]\n",
    "df = spark.createDataFrame(rows, [\"id\", \"a\", \"b\", \"c\"])\n",
    "df.show()\n",
    "print(\"\")\n",
    "df.selectExpr(\"stack(3, id, 'a', a, id, 'b', b, id, 'c', c)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col0|col1|\n",
      "+----+----+\n",
      "|   0|   2|\n",
      "|   0|   4|\n",
      "|   0|null|\n",
      "|null|null|\n",
      "|   1|   2|\n",
      "|   1|   4|\n",
      "|   1|null|\n",
      "|null|null|\n",
      "|   2|   2|\n",
      "|   2|   4|\n",
      "|   2|null|\n",
      "|null|null|\n",
      "|   3|   2|\n",
      "|   3|   4|\n",
      "|   3|null|\n",
      "|null|null|\n",
      "|   4|   2|\n",
      "|   4|   4|\n",
      "|   4|null|\n",
      "|null|null|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).selectExpr(\"stack(4, id, 2, id, 4, id)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- colstring: string (nullable = false)\n",
      " |-- colbyte: byte (nullable = false)\n",
      " |-- colfloat: float (nullable = false)\n",
      " |-- coldecimal: decimal(10,0) (nullable = false)\n",
      " |-- colbinary: binary (nullable = false)\n",
      " |-- coldate: date (nullable = false)\n",
      " |-- colarray: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- colmap: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"col\" + typ.typeName(), typ, False) \n",
    "                       for typ \n",
    "                       in [StringType(), \n",
    "                           ByteType(),\n",
    "                           FloatType(),\n",
    "                           DecimalType(),\n",
    "                           BinaryType(),\n",
    "                           DateType(),\n",
    "                           ArrayType(StringType()),\n",
    "                           MapType(StringType(), IntegerType())\n",
    "                          ]])\n",
    "\n",
    "spark.createDataFrame([(True, \n",
    "                        4, \n",
    "                        1.0, \n",
    "                        decimal.Decimal(\"1.0\"),\n",
    "                        bytearray([1, 2]),\n",
    "                        datetime.datetime(2000, 1, 2, 3, 4),\n",
    "                        [\"ddd\", \"vvv\"],\n",
    "                       {\"a\":2})], schema).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|double|\n",
      "+------+\n",
      "|   1.0|\n",
      "+------+\n",
      "\n",
      "error: DoubleType can not accept object 1 in type <type 'int'>\n",
      "+------+\n",
      "|double|\n",
      "+------+\n",
      "|   0.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "schema = StructType([StructField(\"double\", DoubleType(), False)])\n",
    "\n",
    "spark.createDataFrame([Row(1.0)],schema).show()\n",
    "\n",
    "try:\n",
    "    spark.createDataFrame([Row(1)],schema).show()\n",
    "except Exception as e:\n",
    "    print(\"error: {}\".format(e))\n",
    "\n",
    "spark.createDataFrame([Row(\"ddd\")],schema, verifySchema=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------------+\n",
      "| id| degree|             school|\n",
      "+---+-------+-------------------+\n",
      "|  0|Masters|   Great university|\n",
      "|  1|Masters| Greater university|\n",
      "|  2|  Ph.D.|Greatest university|\n",
      "+---+-------+-------------------+\n",
      "\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "| id| degree|            school| id|       name|study_program|\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "|  0|Masters|  Great university|  0| Ben Benson|            0|\n",
      "|  1|Masters|Greater university|  1|Tom Tomhson|            1|\n",
      "|  1|Masters|Greater university|  2| Jill Jilli|            1|\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "| id| degree|            school| id|       name|study_program|\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "|  0|Masters|  Great university|  0| Ben Benson|            0|\n",
      "|  1|Masters|Greater university|  1|Tom Tomhson|            1|\n",
      "|  1|Masters|Greater university|  2| Jill Jilli|            1|\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "| id| degree|             school|  id|       name|study_program|\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "|  0|Masters|   Great university|   0| Ben Benson|            0|\n",
      "|  1|Masters| Greater university|   1|Tom Tomhson|            1|\n",
      "|  1|Masters| Greater university|   2| Jill Jilli|            1|\n",
      "|  2|  Ph.D.|Greatest university|null|       null|         null|\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "| id| degree|             school|  id|       name|study_program|\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "|  0|Masters|   Great university|   0| Ben Benson|            0|\n",
      "|  1|Masters| Greater university|   1|Tom Tomhson|            1|\n",
      "|  1|Masters| Greater university|   2| Jill Jilli|            1|\n",
      "|  2|  Ph.D.|Greatest university|null|       null|         null|\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "| id| degree|             school|  id|       name|study_program|\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "|  0|Masters|   Great university|   0| Ben Benson|            0|\n",
      "|  1|Masters| Greater university|   1|Tom Tomhson|            1|\n",
      "|  1|Masters| Greater university|   2| Jill Jilli|            1|\n",
      "|  2|  Ph.D.|Greatest university|null|       null|         null|\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "| id| degree|             school|  id|       name|study_program|\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "|  0|Masters|   Great university|   0| Ben Benson|            0|\n",
      "|  1|Masters| Greater university|   1|Tom Tomhson|            1|\n",
      "|  1|Masters| Greater university|   2| Jill Jilli|            1|\n",
      "|  2|  Ph.D.|Greatest university|null|       null|         null|\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "| id| degree|             school|  id|       name|study_program|\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "|  0|Masters|   Great university|   0| Ben Benson|            0|\n",
      "|  1|Masters| Greater university|   1|Tom Tomhson|            1|\n",
      "|  1|Masters| Greater university|   2| Jill Jilli|            1|\n",
      "|  2|  Ph.D.|Greatest university|null|       null|         null|\n",
      "+---+-------+-------------------+----+-----------+-------------+\n",
      "\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "| id| degree|            school| id|       name|study_program|\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "|  0|Masters|  Great university|  0| Ben Benson|            0|\n",
      "|  1|Masters|Greater university|  1|Tom Tomhson|            1|\n",
      "|  1|Masters|Greater university|  2| Jill Jilli|            1|\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "| id| degree|            school| id|       name|study_program|\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "|  0|Masters|  Great university|  0| Ben Benson|            0|\n",
      "|  1|Masters|Greater university|  1|Tom Tomhson|            1|\n",
      "|  1|Masters|Greater university|  2| Jill Jilli|            1|\n",
      "+---+-------+------------------+---+-----------+-------------+\n",
      "\n",
      "+---+-------+------------------+\n",
      "| id| degree|            school|\n",
      "+---+-------+------------------+\n",
      "|  0|Masters|  Great university|\n",
      "|  1|Masters|Greater university|\n",
      "+---+-------+------------------+\n",
      "\n",
      "+---+------+-------------------+\n",
      "| id|degree|             school|\n",
      "+---+------+-------------------+\n",
      "|  2| Ph.D.|Greatest university|\n",
      "+---+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Ben Benson\", 0),\n",
    "    (1, \"Tom Tomhson\", 1),\n",
    "    (2, \"Jill Jilli\", 1)\n",
    "]).toDF(\"id\", \"name\", \"study_program\")\n",
    "\n",
    "studyProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"Great university\"),\n",
    "    (1, \"Masters\", \"Greater university\"),\n",
    "    (2, \"Ph.D.\", \"Greatest university\")\n",
    "]).toDF(\"id\", \"degree\", \"school\")\n",
    "studyProgram.show()\n",
    "\n",
    "join_expr = person[\"study_program\"] == studyProgram[\"id\"]\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"inner\")\n",
    "joined_df.show()\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"cross\")\n",
    "joined_df.show()\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"outer\")\n",
    "joined_df.show()\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"full\")\n",
    "joined_df.show()\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"full_outer\")\n",
    "joined_df.show()\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"left\")\n",
    "joined_df.show()\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"left_outer\")\n",
    "joined_df.show()\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"right\")\n",
    "joined_df.show()\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"right_outer\")\n",
    "joined_df.show()\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"left_semi\")\n",
    "joined_df.show()\n",
    "\n",
    "joined_df = studyProgram.join(person, join_expr, \"left_anti\")\n",
    "joined_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AnalysisException()', 'u\"Reference \\'id\\' is ambiguous, could be: id#5898L, id#5908L.;\"')\n",
      "+---+-----+---+-----+\n",
      "| id|name1| id|name2|\n",
      "+---+-----+---+-----+\n",
      "|  1|  aaa|  1|goose|\n",
      "|  1|  aaa|  1| duck|\n",
      "+---+-----+---+-----+\n",
      "\n",
      "+---+-----+-----+\n",
      "| id|name1|name2|\n",
      "+---+-----+-----+\n",
      "|  1|  aaa|goose|\n",
      "|  1|  aaa| duck|\n",
      "+---+-----+-----+\n",
      "\n",
      "+---+-----+---+-----+\n",
      "| id|name1| id|name2|\n",
      "+---+-----+---+-----+\n",
      "|  1|  aaa|  0|  cow|\n",
      "|  1|  aaa|  1|goose|\n",
      "|  1|  aaa|  1| duck|\n",
      "|  2|  bbb|  0|  cow|\n",
      "|  2|  bbb|  1|goose|\n",
      "|  2|  bbb|  1| duck|\n",
      "|  3|  bbb|  0|  cow|\n",
      "|  3|  bbb|  1|goose|\n",
      "|  3|  bbb|  1| duck|\n",
      "+---+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    (1, \"aaa\"),\n",
    "    (2, \"bbb\"),\n",
    "    (3, \"bbb\"),    \n",
    "]).toDF(\"id\", \"name1\")\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    (0, \"cow\"),\n",
    "    (1, \"goose\"),\n",
    "    (1, \"duck\"),    \n",
    "]).toDF(\"id\", \"name2\")\n",
    "\n",
    "try:\n",
    "    df1.join(df2, col(\"id\") == col(\"id\")).show()\n",
    "except Exception as e:\n",
    "    print(repr(e), str(e))\n",
    "    \n",
    "df1.join(df2, df1[\"id\"] == df2[\"id\"]).show()\n",
    "df1.join(df2, \"id\").show()\n",
    "df1.crossJoin(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+--------------+\n",
      "| id|name1|addition_first|addition_first|\n",
      "+---+-----+--------------+--------------+\n",
      "|  2|  bbb|   17179869184|   17179869184|\n",
      "|  1|  aaa|    8589934592|    8589934592|\n",
      "|  3|  bbb|   25769803776|   25769803776|\n",
      "+---+-----+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df1 = spark.createDataFrame([\n",
    "    (1, \"aaa\"),\n",
    "    (2, \"bbb\"),\n",
    "    (3, \"bbb\"),    \n",
    "]).toDF(\"id\", \"name1\")\n",
    "\n",
    "df1_1 = df1.withColumn(\"addition_first\", monotonically_increasing_id())\n",
    "df1_2 = df1.withColumn(\"addition_first\", monotonically_increasing_id())\n",
    "\n",
    "df1_1.join(df1_2, [\"id\", \"name1\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------------+\n",
      "| id| degree|             school|\n",
      "+---+-------+-------------------+\n",
      "|  0|Masters|   Great university|\n",
      "|  1|Masters| Greater university|\n",
      "|  2|  Ph.D.|Greatest university|\n",
      "+---+-------+-------------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*SortMergeJoin [id#6069L], [study_program#6057L], Inner\n",
      ":- *Sort [id#6069L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(id#6069L, 200)\n",
      ":     +- *Project [_1#6062L AS id#6069L, _2#6063 AS degree#6070, _3#6064 AS school#6071]\n",
      ":        +- *Filter isnotnull(_1#6062L)\n",
      ":           +- Scan ExistingRDD[_1#6062L,_2#6063,_3#6064]\n",
      "+- *Sort [study_program#6057L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(study_program#6057L, 200)\n",
      "      +- *Project [_1#6048L AS id#6055L, _2#6049 AS name#6056, _3#6050L AS study_program#6057L]\n",
      "         +- *Filter isnotnull(_3#6050L)\n",
      "            +- Scan ExistingRDD[_1#6048L,_2#6049,_3#6050L]\n",
      "== Physical Plan ==\n",
      "*BroadcastHashJoin [id#6069L], [study_program#6057L], Inner, BuildRight\n",
      ":- *Project [_1#6062L AS id#6069L, _2#6063 AS degree#6070, _3#6064 AS school#6071]\n",
      ":  +- *Filter isnotnull(_1#6062L)\n",
      ":     +- Scan ExistingRDD[_1#6062L,_2#6063,_3#6064]\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[2, bigint, true]))\n",
      "   +- *Project [_1#6048L AS id#6055L, _2#6049 AS name#6056, _3#6050L AS study_program#6057L]\n",
      "      +- *Filter isnotnull(_3#6050L)\n",
      "         +- Scan ExistingRDD[_1#6048L,_2#6049,_3#6050L]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "person = spark.createDataFrame([\n",
    "    (0, \"Ben Benson\", 0),\n",
    "    (1, \"Tom Tomhson\", 1),\n",
    "    (2, \"Jill Jilli\", 1)\n",
    "]).toDF(\"id\", \"name\", \"study_program\")\n",
    "\n",
    "studyProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"Great university\"),\n",
    "    (1, \"Masters\", \"Greater university\"),\n",
    "    (2, \"Ph.D.\", \"Greatest university\")\n",
    "]).toDF(\"id\", \"degree\", \"school\")\n",
    "studyProgram.show()\n",
    "\n",
    "join_expr = person[\"study_program\"] == studyProgram[\"id\"]\n",
    "joined_df = studyProgram.join(person, join_expr, \"inner\")\n",
    "joined_df.explain()\n",
    "\n",
    "joined_df = studyProgram.join(broadcast(person), join_expr, \"inner\")\n",
    "joined_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>_c0</th>\n",
       "      <th>_c1</th>\n",
       "      <th>_c2</th>\n",
       "      <th>_c3</th>\n",
       "      <th>_c4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>12.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>7.359800721939872</td>\n",
       "      <td>None</td>\n",
       "      <td>1.4433756729740645</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "      <td>ALGERIA</td>\n",
       "      <td>0</td>\n",
       "      <td>haggle. carefully final deposits detect slyly...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>9</td>\n",
       "      <td>VIETNAM</td>\n",
       "      <td>4</td>\n",
       "      <td>y final packages. slow foxes cajole quickly. q...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                _c0      _c1                 _c2  \\\n",
       "0   count                 25       25                  25   \n",
       "1    mean               12.0     None                 2.0   \n",
       "2  stddev  7.359800721939872     None  1.4433756729740645   \n",
       "3     min                  0  ALGERIA                   0   \n",
       "4     max                  9  VIETNAM                   4   \n",
       "\n",
       "                                                 _c3   _c4  \n",
       "0                                                 25     0  \n",
       "1                                               None  None  \n",
       "2                                               None  None  \n",
       "3   haggle. carefully final deposits detect slyly...  None  \n",
       "4  y final packages. slow foxes cajole quickly. q...  None  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "data.count()\n",
    "data.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>*</th>\n",
       "      <td>6001215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6001215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_c0</th>\n",
       "      <td>6001215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_c1</th>\n",
       "      <td>6001215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "*    6001215\n",
       "1    6001215\n",
       "_c0  6001215\n",
       "_c1  6001215"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, lit\n",
    "data = spark.read.format(\"csv\").option(\"sep\", \"|\").load(\"/home/orpington/dbgen-data/lineitem.tbl\")\n",
    "exprs = [count(\"*\").alias(\"*\")]\n",
    "exprs += [count(lit(1)).alias(\"1\")]\n",
    "exprs += [count(\"_c0\").alias(\"_c0\")]\n",
    "exprs += [count(\"_c1\").alias(\"_c1\")]\n",
    "data.select(exprs).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------------+\n",
      "|count(1)|count(DISTINCT _c0, _c1, _c2, _c3, _c4)|\n",
      "+--------+---------------------------------------+\n",
      "|      25|                                      0|\n",
      "+--------+---------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count(DISTINCT _c0)</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count(DISTINCT _c1)</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count(DISTINCT _c2)</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count(DISTINCT _c3)</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count(DISTINCT _c4)</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "count(DISTINCT _c0)  25\n",
       "count(DISTINCT _c1)  25\n",
       "count(DISTINCT _c2)   5\n",
       "count(DISTINCT _c3)  25\n",
       "count(DISTINCT _c4)   0"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "data.createOrReplaceTempView(\"nation\")\n",
    "spark.sql(\"select count(*), count(distinct *) from nation\").show()\n",
    "\n",
    "exprs = [countDistinct(column) for column in data.columns]\n",
    "data.select(exprs).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|approx_count_distinct(_c1)|\n",
      "+--------------------------+\n",
      "|                        24|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "data.createOrReplaceTempView(\"nation\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select approx_count_distinct(_c1, 0.1) from nation limit 1000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count(DISTINCT _c2)</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approx 0.3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approx 0.28</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approx 0.26</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approx 0.24</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approx 0.22</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approx 0.2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approx 0.18</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approx 0.16</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approx 0.14</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approx 0.12</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0\n",
       "count(DISTINCT _c2)  5\n",
       "approx 0.3           5\n",
       "approx 0.28          5\n",
       "approx 0.26          4\n",
       "approx 0.24          4\n",
       "approx 0.22          4\n",
       "approx 0.2           4\n",
       "approx 0.18          4\n",
       "approx 0.16          4\n",
       "approx 0.14          4\n",
       "approx 0.12          5"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct, count, countDistinct\n",
    "data = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "\n",
    "exprs = [countDistinct(\"_c2\")]\n",
    "exprs += [approx_count_distinct(\"_c2\", \n",
    "                               rsd=float(0.30 - (exponent * 0.02))).alias(\"approx \" + str(0.30 - (exponent * 0.02)))\n",
    "         for exponent \n",
    "         in range(10)]\n",
    "data.select(exprs).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>first(_c1, false)</th>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last(_c1, false)</th>\n",
       "      <td>UNITED STATES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first(_c1, true)</th>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last(_c1, true)</th>\n",
       "      <td>UNITED STATES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0\n",
       "first(_c1, false)        ALGERIA\n",
       "last(_c1, false)   UNITED STATES\n",
       "first(_c1, true)         ALGERIA\n",
       "last(_c1, true)    UNITED STATES"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "data = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "data.select(first(\"_c1\"), \n",
    "            last(\"_c1\"),\n",
    "            first(\"_c1\", ignorenulls=True), \n",
    "            last(\"_c1\", ignorenulls=True)\n",
    "           ).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+---------+---------+\n",
      "|sum(nums)|sum(DISTINCT nums)|max(nums)|min(nums)|\n",
      "+---------+------------------+---------+---------+\n",
      "|      135|                45|        9|        0|\n",
      "+---------+------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, array, explode, sum, sumDistinct, max, min\n",
    "spark.range(10).select(explode(array([col(\"id\")] * 3)).alias(\"nums\"))\\\n",
    "                 .select(sum(\"nums\"), sumDistinct(\"nums\"), max(\"nums\"), min(\"nums\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg(id)</th>\n",
       "      <td>499.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg(id)</th>\n",
       "      <td>499.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_pop(id)</th>\n",
       "      <td>83333.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_samp(id)</th>\n",
       "      <td>83416.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stddev_pop(id)</th>\n",
       "      <td>288.674990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stddev_samp(id)</th>\n",
       "      <td>288.819436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skewness(id)</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kurtosis(id)</th>\n",
       "      <td>-1.200002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "avg(id)            499.500000\n",
       "avg(id)            499.500000\n",
       "var_pop(id)      83333.250000\n",
       "var_samp(id)     83416.666667\n",
       "stddev_pop(id)     288.674990\n",
       "stddev_samp(id)    288.819436\n",
       "skewness(id)         0.000000\n",
       "kurtosis(id)        -1.200002"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    avg, mean, \n",
    "    var_pop, var_samp, stddev_pop, stddev_samp, \n",
    "    skewness, kurtosis\n",
    ")\n",
    "numeric_col = \"id\"\n",
    "spark.range(1000).select(avg(numeric_col), \n",
    "                    mean(numeric_col),\n",
    "                    var_pop(numeric_col),\n",
    "                    var_samp(numeric_col),\n",
    "                    stddev_pop(numeric_col),\n",
    "                    stddev_samp(numeric_col),\n",
    "                    skewness(numeric_col),\n",
    "                    kurtosis(numeric_col)).toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|_c2|counts|\n",
      "+---+------+\n",
      "|  3|     5|\n",
      "|  0|     5|\n",
      "|  1|     5|\n",
      "|  4|     5|\n",
      "|  2|     5|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "data.createOrReplaceTempView(\"nation\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  select _c2, \n",
    "         count(1) as counts \n",
    "    from nation \n",
    "group by _c2\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+\n",
      "|_c2|count(_c1)|      kurtosis(_c0)|\n",
      "+---+----------+-------------------+\n",
      "|  3|         5| -1.781599351753203|\n",
      "|  0|         5|-1.4312812469365752|\n",
      "|  1|         5| -1.471609234176733|\n",
      "|  4|         5|-0.6537753970186402|\n",
      "|  2|         5| -1.558881039787595|\n",
      "+---+----------+-------------------+\n",
      "\n",
      "+---+--------+----------+\n",
      "|_c2|count(1)|count(_c1)|\n",
      "+---+--------+----------+\n",
      "|  3|       5|         5|\n",
      "|  0|       5|         5|\n",
      "|  1|       5|         5|\n",
      "|  4|       5|         5|\n",
      "|  2|       5|         5|\n",
      "+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, expr\n",
    "data = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "data.createOrReplaceTempView(\"nation\")\n",
    "\n",
    "data.groupby(\"_c2\").agg(count(\"_c1\"), expr(\"kurtosis(_c0)\")).show()\n",
    "data.groupBy(\"_c2\").agg({\"_c1\": \"count\", \"*\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---+----+----------+------+\n",
      "|_c0|           _c1|_c2|rank|dense_rank|window|\n",
      "+---+--------------+---+----+----------+------+\n",
      "|  7|       GERMANY|  3|   1|         1|   7.0|\n",
      "|  6|        FRANCE|  3|   2|         2|   7.0|\n",
      "| 23|UNITED KINGDOM|  3|   3|         3|  23.0|\n",
      "| 22|        RUSSIA|  3|   4|         4|  23.0|\n",
      "| 19|       ROMANIA|  3|   5|         5|  23.0|\n",
      "|  5|      ETHIOPIA|  0|   1|         1|   5.0|\n",
      "| 16|    MOZAMBIQUE|  0|   2|         2|  16.0|\n",
      "| 15|       MOROCCO|  0|   3|         3|  16.0|\n",
      "| 14|         KENYA|  0|   4|         4|  16.0|\n",
      "|  0|       ALGERIA|  0|   5|         5|  16.0|\n",
      "|  3|        CANADA|  1|   1|         1|   3.0|\n",
      "| 24| UNITED STATES|  1|   2|         2|  24.0|\n",
      "|  2|        BRAZIL|  1|   3|         3|  24.0|\n",
      "| 17|          PERU|  1|   4|         4|  24.0|\n",
      "|  1|     ARGENTINA|  1|   5|         5|  24.0|\n",
      "|  4|         EGYPT|  4|   1|         1|   4.0|\n",
      "| 20|  SAUDI ARABIA|  4|   2|         2|  20.0|\n",
      "| 13|        JORDAN|  4|   3|         3|  20.0|\n",
      "| 11|          IRAQ|  4|   4|         4|  20.0|\n",
      "| 10|          IRAN|  4|   5|         5|  20.0|\n",
      "|  9|     INDONESIA|  2|   1|         1|   9.0|\n",
      "|  8|         INDIA|  2|   2|         2|   9.0|\n",
      "| 21|       VIETNAM|  2|   3|         3|  21.0|\n",
      "| 18|         CHINA|  2|   4|         4|  21.0|\n",
      "| 12|         JAPAN|  2|   5|         5|  21.0|\n",
      "+---+--------------+---+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, max, col, dense_rank, rank\n",
    "\n",
    "data = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "data.createOrReplaceTempView(\"nation\")\n",
    "\n",
    "window_spec = Window.partitionBy(\"_c2\")\\\n",
    "                    .orderBy(desc(\"_c0\"))\\\n",
    "                    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "data.select(\"_c0\",\n",
    "            \"_c1\", \n",
    "            \"_c2\",\n",
    "            rank().over(window_spec).alias(\"rank\"),\n",
    "            dense_rank().over(window_spec).alias(\"dense_rank\"),            \n",
    "            max(col(\"_c0\").cast(\"float\")).over(window_spec).alias(\"window\")).show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+------+------+-----+--------+------+-------+-----+---------+\n",
      "|      _c1|ALGERIA|ARGENTINA|BRAZIL|CANADA|EGYPT|ETHIOPIA|FRANCE|GERMANY|INDIA|INDONESIA|\n",
      "+---------+-------+---------+------+------+-----+--------+------+-------+-----+---------+\n",
      "|  ALGERIA|      1|     null|  null|  null| null|    null|  null|   null| null|     null|\n",
      "|ARGENTINA|   null|        1|  null|  null| null|    null|  null|   null| null|     null|\n",
      "|   BRAZIL|   null|     null|     1|  null| null|    null|  null|   null| null|     null|\n",
      "|   CANADA|   null|     null|  null|     1| null|    null|  null|   null| null|     null|\n",
      "|    EGYPT|   null|     null|  null|  null|    1|    null|  null|   null| null|     null|\n",
      "| ETHIOPIA|   null|     null|  null|  null| null|       1|  null|   null| null|     null|\n",
      "|   FRANCE|   null|     null|  null|  null| null|    null|     1|   null| null|     null|\n",
      "|  GERMANY|   null|     null|  null|  null| null|    null|  null|      1| null|     null|\n",
      "|    INDIA|   null|     null|  null|  null| null|    null|  null|   null|    1|     null|\n",
      "|INDONESIA|   null|     null|  null|  null| null|    null|  null|   null| null|        1|\n",
      "+---------+-------+---------+------+------+-----+--------+------+-------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "data = spark.read.csv(\"/home/orpington/dbgen-data/nation.tbl\", sep=\"|\")\n",
    "data.createOrReplaceTempView(\"nation\")\n",
    "\n",
    "data.limit(10).groupby(\"_c1\").pivot(\"_c1\").agg(count(col(\"_c0\").cast(\"decimal\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

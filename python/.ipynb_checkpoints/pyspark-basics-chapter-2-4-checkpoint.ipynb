{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.range(4).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json\n",
      "--------------------------------------------------------------------\n",
      "{\"col1\":{\"col\":\"tents\",\"id\":25769803776},\"today\":\"2017-09-19\"}\n",
      "{\"col1\":{\"col\":\"proofs\",\"id\":25769803777},\"today\":\"2017-09-19\"}\n",
      "{\"col1\":{\"col\":\"deletion\",\"id\":25769803778},\"today\":\"2017-09-19\"}\n",
      "{\"col1\":{\"col\":\"tuesdays\",\"id\":25769803779},\"today\":\"2017-09-19\"}\n",
      "{\"col1\":{\"col\":\"settings\",\"id\":25769803780},\"today\":\"2017-09-19\"}\n",
      "{\"col1\":{\"col\":\"headsets\",\"id\":25769803781},\"today\":\"2017-09-19\"}\n",
      "{\"col1\":{\"col\":\"cleanliness\",\"id\":25769803782},\"today\":\"2017-09-19\"}\n",
      "{\"col1\":{\"col\":\"concern\",\"id\":25769803783},\"today\":\"2017-09-19\"}\n",
      "{\"col1\":{\"col\":\"picks\",\"id\":25769803784},\"today\":\"2017-09-19\"}\n",
      "{\"col1\":{\"col\":\"warship\",\"id\":25769803785},\"today\":\"2017-09-19\"}\n",
      "\n",
      "root\n",
      " |-- json: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ArrayType, StringType, IntegerType\n",
    "from pyspark.sql.functions import to_json, col, get_json_object, struct, current_date\n",
    "from pyspark.sql.functions import array, lit, explode, monotonically_increasing_id\n",
    "from random_words import RandomWords\n",
    "from tabulate import tabulate\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(\"name1\", StringType(), True),\n",
    "     StructField(\"name2\", StringType(), True)])\n",
    "\n",
    "random_words_expr = array([lit(RandomWords().random_word()) \n",
    "                           for _ in range(10)])\n",
    "\n",
    "inner_struct = struct(col(\"col\"), col(\"id\"))\n",
    "outer_struct = struct(inner_struct, current_date().alias(\"today\"))\n",
    "                      \n",
    "df = spark.range(1)\\\n",
    "          .select(explode(random_words_expr))\\\n",
    "          .withColumn(\"id\", monotonically_increasing_id())\\\n",
    "          .select(to_json(outer_struct).alias(\"json\"))\n",
    "print(tabulate(df.collect(), df.columns))\n",
    "print(\"\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|<lambda>(id)|\n",
      "+------------+\n",
      "|           0|\n",
      "|           1|\n",
      "|           8|\n",
      "|          27|\n",
      "|          64|\n",
      "|         125|\n",
      "|         216|\n",
      "|         343|\n",
      "|         512|\n",
      "|         729|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "spark.range(10).select(udf(lambda x: x**3)(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ints      strings    floats\n",
      "------  ---------  --------\n",
      "                0         0\n",
      "                1         1\n",
      "                8         8\n",
      "               27        27\n",
      "               64        64\n",
      "              125       125\n",
      "              216       216\n",
      "              343       343\n",
      "              512       512\n",
      "              729       729\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "spark.udf.register(\"go-go-go\", lambda x: x**3*1.0, IntegerType())\n",
    "spark.udf.register(\"go-go-go2\", lambda x: x**3*1.0, StringType())\n",
    "spark.udf.register(\"go-go-go3\", lambda x: x**3*1.0, DoubleType())\n",
    "print(tabulate(spark.range(10)\n",
    "                    .selectExpr(\"`go-go-go`(id)\", \"`go-go-go2`(id)\", \"`go-go-go3`(id)\").collect(), \n",
    "               [\"Ints\", \"strings\", \"floats\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

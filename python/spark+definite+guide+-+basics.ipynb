{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([1,2,3,4]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Row in module pyspark.sql.types:\n",
      "\n",
      "class Row(__builtin__.tuple)\n",
      " |  A row in L{DataFrame}.\n",
      " |  The fields in it can be accessed:\n",
      " |  \n",
      " |  * like attributes (``row.key``)\n",
      " |  * like dictionary values (``row[key]``)\n",
      " |  \n",
      " |  ``key in row`` will search through row keys.\n",
      " |  \n",
      " |  Row can be used to create a row object by using named arguments,\n",
      " |  the fields will be sorted by names. It is not allowed to omit\n",
      " |  a named argument to represent the value is None or missing. This should be\n",
      " |  explicitly set to None in this case.\n",
      " |  \n",
      " |  >>> row = Row(name=\"Alice\", age=11)\n",
      " |  >>> row\n",
      " |  Row(age=11, name='Alice')\n",
      " |  >>> row['name'], row['age']\n",
      " |  ('Alice', 11)\n",
      " |  >>> row.name, row.age\n",
      " |  ('Alice', 11)\n",
      " |  >>> 'name' in row\n",
      " |  True\n",
      " |  >>> 'wrong_key' in row\n",
      " |  False\n",
      " |  \n",
      " |  Row also can be used to create another Row like class, then it\n",
      " |  could be used to create Row objects, such as\n",
      " |  \n",
      " |  >>> Person = Row(\"name\", \"age\")\n",
      " |  >>> Person\n",
      " |  <Row(name, age)>\n",
      " |  >>> 'name' in Person\n",
      " |  True\n",
      " |  >>> 'wrong_key' in Person\n",
      " |  False\n",
      " |  >>> Person(\"Alice\", 11)\n",
      " |  Row(name='Alice', age=11)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Row\n",
      " |      __builtin__.tuple\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      create new Row object\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Returns a tuple so Python knows how to pickle Row.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Printable representation of Row used in Python REPL.\n",
      " |  \n",
      " |  __setattr__(self, key, value)\n",
      " |  \n",
      " |  asDict(self, recursive=False)\n",
      " |      Return as an dict\n",
      " |      \n",
      " |      :param recursive: turns the nested Row as dict (default: False).\n",
      " |      \n",
      " |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n",
      " |      True\n",
      " |      >>> row = Row(key=1, value=Row(name='a', age=2))\n",
      " |      >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}\n",
      " |      True\n",
      " |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(self, *args, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from __builtin__.tuple:\n",
      " |  \n",
      " |  __add__(...)\n",
      " |      x.__add__(y) <==> x+y\n",
      " |  \n",
      " |  __eq__(...)\n",
      " |      x.__eq__(y) <==> x==y\n",
      " |  \n",
      " |  __ge__(...)\n",
      " |      x.__ge__(y) <==> x>=y\n",
      " |  \n",
      " |  __getattribute__(...)\n",
      " |      x.__getattribute__('name') <==> x.name\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __getslice__(...)\n",
      " |      x.__getslice__(i, j) <==> x[i:j]\n",
      " |      \n",
      " |      Use of negative indices is not supported.\n",
      " |  \n",
      " |  __gt__(...)\n",
      " |      x.__gt__(y) <==> x>y\n",
      " |  \n",
      " |  __hash__(...)\n",
      " |      x.__hash__() <==> hash(x)\n",
      " |  \n",
      " |  __iter__(...)\n",
      " |      x.__iter__() <==> iter(x)\n",
      " |  \n",
      " |  __le__(...)\n",
      " |      x.__le__(y) <==> x<=y\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      x.__len__() <==> len(x)\n",
      " |  \n",
      " |  __lt__(...)\n",
      " |      x.__lt__(y) <==> x<y\n",
      " |  \n",
      " |  __mul__(...)\n",
      " |      x.__mul__(n) <==> x*n\n",
      " |  \n",
      " |  __ne__(...)\n",
      " |      x.__ne__(y) <==> x!=y\n",
      " |  \n",
      " |  __rmul__(...)\n",
      " |      x.__rmul__(n) <==> n*x\n",
      " |  \n",
      " |  count(...)\n",
      " |      T.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  index(...)\n",
      " |      T.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [Row(id=num, name1=st, name2=st) for num, st in enumerate([\"abc\", \"something\", \"abc\", \"excellent\"])]\n",
    "df = spark.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name1: string (nullable = true)\n",
      " |-- name2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "| id|    name1|    name2|\n",
      "+---+---------+---------+\n",
      "|  0|      abc|      abc|\n",
      "|  1|something|something|\n",
      "|  2|      abc|      abc|\n",
      "|  3|excellent|excellent|\n",
      "+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"name1\", \"name2\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+-----+\n",
      "| id|    name1|    name2| flag|\n",
      "+---+---------+---------+-----+\n",
      "|  0|      abc|      abc| true|\n",
      "|  1|something|something|false|\n",
      "|  2|      abc|      abc| true|\n",
      "|  3|excellent|excellent|false|\n",
      "+---+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"flag\", expr(\"name1 == 'abc'\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://docs.python.org/3.5/library/struct.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://stackoverflow.com/questions/17958347/how-can-i-convert-a-python-urandom-to-a-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 18|\n",
      "| 52|\n",
      "|392|\n",
      "|453|\n",
      "|532|\n",
      "|757|\n",
      "|921|\n",
      "|957|\n",
      "|986|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import os\n",
    "urandom_long = struct.unpack(\"l\", os.urandom(8))[0]\n",
    "spark.range(1000).sample(True, 0.01, urandom_long).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 21, 19]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d.count() for d in spark.range(100).randomSplit([0.75, 0.25, 0.25], 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, df2, df3 = spark.range(100).randomSplit([0.75, 0.25, 0.25], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(df1.union(df2).count())\n",
    "print(df3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "| id|    name1|    name2|\n",
      "+---+---------+---------+\n",
      "|  0|      abc|      abc|\n",
      "|  2|      abc|      abc|\n",
      "|  3|excellent|excellent|\n",
      "|  1|something|something|\n",
      "+---+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"name1\", \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "| id|    name1|    name2|\n",
      "+---+---------+---------+\n",
      "|  0|      abc|      abc|\n",
      "|  2|      abc|      abc|\n",
      "|  3|excellent|excellent|\n",
      "|  1|something|something|\n",
      "+---+---------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.orderBy(col(\"name1\"), col(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://txt.arboreus.com/2013/03/13/pretty-print-tables-in-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  --\n",
      "spam   1\n",
      "eggs  42\n",
      "----  --\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([[\"spam\", 1], [\"eggs\", 42]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item      quantity\n",
      "------  ----------\n",
      "spam             1\n",
      "eggs            42\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([[\"spam\", 1], [\"eggs\", 42]], [\"item\", \"quantity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id  name1      name2\n",
      "----  ---------  ---------\n",
      "   0  abc        abc\n",
      "   1  something  something\n",
      "   2  abc        abc\n",
      "   3  excellent  excellent\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(df.collect(), df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id  name1      name2\n",
      "----  ---------  ---------\n",
      "   0  abc        abc\n",
      "   1  something  something\n",
      "   2  abc        abc\n",
      "   3  excellent  excellent\n",
      "\n",
      "  id  name1      name2\n",
      "----  ---------  ---------\n",
      "   0  abc        abc\n",
      "   2  abc        abc\n",
      "   3  excellent  excellent\n",
      "   1  something  something\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc, desc\n",
    "print(tabulate(df.sortWithinPartitions(asc(\"name1\")).collect(), df.columns))\n",
    "print(\"\")\n",
    "print(tabulate(df.sort(asc(\"name1\")).collect(), df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|999|\n",
      "|998|\n",
      "|997|\n",
      "|996|\n",
      "|995|\n",
      "|994|\n",
      "|993|\n",
      "|992|\n",
      "|991|\n",
      "|990|\n",
      "|989|\n",
      "|988|\n",
      "|987|\n",
      "|986|\n",
      "|985|\n",
      "|984|\n",
      "|983|\n",
      "|982|\n",
      "|981|\n",
      "|980|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1000).sort(desc(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|249|\n",
      "|248|\n",
      "|247|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1000).sortWithinPartitions(expr(\"-id\")).limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method repartition in module pyspark.sql.dataframe:\n",
      "\n",
      "repartition(self, numPartitions, *cols) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      "    resulting DataFrame is hash partitioned.\n",
      "    \n",
      "    ``numPartitions`` can be an int to specify the target number of partitions or a Column.\n",
      "    If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      "    the default number of partitions is used.\n",
      "    \n",
      "    .. versionchanged:: 1.6\n",
      "       Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      "       optional if partitioning columns are specified.\n",
      "    \n",
      "    >>> df.repartition(10).rdd.getNumPartitions()\n",
      "    10\n",
      "    >>> data = df.union(df).repartition(\"age\")\n",
      "    >>> data.show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  5|  Bob|\n",
      "    |  5|  Bob|\n",
      "    |  2|Alice|\n",
      "    |  2|Alice|\n",
      "    +---+-----+\n",
      "    >>> data = data.repartition(7, \"age\")\n",
      "    >>> data.show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  2|Alice|\n",
      "    |  5|  Bob|\n",
      "    |  2|Alice|\n",
      "    |  5|  Bob|\n",
      "    +---+-----+\n",
      "    >>> data.rdd.getNumPartitions()\n",
      "    7\n",
      "    >>> data = data.repartition(\"name\", \"age\")\n",
      "    >>> data.show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  5|  Bob|\n",
      "    |  5|  Bob|\n",
      "    |  2|Alice|\n",
      "    |  2|Alice|\n",
      "    +---+-----+\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.repartition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_jcols',\n",
       " '_jdf',\n",
       " '_jmap',\n",
       " '_jseq',\n",
       " '_lazy_rdd',\n",
       " '_sc',\n",
       " '_schema',\n",
       " '_sort_cols',\n",
       " 'agg',\n",
       " 'alias',\n",
       " 'approxQuantile',\n",
       " 'cache',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'collect',\n",
       " 'columns',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'createGlobalTempView',\n",
       " 'createOrReplaceGlobalTempView',\n",
       " 'createOrReplaceTempView',\n",
       " 'createTempView',\n",
       " 'crossJoin',\n",
       " 'crosstab',\n",
       " 'cube',\n",
       " 'describe',\n",
       " 'distinct',\n",
       " 'drop',\n",
       " 'dropDuplicates',\n",
       " 'drop_duplicates',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'explain',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'freqItems',\n",
       " 'groupBy',\n",
       " 'groupby',\n",
       " 'head',\n",
       " 'hint',\n",
       " 'intersect',\n",
       " 'isLocal',\n",
       " 'isStreaming',\n",
       " 'is_cached',\n",
       " 'join',\n",
       " 'limit',\n",
       " 'na',\n",
       " 'orderBy',\n",
       " 'persist',\n",
       " 'printSchema',\n",
       " 'randomSplit',\n",
       " 'rdd',\n",
       " 'registerTempTable',\n",
       " 'repartition',\n",
       " 'replace',\n",
       " 'rollup',\n",
       " 'sample',\n",
       " 'sampleBy',\n",
       " 'schema',\n",
       " 'select',\n",
       " 'selectExpr',\n",
       " 'show',\n",
       " 'sort',\n",
       " 'sortWithinPartitions',\n",
       " 'sql_ctx',\n",
       " 'stat',\n",
       " 'storageLevel',\n",
       " 'subtract',\n",
       " 'take',\n",
       " 'toDF',\n",
       " 'toJSON',\n",
       " 'toLocalIterator',\n",
       " 'toPandas',\n",
       " 'union',\n",
       " 'unionAll',\n",
       " 'unpersist',\n",
       " 'where',\n",
       " 'withColumn',\n",
       " 'withColumnRenamed',\n",
       " 'withWatermark',\n",
       " 'write',\n",
       " 'writeStream']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://stackoverflow.com/questions/35973590/pyspark-partioning-data-using-partitionby#35973860\n",
    "Some resources claim the number of partitions should around twice as large as the number of available cores. From the other hand a single partition typically shouldn't contain more than 128MB and a single shuffle block cannot be larger than 2GB (See SPARK-6235)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[],\n",
      " [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24],\n",
      " [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23],\n",
      " []]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "result = spark.range(25).repartition(expr(\"(id % 2) + 1\")).coalesce(4).rdd.map(lambda r: r.id).glom().collect()\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 2]), ('b', [1])]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "def a(a):\n",
    "    return [a]\n",
    "\n",
    "def b(a, b):\n",
    "    a += [b]\n",
    "    return a\n",
    "\n",
    "def c(a, b):\n",
    "    a += b\n",
    "    return a\n",
    "\n",
    "x.combineByKey(a, b, c).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

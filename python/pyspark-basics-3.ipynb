{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', ' ', 'i', 'd', '\\n', '-', '-', '-', '-', '\\n', ' ', ' ', ' ', '0', '\\n', ' ', ' ', ' ', '1', '\\n']\n",
      "['\\n', ' ', ' ', 'i', 'd', '\\n', '-', '-', '-', '-', '\\n', ' ', ' ', ' ', '0', '\\n', ' ', ' ', ' ', '1', '\\n']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from tabulate import tabulate\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "expected = \\\n",
    "\"\"\"\n",
    "  id\n",
    "----\n",
    "   0\n",
    "   1\n",
    "\"\"\"\n",
    "df = spark.range(2)\n",
    "got = \"\\n{}\\n\".format(tabulate(df.collect(), df.columns))\n",
    "assert expected == got\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      word|\n",
      "+---+----------+\n",
      "|  0|  stocking|\n",
      "|  1| gyroscope|\n",
      "|  2| injectors|\n",
      "|  3|letterhead|\n",
      "+---+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random_words import RandomWords\n",
    "from pyspark.sql import Row\n",
    "rows = [Row(i, RandomWords().random_word()) for i in range(100)]\n",
    "df = spark.createDataFrame(rows, [\"id\", \"word\"])\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----------------------+\n",
      "|id |word     |today     |now                    |\n",
      "+---+---------+----------+-----------------------+\n",
      "|0  |stocking |2017-09-18|2017-09-18 21:28:17.711|\n",
      "|1  |gyroscope|2017-09-18|2017-09-18 21:28:17.711|\n",
      "|2  |injectors|2017-09-18|2017-09-18 21:28:17.711|\n",
      "+---+---------+----------+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "dateDf = df.withColumn(\"today\", current_date())\\\n",
    "           .withColumn(\"now\", current_timestamp())\n",
    "dateDf.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- word: string (nullable = true)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_add(today, 5)|date_sub(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "|        2017-09-23|        2017-09-13|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDf.select(date_add(col(\"today\"), 5), date_sub(col(\"today\"), 5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----------+----------+-------------------+----------+\n",
      "|dif|     start|       end|     wrong|      unix|          unix_time|month_diff|\n",
      "+---+----------+----------+----------+----------+-------------------+----------+\n",
      "|  5|2017-02-02|2020-10-02|      null|1546390922|2019-02-02 00:02:02|     -44.0|\n",
      "|  5|2017-02-02|2020-10-02|2020-02-01|1546390922|2019-02-02 00:02:02|     -44.0|\n",
      "|  5|2017-02-02|2020-10-02|2020-04-01|1546390922|2019-02-02 00:02:02|     -44.0|\n",
      "|  5|2017-02-02|2020-10-02|2020-06-01|1546390922|2019-02-02 00:02:02|     -44.0|\n",
      "|  5|2017-02-02|2020-10-02|2020-08-01|1546390922|2019-02-02 00:02:02|     -44.0|\n",
      "|  5|2017-02-02|2020-10-02|2020-10-01|1546390922|2019-02-02 00:02:02|     -44.0|\n",
      "|  5|2017-02-02|2020-10-02|2020-12-01|1546390922|2019-02-02 00:02:02|     -44.0|\n",
      "|  5|2017-02-02|2020-10-02|      null|1546390922|2019-02-02 00:02:02|     -44.0|\n",
      "|  5|2017-02-02|2020-10-02|      null|1546390922|2019-02-02 00:02:02|     -44.0|\n",
      "|  5|2017-02-02|2020-10-02|      null|1546390922|2019-02-02 00:02:02|     -44.0|\n",
      "+---+----------+----------+----------+----------+-------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "today = col(\"today\")\n",
    "week_ago = date_sub(col(\"today\"), 5)\n",
    "exprs = [datediff(today, week_ago).alias(\"dif\")]\n",
    "exprs += [to_date(lit(\"2017-02-02\")).alias(\"start\")]\n",
    "exprs += [to_date(lit(\"2020-10-02\")).alias(\"end\")]\n",
    "exprs += [to_date(concat(lit(\"2020-\"),lpad(col(\"id\"), 2, \"0\"), lit(\"-01\"))).alias(\"wrong\")]\n",
    "exprs += [unix_timestamp(lit(\"2019-02-02 02:02:02\"), \"yyyy-mm-dd HH:mm:ss\").alias(\"unix\")]\n",
    "exprs += [from_unixtime(lit(\"1546383722\"), \"yyyy-mm-dd HH:mm:ss\").alias(\"unix_time\")]\n",
    "dateDf2 = dateDf.where(col(\"id\") % 2 == 0).select(exprs).withColumn(\"month_diff\", months_between(col(\"start\"), col(\"end\")))\n",
    "dateDf2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dif  start       end         wrong             unix  unix_time              month_diff\n",
      "-----  ----------  ----------  ----------  ----------  -------------------  ------------\n",
      "    5  2017-02-02  2020-10-02              1546390922  2019-02-02 00:02:02           -44\n",
      "    5  2017-02-02  2020-10-02  2020-02-01  1546390922  2019-02-02 00:02:02           -44\n",
      "    5  2017-02-02  2020-10-02  2020-04-01  1546390922  2019-02-02 00:02:02           -44\n"
     ]
    }
   ],
   "source": [
    "limit_rows = 3\n",
    "print(tabulate(dateDf2.limit(limit_rows).collect(), dateDf2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dif  start       end         wrong             unix  unix_time              month_diff\n",
      "-----  ----------  ----------  ----------  ----------  -------------------  ------------\n",
      "    5  2017-02-02  2020-10-02  2020-02-01  1546390922  2019-02-02 00:02:02           -44\n",
      "    5  2017-02-02  2020-10-02  2020-04-01  1546390922  2019-02-02 00:02:02           -44\n",
      "    5  2017-02-02  2020-10-02  2020-06-01  1546390922  2019-02-02 00:02:02           -44\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(dateDf2.na.drop(\"any\").limit(limit_rows).collect(), dateDf2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dif  start       end         wrong             unix  unix_time              month_diff\n",
      "-----  ----------  ----------  ----------  ----------  -------------------  ------------\n",
      "    5  2017-02-02  2020-10-02              1546390922  2019-02-02 00:02:02           -44\n",
      "    5  2017-02-02  2020-10-02  2020-02-01  1546390922  2019-02-02 00:02:02           -44\n",
      "    5  2017-02-02  2020-10-02  2020-04-01  1546390922  2019-02-02 00:02:02           -44\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(dateDf2.na.drop(\"all\").limit(limit_rows).collect(), dateDf2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dif  start       end         wrong             unix  unix_time              month_diff\n",
      "-----  ----------  ----------  ----------  ----------  -------------------  ------------\n",
      "    5  2017-02-02  2020-10-02  2020-02-01  1546390922  2019-02-02 00:02:02           -44\n",
      "    5  2017-02-02  2020-10-02  2020-04-01  1546390922  2019-02-02 00:02:02           -44\n",
      "    5  2017-02-02  2020-10-02  2020-06-01  1546390922  2019-02-02 00:02:02           -44\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(dateDf2.na.drop(\"any\", subset=[\"wrong\", \"unix\"]).limit(limit_rows).collect(), dateDf2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dif  start       end         wrong             unix  unix_time              month_diff\n",
      "-----  ----------  ----------  ----------  ----------  -------------------  ------------\n",
      "    5  2017-02-02  2020-10-02              1546390922  2019-02-02 00:02:02           -44\n",
      "    5  2017-02-02  2020-10-02  2020-02-01  1546390922  2019-02-02 00:02:02           -44\n",
      "    5  2017-02-02  2020-10-02  2020-04-01  1546390922  2019-02-02 00:02:02           -44\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(dateDf2.na.drop(\"all\", subset=[\"wrong\", \"unix\"]).limit(limit_rows).collect(), dateDf2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|col0| id|\n",
      "+----+---+\n",
      "|   0|  0|\n",
      "|null|  0|\n",
      "|   1|  1|\n",
      "|null|  1|\n",
      "|   2|  2|\n",
      "|null|  2|\n",
      "|   3|  3|\n",
      "|null|  3|\n",
      "|   4|  4|\n",
      "|null|  4|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).selectExpr(\"stack(2, id)\", \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col0|\n",
      "+----+\n",
      "|   0|\n",
      "|  34|\n",
      "|   1|\n",
      "|  34|\n",
      "|   2|\n",
      "|  34|\n",
      "|   3|\n",
      "|  34|\n",
      "|   4|\n",
      "|  34|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).selectExpr(\"stack(2, id)\").na.fill(34, subset=[\"col0\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|col0|\n",
      "+----+\n",
      "|   0|\n",
      "|null|\n",
      "|   1|\n",
      "|null|\n",
      "|  44|\n",
      "|null|\n",
      "|   3|\n",
      "|null|\n",
      "|   4|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).selectExpr(\"stack(2, id)\").na.replace([2], [44], \"col0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|             complex|     start|       end|\n",
      "+--------------------+----------+----------+\n",
      "|   [1546390922,null]|2017-02-02|2020-10-02|\n",
      "|[1546390922,2020-...|2017-02-02|2020-10-02|\n",
      "|[1546390922,2020-...|2017-02-02|2020-10-02|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDf2.selectExpr(\"(unix, wrong) as complex\", \"start\", \"end\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      unix|\n",
      "+----------+\n",
      "|1546390922|\n",
      "|1546390922|\n",
      "|1546390922|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDf2.selectExpr(\"struct(unix, wrong) as complex\", \"start\", \"end\")\\\n",
    "       .select(\"complex.unix\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+--------------------+\n",
      "|      unix|     wrong|    expand|             complex|\n",
      "+----------+----------+----------+--------------------+\n",
      "|1546390922|      null|      null|   [1546390922,null]|\n",
      "|1546390922|2020-02-01|2020-02-01|[1546390922,2020-...|\n",
      "|1546390922|2020-04-01|2020-04-01|[1546390922,2020-...|\n",
      "+----------+----------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDf2.select(struct(col(\"unix\"), col(\"wrong\")).alias(\"complex\"))\\\n",
    "       .withColumn(\"expand\", col(\"complex.wrong\"))\\\n",
    "       .select(\"complex.*\", \"expand\", \"complex\")\\\n",
    "       .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_df = spark.read.text(\"*.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              arr[0]|              arr[0]|\n",
      "+--------------------+--------------------+\n",
      "|                 def|                 def|\n",
      "|              import|              import|\n",
      "|              import|              import|\n",
      "|                    |                    |\n",
      "|                   x|                   x|\n",
      "|                   y|                   y|\n",
      "|                    |                    |\n",
      "|                with|                with|\n",
      "|               reg_x|               reg_x|\n",
      "|               reg_y|               reg_y|\n",
      "|                    |                    |\n",
      "|peachpy.x86_64.LO...|peachpy.x86_64.LO...|\n",
      "|peachpy.x86_64.LO...|peachpy.x86_64.LO...|\n",
      "|                 for|                 for|\n",
      "|peachpy.x86_64.AD...|peachpy.x86_64.AD...|\n",
      "|                    |                    |\n",
      "|peachpy.x86_64.RE...|peachpy.x86_64.RE...|\n",
      "|                    |                    |\n",
      "|              py_fun|              py_fun|\n",
      "|         print(\"done|         print(\"done|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.select(split(trim(col(\"value\")), \" \").alias(\"arr\"))\\\n",
    "       .select(expr(\"arr[0]\"), expr(\"arr\").getItem(0))\\\n",
    "       .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------------------------------+\n",
      "|arr                                   |expld                          |\n",
      "+--------------------------------------+-------------------------------+\n",
      "|[def, peachpy_one():]                 |def                            |\n",
      "|[def, peachpy_one():]                 |peachpy_one():                 |\n",
      "|[def, peachpy_conways_game_of_life():]|def                            |\n",
      "|[def, peachpy_conways_game_of_life():]|peachpy_conways_game_of_life():|\n",
      "|[def, test_this_test_will_pass(self):]|def                            |\n",
      "|[def, test_this_test_will_pass(self):]|test_this_test_will_pass(self):|\n",
      "|[def, test_not_equal(self):]          |def                            |\n",
      "|[def, test_not_equal(self):]          |test_not_equal(self):          |\n",
      "|[def, test_equality(self):]           |def                            |\n",
      "|[def, test_equality(self):]           |test_equality(self):           |\n",
      "+--------------------------------------+-------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.select(split(trim(col(\"value\")), \" \").alias(\"arr\"))\\\n",
    "       .where(array_contains(col(\"arr\"), \"def\"))\\\n",
    "       .withColumn(\"expld\", explode(col(\"arr\")))\\\n",
    "       .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|collect_list(id)|\n",
      "+----------------+\n",
      "| [0, 1, 2, 3, 4]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).select(collect_list(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AutoBatchedSerializer',\n",
       " 'Column',\n",
       " 'DataFrame',\n",
       " 'DataType',\n",
       " 'PickleSerializer',\n",
       " 'SparkContext',\n",
       " 'StringType',\n",
       " 'UserDefinedFunction',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '_binary_mathfunctions',\n",
       " '_create_binary_mathfunction',\n",
       " '_create_function',\n",
       " '_create_window_function',\n",
       " '_functions',\n",
       " '_functions_1_4',\n",
       " '_functions_1_6',\n",
       " '_functions_2_1',\n",
       " '_functions_2_2',\n",
       " '_parse_datatype_string',\n",
       " '_prepare_for_python_RDD',\n",
       " '_string_functions',\n",
       " '_test',\n",
       " '_to_java_column',\n",
       " '_to_seq',\n",
       " '_window_functions',\n",
       " '_wrap_function',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'add_months',\n",
       " 'approxCountDistinct',\n",
       " 'approx_count_distinct',\n",
       " 'array',\n",
       " 'array_contains',\n",
       " 'asc',\n",
       " 'ascii',\n",
       " 'asin',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'avg',\n",
       " 'base64',\n",
       " 'bin',\n",
       " 'bitwiseNOT',\n",
       " 'blacklist',\n",
       " 'broadcast',\n",
       " 'bround',\n",
       " 'cbrt',\n",
       " 'ceil',\n",
       " 'coalesce',\n",
       " 'col',\n",
       " 'collect_list',\n",
       " 'collect_set',\n",
       " 'column',\n",
       " 'concat',\n",
       " 'concat_ws',\n",
       " 'conv',\n",
       " 'corr',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'count',\n",
       " 'countDistinct',\n",
       " 'covar_pop',\n",
       " 'covar_samp',\n",
       " 'crc32',\n",
       " 'create_map',\n",
       " 'cume_dist',\n",
       " 'current_date',\n",
       " 'current_timestamp',\n",
       " 'date_add',\n",
       " 'date_format',\n",
       " 'date_sub',\n",
       " 'datediff',\n",
       " 'dayofmonth',\n",
       " 'dayofyear',\n",
       " 'decode',\n",
       " 'degrees',\n",
       " 'dense_rank',\n",
       " 'desc',\n",
       " 'encode',\n",
       " 'exp',\n",
       " 'explode',\n",
       " 'expm1',\n",
       " 'expr',\n",
       " 'factorial',\n",
       " 'first',\n",
       " 'floor',\n",
       " 'format_number',\n",
       " 'format_string',\n",
       " 'from_json',\n",
       " 'from_unixtime',\n",
       " 'from_utc_timestamp',\n",
       " 'functools',\n",
       " 'get_json_object',\n",
       " 'greatest',\n",
       " 'grouping',\n",
       " 'grouping_id',\n",
       " 'hash',\n",
       " 'hex',\n",
       " 'hour',\n",
       " 'hypot',\n",
       " 'ignore_unicode_prefix',\n",
       " 'initcap',\n",
       " 'input_file_name',\n",
       " 'instr',\n",
       " 'isnan',\n",
       " 'isnull',\n",
       " 'json_tuple',\n",
       " 'k',\n",
       " 'kurtosis',\n",
       " 'lag',\n",
       " 'last',\n",
       " 'last_day',\n",
       " 'lead',\n",
       " 'least',\n",
       " 'length',\n",
       " 'levenshtein',\n",
       " 'lit',\n",
       " 'locate',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log1p',\n",
       " 'log2',\n",
       " 'lower',\n",
       " 'lpad',\n",
       " 'ltrim',\n",
       " 'map',\n",
       " 'math',\n",
       " 'max',\n",
       " 'md5',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minute',\n",
       " 'monotonically_increasing_id',\n",
       " 'month',\n",
       " 'months_between',\n",
       " 'nanvl',\n",
       " 'next_day',\n",
       " 'ntile',\n",
       " 'percent_rank',\n",
       " 'posexplode',\n",
       " 'pow',\n",
       " 'quarter',\n",
       " 'radians',\n",
       " 'rand',\n",
       " 'randn',\n",
       " 'rank',\n",
       " 'regexp_extract',\n",
       " 'regexp_replace',\n",
       " 'repeat',\n",
       " 'reverse',\n",
       " 'rint',\n",
       " 'round',\n",
       " 'row_number',\n",
       " 'rpad',\n",
       " 'rtrim',\n",
       " 'second',\n",
       " 'sha1',\n",
       " 'sha2',\n",
       " 'shiftLeft',\n",
       " 'shiftRight',\n",
       " 'shiftRightUnsigned',\n",
       " 'signum',\n",
       " 'sin',\n",
       " 'since',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'skewness',\n",
       " 'sort_array',\n",
       " 'soundex',\n",
       " 'spark_partition_id',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'stddev',\n",
       " 'stddev_pop',\n",
       " 'stddev_samp',\n",
       " 'struct',\n",
       " 'substring',\n",
       " 'substring_index',\n",
       " 'sum',\n",
       " 'sumDistinct',\n",
       " 'sys',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'toDegrees',\n",
       " 'toRadians',\n",
       " 'to_date',\n",
       " 'to_json',\n",
       " 'to_timestamp',\n",
       " 'to_utc_timestamp',\n",
       " 'translate',\n",
       " 'trim',\n",
       " 'trunc',\n",
       " 'udf',\n",
       " 'unbase64',\n",
       " 'unhex',\n",
       " 'unix_timestamp',\n",
       " 'upper',\n",
       " 'v',\n",
       " 'var_pop',\n",
       " 'var_samp',\n",
       " 'variance',\n",
       " 'weekofyear',\n",
       " 'when',\n",
       " 'window',\n",
       " 'year']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pyspark.sql.functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+-----+---------------------------------+\n",
      "|key  |value|key   |value|d                                |\n",
      "+-----+-----+------+-----+---------------------------------+\n",
      "|name |nope |arrest|0    |Map(name -> nope, name2 -> again)|\n",
      "|name2|again|arrest|0    |Map(name -> nope, name2 -> again)|\n",
      "|name |nope |stuff |1    |Map(name -> nope, name2 -> again)|\n",
      "|name2|again|stuff |1    |Map(name -> nope, name2 -> again)|\n",
      "|name |nope |wrench|2    |Map(name -> nope, name2 -> again)|\n",
      "|name2|again|wrench|2    |Map(name -> nope, name2 -> again)|\n",
      "|name |nope |dam   |3    |Map(name -> nope, name2 -> again)|\n",
      "|name2|again|dam   |3    |Map(name -> nope, name2 -> again)|\n",
      "|name |nope |strobe|4    |Map(name -> nope, name2 -> again)|\n",
      "|name2|again|strobe|4    |Map(name -> nope, name2 -> again)|\n",
      "+-----+-----+------+-----+---------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random_words import RandomWords\n",
    "rows = [Row(i, RandomWords().random_word(), {\"name\": \"nope\", \"name2\": \"again\"}) for i in range(10)]\n",
    "df_map = spark.createDataFrame(rows, [\"id\", \"word\", \"d\"])\\\n",
    "              .select(create_map(col(\"word\"), col(\"id\")).alias(\"map\"), col(\"d\"))\\\n",
    "              .select(explode(col(\"map\")), col(\"d\"))\\\n",
    "              .select(explode(col(\"d\")), col(\"*\"))\\\n",
    "              .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonstring\n",
      "--------------------------------------\n",
      "{\"myJSONKey\": {\"myJSONValue\":[1,2,3]}}\n",
      "\n",
      "root\n",
      " |-- jsonstring: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json = \"\"\"\n",
    "'{\"myJSONKey\": {\"myJSONValue\":[1,2,3]}}' as jsonstring\n",
    "\"\"\"\n",
    "json_df = spark.range(1).selectExpr(json)\n",
    "print(tabulate(json_df.collect(), json_df.columns))\n",
    "print(\"\")\n",
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+--------------------+\n",
      "|get_json_object(jsonstring, $.myJSONKey.myJSONValue[0])|                  c0|\n",
      "+-------------------------------------------------------+--------------------+\n",
      "|                                                      1|{\"myJSONValue\":[1...|\n",
      "+-------------------------------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.select(get_json_object(col(\"jsonstring\"), \"$.myJSONKey.myJSONValue[0]\"),\n",
    "               json_tuple(col(\"jsonstring\"), \"myJSONKey\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------------+\n",
      "|complex       |json                        |\n",
      "+--------------+----------------------------+\n",
      "|[0,opinions]  |{\"id\":0,\"word\":\"opinions\"}  |\n",
      "|[1,hilltops]  |{\"id\":1,\"word\":\"hilltops\"}  |\n",
      "|[2,galleys]   |{\"id\":2,\"word\":\"galleys\"}   |\n",
      "|[3,signaler]  |{\"id\":3,\"word\":\"signaler\"}  |\n",
      "|[4,groan]     |{\"id\":4,\"word\":\"groan\"}     |\n",
      "|[5,contention]|{\"id\":5,\"word\":\"contention\"}|\n",
      "|[6,medicines] |{\"id\":6,\"word\":\"medicines\"} |\n",
      "|[7,sight]     |{\"id\":7,\"word\":\"sight\"}     |\n",
      "|[8,conspiracy]|{\"id\":8,\"word\":\"conspiracy\"}|\n",
      "|[9,fists]     |{\"id\":9,\"word\":\"fists\"}     |\n",
      "+--------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [Row(i, RandomWords().random_word()) for i in range(10)]\n",
    "spark.createDataFrame(rows, [\"id\", \"word\"])\\\n",
    "     .selectExpr(\"(id, word) as complex\")\\\n",
    "     .withColumn(\"json\", to_json(col(\"complex\")))\\\n",
    "     .show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

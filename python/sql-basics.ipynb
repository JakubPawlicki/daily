{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "assert spark.range(5).rdd.flatMap(lambda x: x).sum() == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+--------+------+-----+-----+-----+----+----+----+----+----+----+----+---+---+\n",
      "|  id|           2|       3|     4|    5|    6|    7|   8|   9|  10|  11|  12|  13|  14| 15| 16|\n",
      "+----+------------+--------+------+-----+-----+-----+----+----+----+----+----+----+----+---+---+\n",
      "|3000|101110111000|11010010|232320|44000|21520|11514|5670|4103|3000|2288|18A0|149A|1144|D50|BB8|\n",
      "|3001|101110111001|11010011|232321|44001|21521|11515|5671|4104|3001|2289|18A1|149B|1145|D51|BB9|\n",
      "|3002|101110111010|11010012|232322|44002|21522|11516|5672|4105|3002|228A|18A2|149C|1146|D52|BBA|\n",
      "|3003|101110111011|11010020|232323|44003|21523|11520|5673|4106|3003|2290|18A3|14A0|1147|D53|BBB|\n",
      "|3004|101110111100|11010021|232330|44004|21524|11521|5674|4107|3004|2291|18A4|14A1|1148|D54|BBC|\n",
      "|3005|101110111101|11010022|232331|44010|21525|11522|5675|4108|3005|2292|18A5|14A2|1149|D55|BBD|\n",
      "|3006|101110111110|11010100|232332|44011|21530|11523|5676|4110|3006|2293|18A6|14A3|114A|D56|BBE|\n",
      "|3007|101110111111|11010101|232333|44012|21531|11524|5677|4111|3007|2294|18A7|14A4|114B|D57|BBF|\n",
      "|3008|101111000000|11010102|233000|44013|21532|11525|5700|4112|3008|2295|18A8|14A5|114C|D58|BC0|\n",
      "|3009|101111000001|11010110|233001|44014|21533|11526|5701|4113|3009|2296|18A9|14A6|114D|D59|BC1|\n",
      "|3010|101111000010|11010111|233002|44020|21534|11530|5702|4114|3010|2297|18AA|14A7|1150|D5A|BC2|\n",
      "|3011|101111000011|11010112|233003|44021|21535|11531|5703|4115|3011|2298|18AB|14A8|1151|D5B|BC3|\n",
      "|3012|101111000100|11010120|233010|44022|21540|11532|5704|4116|3012|2299|18B0|14A9|1152|D5C|BC4|\n",
      "|3013|101111000101|11010121|233011|44023|21541|11533|5705|4117|3013|229A|18B1|14AA|1153|D5D|BC5|\n",
      "|3014|101111000110|11010122|233012|44024|21542|11534|5706|4118|3014|22A0|18B2|14AB|1154|D5E|BC6|\n",
      "|3015|101111000111|11010200|233013|44030|21543|11535|5707|4120|3015|22A1|18B3|14AC|1155|D60|BC7|\n",
      "|3016|101111001000|11010201|233020|44031|21544|11536|5710|4121|3016|22A2|18B4|14B0|1156|D61|BC8|\n",
      "|3017|101111001001|11010202|233021|44032|21545|11540|5711|4122|3017|22A3|18B5|14B1|1157|D62|BC9|\n",
      "|3018|101111001010|11010210|233022|44033|21550|11541|5712|4123|3018|22A4|18B6|14B2|1158|D63|BCA|\n",
      "|3019|101111001011|11010211|233023|44034|21551|11542|5713|4124|3019|22A5|18B7|14B3|1159|D64|BCB|\n",
      "+----+------------+--------+------+-----+-----+-----+----+----+----+----+----+----+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import conv\n",
    "spark.range(3000, 3030).select(\"id\", *[conv(\"id\", 10, n).alias(str(n)) for n in range(2, 17)]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "|len| freq|cumul|\n",
      "+---+-----+-----+\n",
      "| 23|    1|    1|\n",
      "| 22|    5|    9|\n",
      "| 21|    3|    4|\n",
      "| 20|   10|   19|\n",
      "| 19|   31|   50|\n",
      "| 18|   72|  174|\n",
      "| 17|  176|  350|\n",
      "| 16|  381|  913|\n",
      "| 15|  890| 2649|\n",
      "| 14| 1676| 4325|\n",
      "| 13| 3234| 7559|\n",
      "| 12| 5502|16413|\n",
      "| 11| 8408|31620|\n",
      "| 10|11529|54451|\n",
      "|  9|14248|68699|\n",
      "|  8|15684|99171|\n",
      "|  7|14788|83487|\n",
      "|  6|11302|42922|\n",
      "|  5| 6799|23212|\n",
      "|  4| 3352|10911|\n",
      "+---+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, length, desc, sum\n",
    "from pyspark.sql import Window\n",
    "cart = spark.read.csv(\"/usr/share/dict/american-english\")\n",
    "cart.select(length(cart[0]).alias(\"len\"))\\\n",
    "    .groupby(\"len\")\\\n",
    "    .agg(count(\"len\").alias(\"freq\"))\\\n",
    "    .withColumn(\"cumul\", sum(\"freq\").over(Window.orderBy(\"freq\").rowsBetween(Window.unboundedPreceding, Window.currentRow)))\\\n",
    "    .sort(desc(\"len\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "| id1|  id2|rank|\n",
      "+----+-----+----+\n",
      "| A's| AZ's|   1|\n",
      "|AB's|ABM's|   1|\n",
      "| A's| Ac's|   1|\n",
      "|   A|   Ag|   1|\n",
      "| A's| Ag's|   1|\n",
      "|   A|   Am|   1|\n",
      "| A's| Al's|   1|\n",
      "|   A|   As|   1|\n",
      "| A's| Am's|   1|\n",
      "| A's| AB's|   1|\n",
      "| A's| Ar's|   1|\n",
      "| A's| AI's|   1|\n",
      "| A's|   As|   1|\n",
      "|AA's| AI's|   1|\n",
      "|AA's| AM's|   1|\n",
      "|   A|   Ac|   1|\n",
      "|AA's| AZ's|   1|\n",
      "|   A|   Ar|   1|\n",
      "|AA's| Ac's|   1|\n",
      "| A's| AC's|   1|\n",
      "+----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import levenshtein, length\n",
    "words = spark.read.csv(\"/usr/share/dict/american-english\").limit(1000).cache()\n",
    "cart = words.select(words[0].alias(\"id1\"))\\\n",
    "            .crossJoin(words.select(words[0].alias(\"id2\")))\n",
    "cart.select(cart[0], cart[1], levenshtein(cart[0],cart[1]).alias(\"rank\"))\\\n",
    "    .where(\"rank != 0\")\\\n",
    "    .sort(\"rank\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+\n",
      "|                 id1|                 id2|rank|\n",
      "+--------------------+--------------------+----+\n",
      "|electrocardiograph's| electrocardiographs|   1|\n",
      "| telecommunication's|  telecommunications|   1|\n",
      "|electroencephalogram|electroencephalog...|   1|\n",
      "|electroencephalog...|electroencephalog...|   1|\n",
      "|oversimplification's| oversimplifications|   1|\n",
      "|electroencephalog...|electroencephalog...|   1|\n",
      "|telecommunications's| telecommunication's|   1|\n",
      "|electroencephalog...|electroencephalogram|   1|\n",
      "| Congregationalist's|  Congregationalists|   1|\n",
      "|chlorofluorocarbon's| chlorofluorocarbons|   1|\n",
      "| chlorofluorocarbons|chlorofluorocarbon's|   1|\n",
      "| electrocardiogram's|  electrocardiograms|   1|\n",
      "| telecommunication's|telecommunications's|   1|\n",
      "| electrocardiographs|electrocardiograph's|   1|\n",
      "|electroencephalog...|electroencephalog...|   1|\n",
      "| electrocardiographs|  electrocardiograph|   1|\n",
      "|electroencephalog...|electroencephalog...|   1|\n",
      "| interrelationship's|  interrelationships|   1|\n",
      "| conceptualization's|  conceptualizations|   1|\n",
      "| misinterpretation's|  misinterpretations|   1|\n",
      "+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import levenshtein, length\n",
    "words = spark.read.csv(\"/usr/share/dict/american-english\")\n",
    "top1000 = words.sort(-length(words[0])).limit(1000).cache()\n",
    "cart = top1000.select(top1000[0].alias(\"id1\"))\\\n",
    "            .crossJoin(top1000.select(top1000[0].alias(\"id2\")))\n",
    "cart.select(cart[0], cart[1], levenshtein(cart[0],cart[1]).alias(\"rank\"))\\\n",
    "    .where(\"rank != 0\")\\\n",
    "    .sort(\"rank\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-32615cec5dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthunder\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoseries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetrend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfourier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'images'"
     ]
    }
   ],
   "source": [
    "import thunder as td\n",
    "\n",
    "data = td.images.fromrandom()\n",
    "ts = data.median_filter(3).toseries()\n",
    "frequencies = ts.detrend().fourier(freq=3).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
